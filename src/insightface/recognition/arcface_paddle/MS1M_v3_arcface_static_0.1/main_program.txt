{ // block_idx:0  parent_idx:-1  forward_idx:-1  backward_idx:-1
    var image : LOD_TENSOR.shape(-1, 3, 112, 112).dtype(float16).stop_gradient(True)
    var label : LOD_TENSOR.shape(-1,).dtype(int32).stop_gradient(True)
    persist trainable param conv2d_0.w_0 : LOD_TENSOR.shape(64, 3, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_0.tmp_0 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_0.w_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_0.b_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist param batch_norm_0.w_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    persist param batch_norm_0.w_2 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_0.tmp_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_0.tmp_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_0.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_0.tmp_3 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    persist trainable param prelu_0.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_0.tmp_0 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_1.w_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_1.b_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist param batch_norm_1.w_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    persist param batch_norm_1.w_2 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_1.tmp_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_1.tmp_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_1.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_1.tmp_3 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_1.w_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_1.tmp_0 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_2.w_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_2.b_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist param batch_norm_2.w_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    persist param batch_norm_2.w_2 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_2.tmp_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_2.tmp_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_2.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_2.tmp_3 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    persist trainable param prelu_1.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_1.tmp_0 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_2.w_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_2.tmp_0 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_3.w_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_3.b_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist param batch_norm_3.w_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    persist param batch_norm_3.w_2 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_3.tmp_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_3.tmp_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_3.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_3.tmp_3 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_3.w_0 : LOD_TENSOR.shape(64, 64, 1, 1).dtype(float16).stop_gradient(False)
    var conv2d_3.tmp_0 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_4.w_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_4.b_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist param batch_norm_4.w_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    persist param batch_norm_4.w_2 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_4.tmp_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_4.tmp_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_4.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_4.tmp_3 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var elementwise_add_0 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_5.w_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_5.b_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist param batch_norm_5.w_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    persist param batch_norm_5.w_2 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_5.tmp_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_5.tmp_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_5.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_5.tmp_3 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_4.w_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_4.tmp_0 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_6.w_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_6.b_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist param batch_norm_6.w_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    persist param batch_norm_6.w_2 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_6.tmp_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_6.tmp_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_6.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_6.tmp_3 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param prelu_2.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_2.tmp_0 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_5.w_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_5.tmp_0 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_7.w_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_7.b_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist param batch_norm_7.w_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    persist param batch_norm_7.w_2 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_7.tmp_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_7.tmp_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_7.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_7.tmp_3 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var elementwise_add_1 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_8.w_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_8.b_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist param batch_norm_8.w_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    persist param batch_norm_8.w_2 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_8.tmp_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_8.tmp_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_8.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_8.tmp_3 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_6.w_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_6.tmp_0 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_9.w_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_9.b_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist param batch_norm_9.w_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    persist param batch_norm_9.w_2 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_9.tmp_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_9.tmp_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_9.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_9.tmp_3 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param prelu_3.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_3.tmp_0 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_7.w_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_7.tmp_0 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_10.w_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_10.b_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist param batch_norm_10.w_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    persist param batch_norm_10.w_2 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_10.tmp_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_10.tmp_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_10.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_10.tmp_3 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var elementwise_add_2 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_11.w_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_11.b_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist param batch_norm_11.w_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    persist param batch_norm_11.w_2 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_11.tmp_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_11.tmp_1 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(True)
    var batch_norm_11.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_11.tmp_3 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_8.w_0 : LOD_TENSOR.shape(128, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_8.tmp_0 : LOD_TENSOR.shape(-1, 128, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_12.w_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_12.b_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist param batch_norm_12.w_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    persist param batch_norm_12.w_2 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_12.tmp_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_12.tmp_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_12.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_12.tmp_3 : LOD_TENSOR.shape(-1, 128, 56, 56).dtype(float16).stop_gradient(False)
    persist trainable param prelu_4.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_4.tmp_0 : LOD_TENSOR.shape(-1, 128, 56, 56).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_9.w_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_9.tmp_0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_13.w_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_13.b_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist param batch_norm_13.w_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    persist param batch_norm_13.w_2 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_13.tmp_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_13.tmp_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_13.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_13.tmp_3 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_10.w_0 : LOD_TENSOR.shape(128, 64, 1, 1).dtype(float16).stop_gradient(False)
    var conv2d_10.tmp_0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_14.w_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_14.b_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist param batch_norm_14.w_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    persist param batch_norm_14.w_2 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_14.tmp_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_14.tmp_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_14.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_14.tmp_3 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_3 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_15.w_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_15.b_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist param batch_norm_15.w_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    persist param batch_norm_15.w_2 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_15.tmp_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_15.tmp_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_15.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_15.tmp_3 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_11.w_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_11.tmp_0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_16.w_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_16.b_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist param batch_norm_16.w_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    persist param batch_norm_16.w_2 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_16.tmp_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_16.tmp_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_16.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_16.tmp_3 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param prelu_5.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_5.tmp_0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_12.w_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_12.tmp_0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_17.w_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_17.b_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist param batch_norm_17.w_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    persist param batch_norm_17.w_2 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_17.tmp_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_17.tmp_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_17.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_17.tmp_3 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_4 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_18.w_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_18.b_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist param batch_norm_18.w_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    persist param batch_norm_18.w_2 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_18.tmp_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_18.tmp_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_18.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_18.tmp_3 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_13.w_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_13.tmp_0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_19.w_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_19.b_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist param batch_norm_19.w_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    persist param batch_norm_19.w_2 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_19.tmp_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_19.tmp_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_19.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_19.tmp_3 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param prelu_6.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_6.tmp_0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_14.w_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_14.tmp_0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_20.w_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_20.b_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist param batch_norm_20.w_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    persist param batch_norm_20.w_2 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_20.tmp_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_20.tmp_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_20.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_20.tmp_3 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_5 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_21.w_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_21.b_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist param batch_norm_21.w_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    persist param batch_norm_21.w_2 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_21.tmp_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_21.tmp_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_21.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_21.tmp_3 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_15.w_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_15.tmp_0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_22.w_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_22.b_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist param batch_norm_22.w_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    persist param batch_norm_22.w_2 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_22.tmp_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_22.tmp_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_22.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_22.tmp_3 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param prelu_7.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_7.tmp_0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_16.w_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_16.tmp_0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_23.w_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_23.b_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist param batch_norm_23.w_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    persist param batch_norm_23.w_2 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_23.tmp_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_23.tmp_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_23.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_23.tmp_3 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_6 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_24.w_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_24.b_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist param batch_norm_24.w_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    persist param batch_norm_24.w_2 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_24.tmp_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_24.tmp_1 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(True)
    var batch_norm_24.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_24.tmp_3 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_17.w_0 : LOD_TENSOR.shape(256, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_17.tmp_0 : LOD_TENSOR.shape(-1, 256, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_25.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_25.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_25.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_25.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_25.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_25.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_25.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_25.tmp_3 : LOD_TENSOR.shape(-1, 256, 28, 28).dtype(float16).stop_gradient(False)
    persist trainable param prelu_8.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_8.tmp_0 : LOD_TENSOR.shape(-1, 256, 28, 28).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_18.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_18.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_26.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_26.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_26.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_26.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_26.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_26.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_26.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_26.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_19.w_0 : LOD_TENSOR.shape(256, 128, 1, 1).dtype(float16).stop_gradient(False)
    var conv2d_19.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_27.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_27.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_27.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_27.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_27.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_27.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_27.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_27.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_7 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_28.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_28.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_28.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_28.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_28.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_28.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_28.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_28.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_20.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_20.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_29.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_29.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_29.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_29.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_29.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_29.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_29.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_29.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_9.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_9.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_21.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_21.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_30.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_30.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_30.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_30.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_30.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_30.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_30.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_30.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_8 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_31.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_31.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_31.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_31.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_31.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_31.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_31.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_31.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_22.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_22.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_32.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_32.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_32.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_32.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_32.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_32.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_32.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_32.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_10.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_10.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_23.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_23.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_33.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_33.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_33.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_33.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_33.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_33.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_33.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_33.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_9 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_34.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_34.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_34.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_34.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_34.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_34.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_34.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_34.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_24.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_24.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_35.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_35.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_35.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_35.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_35.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_35.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_35.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_35.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_11.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_11.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_25.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_25.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_36.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_36.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_36.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_36.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_36.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_36.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_36.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_36.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_10 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_37.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_37.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_37.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_37.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_37.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_37.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_37.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_37.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_26.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_26.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_38.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_38.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_38.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_38.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_38.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_38.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_38.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_38.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_12.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_12.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_27.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_27.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_39.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_39.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_39.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_39.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_39.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_39.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_39.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_39.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_11 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_40.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_40.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_40.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_40.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_40.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_40.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_40.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_40.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_28.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_28.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_41.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_41.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_41.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_41.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_41.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_41.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_41.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_41.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_13.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_13.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_29.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_29.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_42.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_42.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_42.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_42.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_42.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_42.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_42.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_42.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_12 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_43.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_43.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_43.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_43.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_43.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_43.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_43.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_43.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_30.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_30.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_44.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_44.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_44.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_44.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_44.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_44.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_44.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_44.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_14.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_14.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_31.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_31.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_45.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_45.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_45.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_45.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_45.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_45.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_45.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_45.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_13 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_46.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_46.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_46.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_46.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_46.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_46.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_46.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_46.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_32.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_32.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_47.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_47.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_47.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_47.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_47.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_47.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_47.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_47.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_15.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_15.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_33.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_33.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_48.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_48.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_48.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_48.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_48.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_48.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_48.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_48.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_14 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_49.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_49.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_49.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_49.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_49.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_49.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_49.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_49.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_34.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_34.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_50.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_50.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_50.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_50.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_50.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_50.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_50.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_50.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_16.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_16.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_35.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_35.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_51.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_51.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_51.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_51.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_51.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_51.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_51.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_51.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_15 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_52.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_52.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_52.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_52.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_52.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_52.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_52.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_52.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_36.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_36.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_53.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_53.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_53.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_53.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_53.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_53.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_53.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_53.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_17.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_17.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_37.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_37.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_54.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_54.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_54.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_54.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_54.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_54.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_54.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_54.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_55.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_55.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_55.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_55.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_55.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_55.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_55.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_55.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_38.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_38.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_56.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_56.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_56.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_56.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_56.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_56.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_56.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_56.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_18.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_18.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_39.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_39.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_57.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_57.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_57.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_57.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_57.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_57.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_57.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_57.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_17 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_58.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_58.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_58.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_58.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_58.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_58.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_58.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_58.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_40.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_40.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_59.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_59.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_59.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_59.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_59.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_59.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_59.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_59.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_19.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_19.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_41.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_41.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_60.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_60.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_60.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_60.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_60.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_60.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_60.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_60.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_18 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_61.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_61.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_61.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_61.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_61.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_61.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_61.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_61.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_42.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_42.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_62.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_62.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_62.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_62.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_62.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_62.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_62.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_62.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_20.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_20.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_43.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_43.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_63.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_63.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_63.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_63.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_63.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_63.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_63.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_63.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_19 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_64.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_64.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_64.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_64.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_64.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_64.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_64.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_64.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_44.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_44.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_65.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_65.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_65.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_65.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_65.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_65.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_65.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_65.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_21.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_21.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_45.w_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_45.tmp_0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_66.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_66.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_66.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_66.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_66.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_66.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_66.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_66.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_20 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_67.w_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_67.b_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist param batch_norm_67.w_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    persist param batch_norm_67.w_2 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_67.tmp_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_67.tmp_1 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(True)
    var batch_norm_67.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_67.tmp_3 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_46.w_0 : LOD_TENSOR.shape(512, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_46.tmp_0 : LOD_TENSOR.shape(-1, 512, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_68.w_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_68.b_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist param batch_norm_68.w_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    persist param batch_norm_68.w_2 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_68.tmp_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_68.tmp_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_68.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_68.tmp_3 : LOD_TENSOR.shape(-1, 512, 14, 14).dtype(float16).stop_gradient(False)
    persist trainable param prelu_22.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_22.tmp_0 : LOD_TENSOR.shape(-1, 512, 14, 14).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_47.w_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_47.tmp_0 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_69.w_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_69.b_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist param batch_norm_69.w_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    persist param batch_norm_69.w_2 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_69.tmp_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_69.tmp_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_69.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_69.tmp_3 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_48.w_0 : LOD_TENSOR.shape(512, 256, 1, 1).dtype(float16).stop_gradient(False)
    var conv2d_48.tmp_0 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_70.w_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_70.b_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist param batch_norm_70.w_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    persist param batch_norm_70.w_2 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_70.tmp_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_70.tmp_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_70.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_70.tmp_3 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var elementwise_add_21 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_71.w_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_71.b_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist param batch_norm_71.w_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    persist param batch_norm_71.w_2 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_71.tmp_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_71.tmp_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_71.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_71.tmp_3 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_49.w_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_49.tmp_0 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_72.w_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_72.b_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist param batch_norm_72.w_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    persist param batch_norm_72.w_2 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_72.tmp_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_72.tmp_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_72.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_72.tmp_3 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param prelu_23.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_23.tmp_0 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_50.w_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_50.tmp_0 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_73.w_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_73.b_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist param batch_norm_73.w_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    persist param batch_norm_73.w_2 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_73.tmp_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_73.tmp_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_73.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_73.tmp_3 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var elementwise_add_22 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_74.w_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_74.b_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist param batch_norm_74.w_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    persist param batch_norm_74.w_2 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_74.tmp_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_74.tmp_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_74.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_74.tmp_3 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param conv2d_51.w_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_51.tmp_0 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_75.w_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_75.b_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist param batch_norm_75.w_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    persist param batch_norm_75.w_2 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_75.tmp_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_75.tmp_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_75.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_75.tmp_3 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param prelu_24.w_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_24.tmp_0 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float32).stop_gradient(False)
    persist trainable param conv2d_52.w_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_52.tmp_0 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_76.w_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_76.b_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist param batch_norm_76.w_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    persist param batch_norm_76.w_2 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_76.tmp_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_76.tmp_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_76.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_76.tmp_3 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var elementwise_add_23 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_77.w_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_77.b_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist param batch_norm_77.w_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    persist param batch_norm_77.w_2 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_77.tmp_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_77.tmp_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_77.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_77.tmp_3 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    persist trainable param fc_0.w_0 : LOD_TENSOR.shape(25088, 512).dtype(float16).stop_gradient(False)
    var fc_0.tmp_0 : LOD_TENSOR.shape(-1, 512).dtype(float16).stop_gradient(False)
    persist trainable param fc_0.b_0 : LOD_TENSOR.shape(512,).dtype(float16).stop_gradient(False)
    var fc_0.tmp_1 : LOD_TENSOR.shape(-1, 512).dtype(float16).stop_gradient(False)
    persist trainable param batch_norm_78.w_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist trainable param batch_norm_78.b_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist param batch_norm_78.w_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    persist param batch_norm_78.w_2 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_78.tmp_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_78.tmp_1 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    var batch_norm_78.tmp_2 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(True)
    var batch_norm_78.tmp_3 : LOD_TENSOR.shape(-1, 512).dtype(float16).stop_gradient(False)
    persist trainable param dist@fc@rank@00000.w.w_0 : LOD_TENSOR.shape(512, 312).dtype(float16).stop_gradient(False)
    var class_center_sample_0.tmp_0 : LOD_TENSOR.shape(-1,).dtype(int32).stop_gradient(True)
    var class_center_sample_0.tmp_1 : LOD_TENSOR.shape(31,).dtype(int32).stop_gradient(True)
    var gather_0.tmp_0 : LOD_TENSOR.shape(512, 31).dtype(float16).stop_gradient(False)
    var p_norm_0.tmp_0 : LOD_TENSOR.shape(-1, 1).dtype(float32).stop_gradient(False)
    var _generated_var_0 : LOD_TENSOR.shape().dtype(float16).stop_gradient(False)
    var fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(True)
    var elementwise_max_0 : LOD_TENSOR.shape(-1, 1).dtype(float32).stop_gradient(False)
    var elementwise_div_0 : LOD_TENSOR.shape(-1, 512).dtype(float32).stop_gradient(False)
    var p_norm_1.tmp_0 : LOD_TENSOR.shape(1, 31).dtype(float32).stop_gradient(False)
    var _generated_var_1 : LOD_TENSOR.shape().dtype(float16).stop_gradient(False)
    var fill_constant_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(True)
    var elementwise_max_1 : LOD_TENSOR.shape(1, 31).dtype(float32).stop_gradient(False)
    var elementwise_div_1 : LOD_TENSOR.shape(512, 31).dtype(float32).stop_gradient(False)
    var matmul_v2_0.tmp_0 : LOD_TENSOR.shape(-1, 31).dtype(float16).stop_gradient(False)
    var unsqueeze2_0.tmp_0 : LOD_TENSOR.shape(-1, 1).dtype(int32).stop_gradient(True)
    var unsqueeze2_0.tmp_1 : LOD_TENSOR.shape(0, -1).dtype(int32).stop_gradient(True)
    var margin_cross_entropy_0.tmp_0 : LOD_TENSOR.shape(-1, 31).dtype(float32).stop_gradient(False)
    var margin_cross_entropy_0.tmp_1 : LOD_TENSOR.shape(-1, 1).dtype(float32).stop_gradient(False)
    var mean_0.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    persist var loss_scaling_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var num_good_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var num_bad_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    var prelu_0.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_0.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float32).stop_gradient(False)
    var batch_norm_1.tmp_3.cast_fp16 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    var prelu_1.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_2.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float32).stop_gradient(False)
    var prelu_1.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    var prelu_0.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    var prelu_2.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_6.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float32).stop_gradient(False)
    var prelu_2.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var prelu_3.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_9.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float32).stop_gradient(False)
    var prelu_3.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var prelu_4.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_12.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 128, 56, 56).dtype(float32).stop_gradient(False)
    var prelu_4.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 128, 56, 56).dtype(float16).stop_gradient(False)
    var prelu_5.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_16.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float32).stop_gradient(False)
    var prelu_5.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var prelu_6.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_19.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float32).stop_gradient(False)
    var prelu_6.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var prelu_7.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_22.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float32).stop_gradient(False)
    var prelu_7.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var prelu_8.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_25.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 28, 28).dtype(float32).stop_gradient(False)
    var prelu_8.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 28, 28).dtype(float16).stop_gradient(False)
    var prelu_9.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_29.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_9.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_10.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_32.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_10.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_11.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_35.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_11.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_12.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_38.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_12.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_13.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_41.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_13.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_14.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_44.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_14.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_15.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_47.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_15.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_16.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_50.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_16.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_17.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_53.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_17.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_18.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_56.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_18.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_19.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_59.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_19.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_20.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_62.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_20.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_21.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_65.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_21.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_22.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_68.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 512, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_22.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 512, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_23.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_72.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float32).stop_gradient(False)
    var prelu_23.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var prelu_24.w_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_75.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float32).stop_gradient(False)
    var prelu_24.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var batch_norm_78.tmp_3.cast_fp32 : LOD_TENSOR.shape(-1, 512).dtype(float32).stop_gradient(False)
    var fill_constant_1.tmp_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var gather_0.tmp_0.cast_fp32 : LOD_TENSOR.shape(512, 31).dtype(float32).stop_gradient(False)
    var fill_constant_3.tmp_0.cast_fp32 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var elementwise_div_0.cast_fp16 : LOD_TENSOR.shape(-1, 512).dtype(float16).stop_gradient(False)
    var elementwise_div_1.cast_fp16 : LOD_TENSOR.shape(512, 31).dtype(float16).stop_gradient(False)
    var matmul_v2_0.tmp_0.cast_fp32 : LOD_TENSOR.shape(-1, 31).dtype(float32).stop_gradient(False)
    var tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var batch_norm_0.b_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_0.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float32).stop_gradient(False)
    var batch_norm_0.tmp_3@GRAD : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    var batch_norm_0.w_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_1.b_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_1.tmp_3.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    var batch_norm_1.tmp_3@GRAD : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float32).stop_gradient(False)
    var batch_norm_1.w_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_10.b_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_10.tmp_3@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var batch_norm_10.w_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_11.b_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_11.tmp_3@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var batch_norm_11.w_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_12.b_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_12.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 128, 56, 56).dtype(float32).stop_gradient(False)
    var batch_norm_12.tmp_3@GRAD : LOD_TENSOR.shape(-1, 128, 56, 56).dtype(float16).stop_gradient(False)
    var batch_norm_12.w_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_13.b_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_13.tmp_3@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var batch_norm_13.w_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_14.b_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_14.tmp_3@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var batch_norm_14.w_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_15.b_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_15.tmp_3@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var batch_norm_15.w_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_16.b_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_16.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float32).stop_gradient(False)
    var batch_norm_16.tmp_3@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var batch_norm_16.w_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_17.b_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_17.tmp_3@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var batch_norm_17.w_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_18.b_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_18.tmp_3@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var batch_norm_18.w_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_19.b_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_19.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float32).stop_gradient(False)
    var batch_norm_19.tmp_3@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var batch_norm_19.w_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_2.b_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_2.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float32).stop_gradient(False)
    var batch_norm_2.tmp_3@GRAD : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    var batch_norm_2.w_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_20.b_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_20.tmp_3@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var batch_norm_20.w_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_21.b_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_21.tmp_3@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var batch_norm_21.w_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_22.b_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_22.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float32).stop_gradient(False)
    var batch_norm_22.tmp_3@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var batch_norm_22.w_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_23.b_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_23.tmp_3@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var batch_norm_23.w_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_24.b_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_24.tmp_3@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var batch_norm_24.w_0@GRAD : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    var batch_norm_25.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_25.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 28, 28).dtype(float32).stop_gradient(False)
    var batch_norm_25.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 28, 28).dtype(float16).stop_gradient(False)
    var batch_norm_25.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_26.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_26.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_26.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_27.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_27.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_27.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_28.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_28.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_28.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_29.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_29.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_29.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_29.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_3.b_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_3.tmp_3@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var batch_norm_3.w_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_30.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_30.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_30.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_31.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_31.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_31.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_32.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_32.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_32.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_32.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_33.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_33.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_33.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_34.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_34.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_34.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_35.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_35.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_35.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_35.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_36.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_36.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_36.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_37.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_37.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_37.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_38.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_38.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_38.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_38.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_39.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_39.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_39.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_4.b_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_4.tmp_3@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var batch_norm_4.w_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_40.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_40.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_40.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_41.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_41.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_41.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_41.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_42.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_42.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_42.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_43.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_43.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_43.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_44.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_44.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_44.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_44.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_45.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_45.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_45.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_46.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_46.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_46.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_47.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_47.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_47.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_47.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_48.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_48.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_48.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_49.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_49.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_49.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_5.b_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_5.tmp_3@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var batch_norm_5.w_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_50.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_50.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_50.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_50.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_51.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_51.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_51.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_52.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_52.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_52.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_53.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_53.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_53.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_53.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_54.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_54.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_54.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_55.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_55.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_55.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_56.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_56.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_56.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_56.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_57.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_57.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_57.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_58.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_58.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_58.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_59.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_59.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_59.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_59.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_6.b_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_6.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float32).stop_gradient(False)
    var batch_norm_6.tmp_3@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var batch_norm_6.w_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_60.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_60.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_60.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_61.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_61.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_61.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_62.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_62.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_62.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_62.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_63.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_63.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_63.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_64.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_64.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_64.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_65.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_65.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_65.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_65.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_66.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_66.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_66.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_67.b_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_67.tmp_3@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_67.w_0@GRAD : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    var batch_norm_68.b_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_68.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 14, 14).dtype(float32).stop_gradient(False)
    var batch_norm_68.tmp_3@GRAD : LOD_TENSOR.shape(-1, 512, 14, 14).dtype(float16).stop_gradient(False)
    var batch_norm_68.w_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_69.b_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_69.tmp_3@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var batch_norm_69.w_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_7.b_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_7.tmp_3@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var batch_norm_7.w_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_70.b_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_70.tmp_3@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var batch_norm_70.w_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_71.b_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_71.tmp_3@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var batch_norm_71.w_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_72.b_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_72.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float32).stop_gradient(False)
    var batch_norm_72.tmp_3@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var batch_norm_72.w_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_73.b_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_73.tmp_3@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var batch_norm_73.w_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_74.b_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_74.tmp_3@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var batch_norm_74.w_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_75.b_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_75.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float32).stop_gradient(False)
    var batch_norm_75.tmp_3@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var batch_norm_75.w_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_76.b_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_76.tmp_3@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var batch_norm_76.w_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_77.b_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_77.tmp_3@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var batch_norm_77.w_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_78.b_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_78.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512).dtype(float32).stop_gradient(False)
    var batch_norm_78.tmp_3.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512).dtype(float32).stop_gradient(False)
    var batch_norm_78.tmp_3.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512).dtype(float32).stop_gradient(False)
    var batch_norm_78.tmp_3@GRAD : LOD_TENSOR.shape(-1, 512).dtype(float16).stop_gradient(False)
    var batch_norm_78.w_0@GRAD : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    var batch_norm_8.b_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_8.tmp_3@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var batch_norm_8.w_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_9.b_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var batch_norm_9.tmp_3.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float32).stop_gradient(False)
    var batch_norm_9.tmp_3@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var batch_norm_9.w_0@GRAD : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    var conv2d_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    var conv2d_0.w_0@GRAD : LOD_TENSOR.shape(64, 3, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    var conv2d_1.w_0@GRAD : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var conv2d_10.w_0@GRAD : LOD_TENSOR.shape(128, 64, 1, 1).dtype(float16).stop_gradient(False)
    var conv2d_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var conv2d_11.w_0@GRAD : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var conv2d_12.w_0@GRAD : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_13.tmp_0@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var conv2d_13.w_0@GRAD : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_14.tmp_0@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var conv2d_14.w_0@GRAD : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_15.tmp_0@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var conv2d_15.w_0@GRAD : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_16.tmp_0@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var conv2d_16.w_0@GRAD : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_17.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 28, 28).dtype(float16).stop_gradient(False)
    var conv2d_17.w_0@GRAD : LOD_TENSOR.shape(256, 128, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_18.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_18.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_19.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_19.w_0@GRAD : LOD_TENSOR.shape(256, 128, 1, 1).dtype(float16).stop_gradient(False)
    var conv2d_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var conv2d_2.w_0@GRAD : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_20.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_20.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_21.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_21.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_22.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_22.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_23.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_23.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_24.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_24.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_25.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_25.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_26.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_26.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_27.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_27.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_28.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_28.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_29.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_29.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var conv2d_3.w_0@GRAD : LOD_TENSOR.shape(64, 64, 1, 1).dtype(float16).stop_gradient(False)
    var conv2d_30.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_30.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_31.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_31.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_32.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_32.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_33.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_33.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_34.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_34.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_35.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_35.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_36.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_36.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_37.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_37.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_38.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_38.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_39.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_39.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var conv2d_4.w_0@GRAD : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_40.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_40.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_41.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_41.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_42.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_42.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_43.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_43.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_44.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_44.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_45.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_45.w_0@GRAD : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_46.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 14, 14).dtype(float16).stop_gradient(False)
    var conv2d_46.w_0@GRAD : LOD_TENSOR.shape(512, 256, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_47.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var conv2d_47.w_0@GRAD : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_48.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var conv2d_48.w_0@GRAD : LOD_TENSOR.shape(512, 256, 1, 1).dtype(float16).stop_gradient(False)
    var conv2d_49.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var conv2d_49.w_0@GRAD : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var conv2d_5.w_0@GRAD : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_50.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var conv2d_50.w_0@GRAD : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_51.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var conv2d_51.w_0@GRAD : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_52.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var conv2d_52.w_0@GRAD : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var conv2d_6.w_0@GRAD : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var conv2d_7.w_0@GRAD : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 128, 56, 56).dtype(float16).stop_gradient(False)
    var conv2d_8.w_0@GRAD : LOD_TENSOR.shape(128, 64, 3, 3).dtype(float16).stop_gradient(False)
    var conv2d_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var conv2d_9.w_0@GRAD : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float16).stop_gradient(False)
    var elementwise_add_0@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var elementwise_add_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var elementwise_add_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var elementwise_add_10@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_10@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_10@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_11@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_11@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_11@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_12@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_12@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_12@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_13@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_13@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_13@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_14@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_14@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_14@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_15@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_15@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_15@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_16@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_16@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_17@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_17@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_17@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_18@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_18@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_18@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_19@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_19@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_19@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_1@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var elementwise_add_1@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var elementwise_add_1@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var elementwise_add_20@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_20@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_20@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_21@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var elementwise_add_21@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var elementwise_add_21@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var elementwise_add_22@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var elementwise_add_22@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var elementwise_add_22@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var elementwise_add_23@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var elementwise_add_2@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var elementwise_add_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var elementwise_add_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var elementwise_add_3@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_3@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_3@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_4@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_4@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_4@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_5@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_5@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_5@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_6@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_6@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_6@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var elementwise_add_7@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_7@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_7@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_8@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_8@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_8@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_9@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_9@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_add_9@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var elementwise_div_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512).dtype(float16).stop_gradient(False)
    var elementwise_div_0@GRAD : LOD_TENSOR.shape(-1, 512).dtype(float32).stop_gradient(False)
    var elementwise_div_1.cast_fp16@GRAD : LOD_TENSOR.shape(512, 31).dtype(float16).stop_gradient(False)
    var elementwise_div_1@GRAD : LOD_TENSOR.shape(512, 31).dtype(float32).stop_gradient(False)
    var elementwise_max_0@GRAD : LOD_TENSOR.shape(-1, 1).dtype(float32).stop_gradient(False)
    var elementwise_max_1@GRAD : LOD_TENSOR.shape(1, 31).dtype(float32).stop_gradient(False)
    var fc_0.b_0@GRAD : LOD_TENSOR.shape(512,).dtype(float16).stop_gradient(False)
    var fc_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512).dtype(float16).stop_gradient(False)
    var fc_0.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512).dtype(float16).stop_gradient(False)
    var fc_0.w_0@GRAD : LOD_TENSOR.shape(25088, 512).dtype(float16).stop_gradient(False)
    var gather_0.tmp_0.cast_fp32@GRAD : LOD_TENSOR.shape(512, 31).dtype(float32).stop_gradient(False)
    var gather_0.tmp_0.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(512, 31).dtype(float32).stop_gradient(False)
    var gather_0.tmp_0.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(512, 31).dtype(float32).stop_gradient(False)
    var gather_0.tmp_0@GRAD : LOD_TENSOR.shape(512, 31).dtype(float16).stop_gradient(False)
    var margin_cross_entropy_0.tmp_1@GRAD : LOD_TENSOR.shape(-1, 1).dtype(float32).stop_gradient(False)
    var matmul_v2_0.tmp_0.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 31).dtype(float32).stop_gradient(False)
    var matmul_v2_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 31).dtype(float16).stop_gradient(False)
    var mean_0.tmp_0@GRAD : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var p_norm_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 1).dtype(float32).stop_gradient(False)
    var p_norm_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 31).dtype(float32).stop_gradient(False)
    var prelu_0.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    var prelu_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float32).stop_gradient(False)
    var prelu_0.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float32).stop_gradient(False)
    var prelu_0.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float32).stop_gradient(False)
    var prelu_0.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_0.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_1.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float16).stop_gradient(False)
    var prelu_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 64, 112, 112).dtype(float32).stop_gradient(False)
    var prelu_1.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_1.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_10.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_10.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_10.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_11.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_11.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_11.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_12.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_12.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_12.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_13.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_13.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_13.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_13.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_14.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_14.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_14.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_14.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_15.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_15.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_15.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_15.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_16.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_16.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_16.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_16.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_17.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_17.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_17.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_17.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_18.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_18.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_18.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_18.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_19.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_19.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_19.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_19.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_2.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var prelu_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float32).stop_gradient(False)
    var prelu_2.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_2.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_20.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_20.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_20.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_20.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_21.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_21.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_21.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_21.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_22.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_22.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_22.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_22.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_23.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var prelu_23.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float32).stop_gradient(False)
    var prelu_23.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_23.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_24.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float16).stop_gradient(False)
    var prelu_24.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 7, 7).dtype(float32).stop_gradient(False)
    var prelu_24.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_24.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_3.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float16).stop_gradient(False)
    var prelu_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 64, 56, 56).dtype(float32).stop_gradient(False)
    var prelu_3.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_3.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_4.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 128, 56, 56).dtype(float16).stop_gradient(False)
    var prelu_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 128, 56, 56).dtype(float32).stop_gradient(False)
    var prelu_4.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_4.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_5.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var prelu_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float32).stop_gradient(False)
    var prelu_5.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_5.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_6.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var prelu_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float32).stop_gradient(False)
    var prelu_6.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_6.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_7.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float16).stop_gradient(False)
    var prelu_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 128, 28, 28).dtype(float32).stop_gradient(False)
    var prelu_7.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_7.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_8.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 28, 28).dtype(float16).stop_gradient(False)
    var prelu_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 28, 28).dtype(float32).stop_gradient(False)
    var prelu_8.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_8.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var prelu_9.tmp_0.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float16).stop_gradient(False)
    var prelu_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 256, 14, 14).dtype(float32).stop_gradient(False)
    var prelu_9.w_0.cast_fp32@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var prelu_9.w_0@GRAD : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var tmp_0@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var find_infinite_scale.tmp_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    persist var learning_rate_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(True)
    persist var batch_norm_0.b_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_0.w_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_1.b_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_1.w_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_10.b_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_10.w_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_11.b_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_11.w_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_12.b_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_12.w_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_13.b_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_13.w_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_14.b_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_14.w_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_15.b_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_15.w_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_16.b_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_16.w_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_17.b_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_17.w_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_18.b_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_18.w_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_19.b_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_19.w_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_2.b_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_2.w_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_20.b_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_20.w_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_21.b_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_21.w_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_22.b_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_22.w_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_23.b_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_23.w_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_24.b_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_24.w_0_velocity_0 : LOD_TENSOR.shape(128,).dtype(float32).stop_gradient(False)
    persist var batch_norm_25.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_25.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_26.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_26.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_27.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_27.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_28.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_28.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_29.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_29.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_3.b_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_3.w_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_30.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_30.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_31.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_31.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_32.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_32.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_33.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_33.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_34.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_34.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_35.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_35.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_36.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_36.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_37.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_37.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_38.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_38.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_39.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_39.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_4.b_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_4.w_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_40.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_40.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_41.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_41.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_42.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_42.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_43.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_43.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_44.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_44.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_45.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_45.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_46.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_46.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_47.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_47.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_48.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_48.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_49.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_49.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_5.b_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_5.w_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_50.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_50.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_51.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_51.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_52.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_52.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_53.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_53.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_54.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_54.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_55.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_55.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_56.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_56.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_57.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_57.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_58.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_58.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_59.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_59.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_6.b_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_6.w_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_60.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_60.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_61.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_61.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_62.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_62.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_63.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_63.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_64.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_64.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_65.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_65.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_66.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_66.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_67.b_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_67.w_0_velocity_0 : LOD_TENSOR.shape(256,).dtype(float32).stop_gradient(False)
    persist var batch_norm_68.b_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_68.w_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_69.b_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_69.w_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_7.b_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_7.w_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_70.b_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_70.w_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_71.b_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_71.w_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_72.b_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_72.w_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_73.b_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_73.w_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_74.b_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_74.w_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_75.b_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_75.w_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_76.b_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_76.w_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_77.b_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_77.w_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_78.b_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_78.w_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var batch_norm_8.b_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_8.w_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_9.b_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var batch_norm_9.w_0_velocity_0 : LOD_TENSOR.shape(64,).dtype(float32).stop_gradient(False)
    persist var conv2d_0.w_0_fp32_master_0 : LOD_TENSOR.shape(64, 3, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_0.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(64, 3, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_1.w_0_fp32_master_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_1.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_10.w_0_fp32_master_0 : LOD_TENSOR.shape(128, 64, 1, 1).dtype(float32).stop_gradient(True)
    persist var conv2d_10.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(128, 64, 1, 1).dtype(float32).stop_gradient(False)
    persist var conv2d_11.w_0_fp32_master_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_11.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_12.w_0_fp32_master_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_12.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_13.w_0_fp32_master_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_13.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_14.w_0_fp32_master_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_14.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_15.w_0_fp32_master_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_15.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_16.w_0_fp32_master_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_16.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_17.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 128, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_17.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 128, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_18.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_18.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_19.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 128, 1, 1).dtype(float32).stop_gradient(True)
    persist var conv2d_19.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 128, 1, 1).dtype(float32).stop_gradient(False)
    persist var conv2d_2.w_0_fp32_master_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_2.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_20.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_20.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_21.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_21.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_22.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_22.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_23.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_23.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_24.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_24.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_25.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_25.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_26.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_26.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_27.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_27.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_28.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_28.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_29.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_29.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_3.w_0_fp32_master_0 : LOD_TENSOR.shape(64, 64, 1, 1).dtype(float32).stop_gradient(True)
    persist var conv2d_3.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(64, 64, 1, 1).dtype(float32).stop_gradient(False)
    persist var conv2d_30.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_30.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_31.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_31.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_32.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_32.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_33.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_33.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_34.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_34.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_35.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_35.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_36.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_36.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_37.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_37.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_38.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_38.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_39.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_39.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_4.w_0_fp32_master_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_4.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_40.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_40.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_41.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_41.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_42.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_42.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_43.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_43.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_44.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_44.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_45.w_0_fp32_master_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_45.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(256, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_46.w_0_fp32_master_0 : LOD_TENSOR.shape(512, 256, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_46.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(512, 256, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_47.w_0_fp32_master_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_47.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_48.w_0_fp32_master_0 : LOD_TENSOR.shape(512, 256, 1, 1).dtype(float32).stop_gradient(True)
    persist var conv2d_48.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(512, 256, 1, 1).dtype(float32).stop_gradient(False)
    persist var conv2d_49.w_0_fp32_master_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_49.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_5.w_0_fp32_master_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_5.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_50.w_0_fp32_master_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_50.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_51.w_0_fp32_master_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_51.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_52.w_0_fp32_master_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_52.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(512, 512, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_6.w_0_fp32_master_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_6.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_7.w_0_fp32_master_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_7.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(64, 64, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_8.w_0_fp32_master_0 : LOD_TENSOR.shape(128, 64, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_8.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(128, 64, 3, 3).dtype(float32).stop_gradient(False)
    persist var conv2d_9.w_0_fp32_master_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(True)
    persist var conv2d_9.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(128, 128, 3, 3).dtype(float32).stop_gradient(False)
    persist var dist@fc@rank@00000.w.w_0_fp32_master_0 : LOD_TENSOR.shape(512, 312).dtype(float32).stop_gradient(True)
    persist var dist@fc@rank@00000.w.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(512, 312).dtype(float32).stop_gradient(False)
    persist var fc_0.b_0_fp32_master_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(True)
    persist var fc_0.b_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(512,).dtype(float32).stop_gradient(False)
    persist var fc_0.w_0_fp32_master_0 : LOD_TENSOR.shape(25088, 512).dtype(float32).stop_gradient(True)
    persist var fc_0.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(25088, 512).dtype(float32).stop_gradient(False)
    persist var prelu_0.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_0.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_1.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_1.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_10.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_10.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_11.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_11.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_12.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_12.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_13.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_13.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_14.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_14.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_15.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_15.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_16.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_16.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_17.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_17.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_18.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_18.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_19.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_19.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_2.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_2.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_20.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_20.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_21.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_21.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_22.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_22.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_23.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_23.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_24.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_24.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_3.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_3.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_4.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_4.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_5.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_5.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_6.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_6.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_7.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_7.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_8.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_8.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var prelu_9.w_0_fp32_master_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var prelu_9.w_0_fp32_master_0_velocity_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)

    {Output=['conv2d_0.tmp_0']} = conv2d(inputs={Filter=['conv2d_0.w_0'], Input=['image']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_0.w_1'], ReserveSpace=['batch_norm_0.tmp_2'], SavedMean=['batch_norm_0.tmp_0'], SavedVariance=['batch_norm_0.tmp_1'], VarianceOut=['batch_norm_0.w_2'], Y=['batch_norm_0.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_0.b_0'], Mean=['batch_norm_0.w_1'], MomentumTensor=[], Scale=['batch_norm_0.w_0'], Variance=['batch_norm_0.w_2'], X=['conv2d_0.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_0.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_0.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_0.w_0.cast_fp32']} = cast(inputs={X=['prelu_0.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_0.tmp_0']} = prelu(inputs={Alpha=['prelu_0.w_0.cast_fp32'], X=['batch_norm_0.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_1.w_1'], ReserveSpace=['batch_norm_1.tmp_2'], SavedMean=['batch_norm_1.tmp_0'], SavedVariance=['batch_norm_1.tmp_1'], VarianceOut=['batch_norm_1.w_2'], Y=['batch_norm_1.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_1.b_0'], Mean=['batch_norm_1.w_1'], MomentumTensor=[], Scale=['batch_norm_1.w_0'], Variance=['batch_norm_1.w_2'], X=['prelu_0.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_1.tmp_3.cast_fp16']} = cast(inputs={X=['batch_norm_1.tmp_3']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_1.tmp_0']} = conv2d(inputs={Filter=['conv2d_1.w_0'], Input=['batch_norm_1.tmp_3.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_2.w_1'], ReserveSpace=['batch_norm_2.tmp_2'], SavedMean=['batch_norm_2.tmp_0'], SavedVariance=['batch_norm_2.tmp_1'], VarianceOut=['batch_norm_2.w_2'], Y=['batch_norm_2.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_2.b_0'], Mean=['batch_norm_2.w_1'], MomentumTensor=[], Scale=['batch_norm_2.w_0'], Variance=['batch_norm_2.w_2'], X=['conv2d_1.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_2.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_2.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_1.w_0.cast_fp32']} = cast(inputs={X=['prelu_1.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_1.tmp_0']} = prelu(inputs={Alpha=['prelu_1.w_0.cast_fp32'], X=['batch_norm_2.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_1.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_1.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_2.tmp_0']} = conv2d(inputs={Filter=['conv2d_2.w_0'], Input=['prelu_1.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [2, 2], with_quant_attr = False)
    {MeanOut=['batch_norm_3.w_1'], ReserveSpace=['batch_norm_3.tmp_2'], SavedMean=['batch_norm_3.tmp_0'], SavedVariance=['batch_norm_3.tmp_1'], VarianceOut=['batch_norm_3.w_2'], Y=['batch_norm_3.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_3.b_0'], Mean=['batch_norm_3.w_1'], MomentumTensor=[], Scale=['batch_norm_3.w_0'], Variance=['batch_norm_3.w_2'], X=['conv2d_2.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['prelu_0.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_0.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_3.tmp_0']} = conv2d(inputs={Filter=['conv2d_3.w_0'], Input=['prelu_0.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [0, 0], strides = [2, 2], with_quant_attr = False)
    {MeanOut=['batch_norm_4.w_1'], ReserveSpace=['batch_norm_4.tmp_2'], SavedMean=['batch_norm_4.tmp_0'], SavedVariance=['batch_norm_4.tmp_1'], VarianceOut=['batch_norm_4.w_2'], Y=['batch_norm_4.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_4.b_0'], Mean=['batch_norm_4.w_1'], MomentumTensor=[], Scale=['batch_norm_4.w_0'], Variance=['batch_norm_4.w_2'], X=['conv2d_3.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_0']} = elementwise_add(inputs={X=['batch_norm_3.tmp_3'], Y=['batch_norm_4.tmp_3']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_5.w_1'], ReserveSpace=['batch_norm_5.tmp_2'], SavedMean=['batch_norm_5.tmp_0'], SavedVariance=['batch_norm_5.tmp_1'], VarianceOut=['batch_norm_5.w_2'], Y=['batch_norm_5.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_5.b_0'], Mean=['batch_norm_5.w_1'], MomentumTensor=[], Scale=['batch_norm_5.w_0'], Variance=['batch_norm_5.w_2'], X=['elementwise_add_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_4.tmp_0']} = conv2d(inputs={Filter=['conv2d_4.w_0'], Input=['batch_norm_5.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_6.w_1'], ReserveSpace=['batch_norm_6.tmp_2'], SavedMean=['batch_norm_6.tmp_0'], SavedVariance=['batch_norm_6.tmp_1'], VarianceOut=['batch_norm_6.w_2'], Y=['batch_norm_6.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_6.b_0'], Mean=['batch_norm_6.w_1'], MomentumTensor=[], Scale=['batch_norm_6.w_0'], Variance=['batch_norm_6.w_2'], X=['conv2d_4.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_6.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_6.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_2.w_0.cast_fp32']} = cast(inputs={X=['prelu_2.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_2.tmp_0']} = prelu(inputs={Alpha=['prelu_2.w_0.cast_fp32'], X=['batch_norm_6.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_2.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_2.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_5.tmp_0']} = conv2d(inputs={Filter=['conv2d_5.w_0'], Input=['prelu_2.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_7.w_1'], ReserveSpace=['batch_norm_7.tmp_2'], SavedMean=['batch_norm_7.tmp_0'], SavedVariance=['batch_norm_7.tmp_1'], VarianceOut=['batch_norm_7.w_2'], Y=['batch_norm_7.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_7.b_0'], Mean=['batch_norm_7.w_1'], MomentumTensor=[], Scale=['batch_norm_7.w_0'], Variance=['batch_norm_7.w_2'], X=['conv2d_5.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_1']} = elementwise_add(inputs={X=['batch_norm_7.tmp_3'], Y=['elementwise_add_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_8.w_1'], ReserveSpace=['batch_norm_8.tmp_2'], SavedMean=['batch_norm_8.tmp_0'], SavedVariance=['batch_norm_8.tmp_1'], VarianceOut=['batch_norm_8.w_2'], Y=['batch_norm_8.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_8.b_0'], Mean=['batch_norm_8.w_1'], MomentumTensor=[], Scale=['batch_norm_8.w_0'], Variance=['batch_norm_8.w_2'], X=['elementwise_add_1']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_6.tmp_0']} = conv2d(inputs={Filter=['conv2d_6.w_0'], Input=['batch_norm_8.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_9.w_1'], ReserveSpace=['batch_norm_9.tmp_2'], SavedMean=['batch_norm_9.tmp_0'], SavedVariance=['batch_norm_9.tmp_1'], VarianceOut=['batch_norm_9.w_2'], Y=['batch_norm_9.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_9.b_0'], Mean=['batch_norm_9.w_1'], MomentumTensor=[], Scale=['batch_norm_9.w_0'], Variance=['batch_norm_9.w_2'], X=['conv2d_6.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_9.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_9.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_3.w_0.cast_fp32']} = cast(inputs={X=['prelu_3.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_3.tmp_0']} = prelu(inputs={Alpha=['prelu_3.w_0.cast_fp32'], X=['batch_norm_9.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_3.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_3.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_7.tmp_0']} = conv2d(inputs={Filter=['conv2d_7.w_0'], Input=['prelu_3.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_10.w_1'], ReserveSpace=['batch_norm_10.tmp_2'], SavedMean=['batch_norm_10.tmp_0'], SavedVariance=['batch_norm_10.tmp_1'], VarianceOut=['batch_norm_10.w_2'], Y=['batch_norm_10.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_10.b_0'], Mean=['batch_norm_10.w_1'], MomentumTensor=[], Scale=['batch_norm_10.w_0'], Variance=['batch_norm_10.w_2'], X=['conv2d_7.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_2']} = elementwise_add(inputs={X=['batch_norm_10.tmp_3'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_11.w_1'], ReserveSpace=['batch_norm_11.tmp_2'], SavedMean=['batch_norm_11.tmp_0'], SavedVariance=['batch_norm_11.tmp_1'], VarianceOut=['batch_norm_11.w_2'], Y=['batch_norm_11.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_11.b_0'], Mean=['batch_norm_11.w_1'], MomentumTensor=[], Scale=['batch_norm_11.w_0'], Variance=['batch_norm_11.w_2'], X=['elementwise_add_2']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_8.tmp_0']} = conv2d(inputs={Filter=['conv2d_8.w_0'], Input=['batch_norm_11.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_12.w_1'], ReserveSpace=['batch_norm_12.tmp_2'], SavedMean=['batch_norm_12.tmp_0'], SavedVariance=['batch_norm_12.tmp_1'], VarianceOut=['batch_norm_12.w_2'], Y=['batch_norm_12.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_12.b_0'], Mean=['batch_norm_12.w_1'], MomentumTensor=[], Scale=['batch_norm_12.w_0'], Variance=['batch_norm_12.w_2'], X=['conv2d_8.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_12.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_12.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_4.w_0.cast_fp32']} = cast(inputs={X=['prelu_4.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_4.tmp_0']} = prelu(inputs={Alpha=['prelu_4.w_0.cast_fp32'], X=['batch_norm_12.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_4.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_4.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_9.tmp_0']} = conv2d(inputs={Filter=['conv2d_9.w_0'], Input=['prelu_4.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [2, 2], with_quant_attr = False)
    {MeanOut=['batch_norm_13.w_1'], ReserveSpace=['batch_norm_13.tmp_2'], SavedMean=['batch_norm_13.tmp_0'], SavedVariance=['batch_norm_13.tmp_1'], VarianceOut=['batch_norm_13.w_2'], Y=['batch_norm_13.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_13.b_0'], Mean=['batch_norm_13.w_1'], MomentumTensor=[], Scale=['batch_norm_13.w_0'], Variance=['batch_norm_13.w_2'], X=['conv2d_9.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_10.tmp_0']} = conv2d(inputs={Filter=['conv2d_10.w_0'], Input=['elementwise_add_2']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [0, 0], strides = [2, 2], with_quant_attr = False)
    {MeanOut=['batch_norm_14.w_1'], ReserveSpace=['batch_norm_14.tmp_2'], SavedMean=['batch_norm_14.tmp_0'], SavedVariance=['batch_norm_14.tmp_1'], VarianceOut=['batch_norm_14.w_2'], Y=['batch_norm_14.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_14.b_0'], Mean=['batch_norm_14.w_1'], MomentumTensor=[], Scale=['batch_norm_14.w_0'], Variance=['batch_norm_14.w_2'], X=['conv2d_10.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_3']} = elementwise_add(inputs={X=['batch_norm_13.tmp_3'], Y=['batch_norm_14.tmp_3']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_15.w_1'], ReserveSpace=['batch_norm_15.tmp_2'], SavedMean=['batch_norm_15.tmp_0'], SavedVariance=['batch_norm_15.tmp_1'], VarianceOut=['batch_norm_15.w_2'], Y=['batch_norm_15.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_15.b_0'], Mean=['batch_norm_15.w_1'], MomentumTensor=[], Scale=['batch_norm_15.w_0'], Variance=['batch_norm_15.w_2'], X=['elementwise_add_3']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_11.tmp_0']} = conv2d(inputs={Filter=['conv2d_11.w_0'], Input=['batch_norm_15.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_16.w_1'], ReserveSpace=['batch_norm_16.tmp_2'], SavedMean=['batch_norm_16.tmp_0'], SavedVariance=['batch_norm_16.tmp_1'], VarianceOut=['batch_norm_16.w_2'], Y=['batch_norm_16.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_16.b_0'], Mean=['batch_norm_16.w_1'], MomentumTensor=[], Scale=['batch_norm_16.w_0'], Variance=['batch_norm_16.w_2'], X=['conv2d_11.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_16.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_16.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_5.w_0.cast_fp32']} = cast(inputs={X=['prelu_5.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_5.tmp_0']} = prelu(inputs={Alpha=['prelu_5.w_0.cast_fp32'], X=['batch_norm_16.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_5.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_5.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_12.tmp_0']} = conv2d(inputs={Filter=['conv2d_12.w_0'], Input=['prelu_5.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_17.w_1'], ReserveSpace=['batch_norm_17.tmp_2'], SavedMean=['batch_norm_17.tmp_0'], SavedVariance=['batch_norm_17.tmp_1'], VarianceOut=['batch_norm_17.w_2'], Y=['batch_norm_17.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_17.b_0'], Mean=['batch_norm_17.w_1'], MomentumTensor=[], Scale=['batch_norm_17.w_0'], Variance=['batch_norm_17.w_2'], X=['conv2d_12.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_4']} = elementwise_add(inputs={X=['batch_norm_17.tmp_3'], Y=['elementwise_add_3']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_18.w_1'], ReserveSpace=['batch_norm_18.tmp_2'], SavedMean=['batch_norm_18.tmp_0'], SavedVariance=['batch_norm_18.tmp_1'], VarianceOut=['batch_norm_18.w_2'], Y=['batch_norm_18.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_18.b_0'], Mean=['batch_norm_18.w_1'], MomentumTensor=[], Scale=['batch_norm_18.w_0'], Variance=['batch_norm_18.w_2'], X=['elementwise_add_4']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_13.tmp_0']} = conv2d(inputs={Filter=['conv2d_13.w_0'], Input=['batch_norm_18.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_19.w_1'], ReserveSpace=['batch_norm_19.tmp_2'], SavedMean=['batch_norm_19.tmp_0'], SavedVariance=['batch_norm_19.tmp_1'], VarianceOut=['batch_norm_19.w_2'], Y=['batch_norm_19.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_19.b_0'], Mean=['batch_norm_19.w_1'], MomentumTensor=[], Scale=['batch_norm_19.w_0'], Variance=['batch_norm_19.w_2'], X=['conv2d_13.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_19.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_19.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_6.w_0.cast_fp32']} = cast(inputs={X=['prelu_6.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_6.tmp_0']} = prelu(inputs={Alpha=['prelu_6.w_0.cast_fp32'], X=['batch_norm_19.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_6.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_6.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_14.tmp_0']} = conv2d(inputs={Filter=['conv2d_14.w_0'], Input=['prelu_6.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_20.w_1'], ReserveSpace=['batch_norm_20.tmp_2'], SavedMean=['batch_norm_20.tmp_0'], SavedVariance=['batch_norm_20.tmp_1'], VarianceOut=['batch_norm_20.w_2'], Y=['batch_norm_20.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_20.b_0'], Mean=['batch_norm_20.w_1'], MomentumTensor=[], Scale=['batch_norm_20.w_0'], Variance=['batch_norm_20.w_2'], X=['conv2d_14.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_5']} = elementwise_add(inputs={X=['batch_norm_20.tmp_3'], Y=['elementwise_add_4']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_21.w_1'], ReserveSpace=['batch_norm_21.tmp_2'], SavedMean=['batch_norm_21.tmp_0'], SavedVariance=['batch_norm_21.tmp_1'], VarianceOut=['batch_norm_21.w_2'], Y=['batch_norm_21.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_21.b_0'], Mean=['batch_norm_21.w_1'], MomentumTensor=[], Scale=['batch_norm_21.w_0'], Variance=['batch_norm_21.w_2'], X=['elementwise_add_5']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_15.tmp_0']} = conv2d(inputs={Filter=['conv2d_15.w_0'], Input=['batch_norm_21.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_22.w_1'], ReserveSpace=['batch_norm_22.tmp_2'], SavedMean=['batch_norm_22.tmp_0'], SavedVariance=['batch_norm_22.tmp_1'], VarianceOut=['batch_norm_22.w_2'], Y=['batch_norm_22.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_22.b_0'], Mean=['batch_norm_22.w_1'], MomentumTensor=[], Scale=['batch_norm_22.w_0'], Variance=['batch_norm_22.w_2'], X=['conv2d_15.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_22.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_22.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_7.w_0.cast_fp32']} = cast(inputs={X=['prelu_7.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_7.tmp_0']} = prelu(inputs={Alpha=['prelu_7.w_0.cast_fp32'], X=['batch_norm_22.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_7.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_7.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_16.tmp_0']} = conv2d(inputs={Filter=['conv2d_16.w_0'], Input=['prelu_7.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_23.w_1'], ReserveSpace=['batch_norm_23.tmp_2'], SavedMean=['batch_norm_23.tmp_0'], SavedVariance=['batch_norm_23.tmp_1'], VarianceOut=['batch_norm_23.w_2'], Y=['batch_norm_23.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_23.b_0'], Mean=['batch_norm_23.w_1'], MomentumTensor=[], Scale=['batch_norm_23.w_0'], Variance=['batch_norm_23.w_2'], X=['conv2d_16.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_6']} = elementwise_add(inputs={X=['batch_norm_23.tmp_3'], Y=['elementwise_add_5']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_24.w_1'], ReserveSpace=['batch_norm_24.tmp_2'], SavedMean=['batch_norm_24.tmp_0'], SavedVariance=['batch_norm_24.tmp_1'], VarianceOut=['batch_norm_24.w_2'], Y=['batch_norm_24.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_24.b_0'], Mean=['batch_norm_24.w_1'], MomentumTensor=[], Scale=['batch_norm_24.w_0'], Variance=['batch_norm_24.w_2'], X=['elementwise_add_6']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_17.tmp_0']} = conv2d(inputs={Filter=['conv2d_17.w_0'], Input=['batch_norm_24.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_25.w_1'], ReserveSpace=['batch_norm_25.tmp_2'], SavedMean=['batch_norm_25.tmp_0'], SavedVariance=['batch_norm_25.tmp_1'], VarianceOut=['batch_norm_25.w_2'], Y=['batch_norm_25.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_25.b_0'], Mean=['batch_norm_25.w_1'], MomentumTensor=[], Scale=['batch_norm_25.w_0'], Variance=['batch_norm_25.w_2'], X=['conv2d_17.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_25.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_25.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_8.w_0.cast_fp32']} = cast(inputs={X=['prelu_8.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_8.tmp_0']} = prelu(inputs={Alpha=['prelu_8.w_0.cast_fp32'], X=['batch_norm_25.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_8.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_8.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_18.tmp_0']} = conv2d(inputs={Filter=['conv2d_18.w_0'], Input=['prelu_8.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [2, 2], with_quant_attr = False)
    {MeanOut=['batch_norm_26.w_1'], ReserveSpace=['batch_norm_26.tmp_2'], SavedMean=['batch_norm_26.tmp_0'], SavedVariance=['batch_norm_26.tmp_1'], VarianceOut=['batch_norm_26.w_2'], Y=['batch_norm_26.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_26.b_0'], Mean=['batch_norm_26.w_1'], MomentumTensor=[], Scale=['batch_norm_26.w_0'], Variance=['batch_norm_26.w_2'], X=['conv2d_18.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_19.tmp_0']} = conv2d(inputs={Filter=['conv2d_19.w_0'], Input=['elementwise_add_6']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [0, 0], strides = [2, 2], with_quant_attr = False)
    {MeanOut=['batch_norm_27.w_1'], ReserveSpace=['batch_norm_27.tmp_2'], SavedMean=['batch_norm_27.tmp_0'], SavedVariance=['batch_norm_27.tmp_1'], VarianceOut=['batch_norm_27.w_2'], Y=['batch_norm_27.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_27.b_0'], Mean=['batch_norm_27.w_1'], MomentumTensor=[], Scale=['batch_norm_27.w_0'], Variance=['batch_norm_27.w_2'], X=['conv2d_19.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_7']} = elementwise_add(inputs={X=['batch_norm_26.tmp_3'], Y=['batch_norm_27.tmp_3']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_28.w_1'], ReserveSpace=['batch_norm_28.tmp_2'], SavedMean=['batch_norm_28.tmp_0'], SavedVariance=['batch_norm_28.tmp_1'], VarianceOut=['batch_norm_28.w_2'], Y=['batch_norm_28.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_28.b_0'], Mean=['batch_norm_28.w_1'], MomentumTensor=[], Scale=['batch_norm_28.w_0'], Variance=['batch_norm_28.w_2'], X=['elementwise_add_7']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_20.tmp_0']} = conv2d(inputs={Filter=['conv2d_20.w_0'], Input=['batch_norm_28.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_29.w_1'], ReserveSpace=['batch_norm_29.tmp_2'], SavedMean=['batch_norm_29.tmp_0'], SavedVariance=['batch_norm_29.tmp_1'], VarianceOut=['batch_norm_29.w_2'], Y=['batch_norm_29.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_29.b_0'], Mean=['batch_norm_29.w_1'], MomentumTensor=[], Scale=['batch_norm_29.w_0'], Variance=['batch_norm_29.w_2'], X=['conv2d_20.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_29.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_29.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_9.w_0.cast_fp32']} = cast(inputs={X=['prelu_9.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_9.tmp_0']} = prelu(inputs={Alpha=['prelu_9.w_0.cast_fp32'], X=['batch_norm_29.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_9.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_9.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_21.tmp_0']} = conv2d(inputs={Filter=['conv2d_21.w_0'], Input=['prelu_9.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_30.w_1'], ReserveSpace=['batch_norm_30.tmp_2'], SavedMean=['batch_norm_30.tmp_0'], SavedVariance=['batch_norm_30.tmp_1'], VarianceOut=['batch_norm_30.w_2'], Y=['batch_norm_30.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_30.b_0'], Mean=['batch_norm_30.w_1'], MomentumTensor=[], Scale=['batch_norm_30.w_0'], Variance=['batch_norm_30.w_2'], X=['conv2d_21.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_8']} = elementwise_add(inputs={X=['batch_norm_30.tmp_3'], Y=['elementwise_add_7']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_31.w_1'], ReserveSpace=['batch_norm_31.tmp_2'], SavedMean=['batch_norm_31.tmp_0'], SavedVariance=['batch_norm_31.tmp_1'], VarianceOut=['batch_norm_31.w_2'], Y=['batch_norm_31.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_31.b_0'], Mean=['batch_norm_31.w_1'], MomentumTensor=[], Scale=['batch_norm_31.w_0'], Variance=['batch_norm_31.w_2'], X=['elementwise_add_8']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_22.tmp_0']} = conv2d(inputs={Filter=['conv2d_22.w_0'], Input=['batch_norm_31.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_32.w_1'], ReserveSpace=['batch_norm_32.tmp_2'], SavedMean=['batch_norm_32.tmp_0'], SavedVariance=['batch_norm_32.tmp_1'], VarianceOut=['batch_norm_32.w_2'], Y=['batch_norm_32.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_32.b_0'], Mean=['batch_norm_32.w_1'], MomentumTensor=[], Scale=['batch_norm_32.w_0'], Variance=['batch_norm_32.w_2'], X=['conv2d_22.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_32.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_32.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_10.w_0.cast_fp32']} = cast(inputs={X=['prelu_10.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_10.tmp_0']} = prelu(inputs={Alpha=['prelu_10.w_0.cast_fp32'], X=['batch_norm_32.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_10.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_10.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_23.tmp_0']} = conv2d(inputs={Filter=['conv2d_23.w_0'], Input=['prelu_10.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_33.w_1'], ReserveSpace=['batch_norm_33.tmp_2'], SavedMean=['batch_norm_33.tmp_0'], SavedVariance=['batch_norm_33.tmp_1'], VarianceOut=['batch_norm_33.w_2'], Y=['batch_norm_33.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_33.b_0'], Mean=['batch_norm_33.w_1'], MomentumTensor=[], Scale=['batch_norm_33.w_0'], Variance=['batch_norm_33.w_2'], X=['conv2d_23.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_9']} = elementwise_add(inputs={X=['batch_norm_33.tmp_3'], Y=['elementwise_add_8']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_34.w_1'], ReserveSpace=['batch_norm_34.tmp_2'], SavedMean=['batch_norm_34.tmp_0'], SavedVariance=['batch_norm_34.tmp_1'], VarianceOut=['batch_norm_34.w_2'], Y=['batch_norm_34.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_34.b_0'], Mean=['batch_norm_34.w_1'], MomentumTensor=[], Scale=['batch_norm_34.w_0'], Variance=['batch_norm_34.w_2'], X=['elementwise_add_9']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_24.tmp_0']} = conv2d(inputs={Filter=['conv2d_24.w_0'], Input=['batch_norm_34.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_35.w_1'], ReserveSpace=['batch_norm_35.tmp_2'], SavedMean=['batch_norm_35.tmp_0'], SavedVariance=['batch_norm_35.tmp_1'], VarianceOut=['batch_norm_35.w_2'], Y=['batch_norm_35.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_35.b_0'], Mean=['batch_norm_35.w_1'], MomentumTensor=[], Scale=['batch_norm_35.w_0'], Variance=['batch_norm_35.w_2'], X=['conv2d_24.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_35.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_35.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_11.w_0.cast_fp32']} = cast(inputs={X=['prelu_11.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_11.tmp_0']} = prelu(inputs={Alpha=['prelu_11.w_0.cast_fp32'], X=['batch_norm_35.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_11.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_11.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_25.tmp_0']} = conv2d(inputs={Filter=['conv2d_25.w_0'], Input=['prelu_11.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_36.w_1'], ReserveSpace=['batch_norm_36.tmp_2'], SavedMean=['batch_norm_36.tmp_0'], SavedVariance=['batch_norm_36.tmp_1'], VarianceOut=['batch_norm_36.w_2'], Y=['batch_norm_36.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_36.b_0'], Mean=['batch_norm_36.w_1'], MomentumTensor=[], Scale=['batch_norm_36.w_0'], Variance=['batch_norm_36.w_2'], X=['conv2d_25.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_10']} = elementwise_add(inputs={X=['batch_norm_36.tmp_3'], Y=['elementwise_add_9']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_37.w_1'], ReserveSpace=['batch_norm_37.tmp_2'], SavedMean=['batch_norm_37.tmp_0'], SavedVariance=['batch_norm_37.tmp_1'], VarianceOut=['batch_norm_37.w_2'], Y=['batch_norm_37.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_37.b_0'], Mean=['batch_norm_37.w_1'], MomentumTensor=[], Scale=['batch_norm_37.w_0'], Variance=['batch_norm_37.w_2'], X=['elementwise_add_10']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_26.tmp_0']} = conv2d(inputs={Filter=['conv2d_26.w_0'], Input=['batch_norm_37.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_38.w_1'], ReserveSpace=['batch_norm_38.tmp_2'], SavedMean=['batch_norm_38.tmp_0'], SavedVariance=['batch_norm_38.tmp_1'], VarianceOut=['batch_norm_38.w_2'], Y=['batch_norm_38.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_38.b_0'], Mean=['batch_norm_38.w_1'], MomentumTensor=[], Scale=['batch_norm_38.w_0'], Variance=['batch_norm_38.w_2'], X=['conv2d_26.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_38.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_38.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_12.w_0.cast_fp32']} = cast(inputs={X=['prelu_12.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_12.tmp_0']} = prelu(inputs={Alpha=['prelu_12.w_0.cast_fp32'], X=['batch_norm_38.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_12.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_12.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_27.tmp_0']} = conv2d(inputs={Filter=['conv2d_27.w_0'], Input=['prelu_12.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_39.w_1'], ReserveSpace=['batch_norm_39.tmp_2'], SavedMean=['batch_norm_39.tmp_0'], SavedVariance=['batch_norm_39.tmp_1'], VarianceOut=['batch_norm_39.w_2'], Y=['batch_norm_39.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_39.b_0'], Mean=['batch_norm_39.w_1'], MomentumTensor=[], Scale=['batch_norm_39.w_0'], Variance=['batch_norm_39.w_2'], X=['conv2d_27.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_11']} = elementwise_add(inputs={X=['batch_norm_39.tmp_3'], Y=['elementwise_add_10']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_40.w_1'], ReserveSpace=['batch_norm_40.tmp_2'], SavedMean=['batch_norm_40.tmp_0'], SavedVariance=['batch_norm_40.tmp_1'], VarianceOut=['batch_norm_40.w_2'], Y=['batch_norm_40.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_40.b_0'], Mean=['batch_norm_40.w_1'], MomentumTensor=[], Scale=['batch_norm_40.w_0'], Variance=['batch_norm_40.w_2'], X=['elementwise_add_11']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_28.tmp_0']} = conv2d(inputs={Filter=['conv2d_28.w_0'], Input=['batch_norm_40.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_41.w_1'], ReserveSpace=['batch_norm_41.tmp_2'], SavedMean=['batch_norm_41.tmp_0'], SavedVariance=['batch_norm_41.tmp_1'], VarianceOut=['batch_norm_41.w_2'], Y=['batch_norm_41.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_41.b_0'], Mean=['batch_norm_41.w_1'], MomentumTensor=[], Scale=['batch_norm_41.w_0'], Variance=['batch_norm_41.w_2'], X=['conv2d_28.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_41.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_41.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_13.w_0.cast_fp32']} = cast(inputs={X=['prelu_13.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_13.tmp_0']} = prelu(inputs={Alpha=['prelu_13.w_0.cast_fp32'], X=['batch_norm_41.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_13.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_13.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_29.tmp_0']} = conv2d(inputs={Filter=['conv2d_29.w_0'], Input=['prelu_13.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_42.w_1'], ReserveSpace=['batch_norm_42.tmp_2'], SavedMean=['batch_norm_42.tmp_0'], SavedVariance=['batch_norm_42.tmp_1'], VarianceOut=['batch_norm_42.w_2'], Y=['batch_norm_42.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_42.b_0'], Mean=['batch_norm_42.w_1'], MomentumTensor=[], Scale=['batch_norm_42.w_0'], Variance=['batch_norm_42.w_2'], X=['conv2d_29.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_12']} = elementwise_add(inputs={X=['batch_norm_42.tmp_3'], Y=['elementwise_add_11']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_43.w_1'], ReserveSpace=['batch_norm_43.tmp_2'], SavedMean=['batch_norm_43.tmp_0'], SavedVariance=['batch_norm_43.tmp_1'], VarianceOut=['batch_norm_43.w_2'], Y=['batch_norm_43.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_43.b_0'], Mean=['batch_norm_43.w_1'], MomentumTensor=[], Scale=['batch_norm_43.w_0'], Variance=['batch_norm_43.w_2'], X=['elementwise_add_12']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_30.tmp_0']} = conv2d(inputs={Filter=['conv2d_30.w_0'], Input=['batch_norm_43.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_44.w_1'], ReserveSpace=['batch_norm_44.tmp_2'], SavedMean=['batch_norm_44.tmp_0'], SavedVariance=['batch_norm_44.tmp_1'], VarianceOut=['batch_norm_44.w_2'], Y=['batch_norm_44.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_44.b_0'], Mean=['batch_norm_44.w_1'], MomentumTensor=[], Scale=['batch_norm_44.w_0'], Variance=['batch_norm_44.w_2'], X=['conv2d_30.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_44.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_44.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_14.w_0.cast_fp32']} = cast(inputs={X=['prelu_14.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_14.tmp_0']} = prelu(inputs={Alpha=['prelu_14.w_0.cast_fp32'], X=['batch_norm_44.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_14.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_14.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_31.tmp_0']} = conv2d(inputs={Filter=['conv2d_31.w_0'], Input=['prelu_14.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_45.w_1'], ReserveSpace=['batch_norm_45.tmp_2'], SavedMean=['batch_norm_45.tmp_0'], SavedVariance=['batch_norm_45.tmp_1'], VarianceOut=['batch_norm_45.w_2'], Y=['batch_norm_45.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_45.b_0'], Mean=['batch_norm_45.w_1'], MomentumTensor=[], Scale=['batch_norm_45.w_0'], Variance=['batch_norm_45.w_2'], X=['conv2d_31.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_13']} = elementwise_add(inputs={X=['batch_norm_45.tmp_3'], Y=['elementwise_add_12']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_46.w_1'], ReserveSpace=['batch_norm_46.tmp_2'], SavedMean=['batch_norm_46.tmp_0'], SavedVariance=['batch_norm_46.tmp_1'], VarianceOut=['batch_norm_46.w_2'], Y=['batch_norm_46.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_46.b_0'], Mean=['batch_norm_46.w_1'], MomentumTensor=[], Scale=['batch_norm_46.w_0'], Variance=['batch_norm_46.w_2'], X=['elementwise_add_13']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_32.tmp_0']} = conv2d(inputs={Filter=['conv2d_32.w_0'], Input=['batch_norm_46.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_47.w_1'], ReserveSpace=['batch_norm_47.tmp_2'], SavedMean=['batch_norm_47.tmp_0'], SavedVariance=['batch_norm_47.tmp_1'], VarianceOut=['batch_norm_47.w_2'], Y=['batch_norm_47.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_47.b_0'], Mean=['batch_norm_47.w_1'], MomentumTensor=[], Scale=['batch_norm_47.w_0'], Variance=['batch_norm_47.w_2'], X=['conv2d_32.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_47.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_47.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_15.w_0.cast_fp32']} = cast(inputs={X=['prelu_15.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_15.tmp_0']} = prelu(inputs={Alpha=['prelu_15.w_0.cast_fp32'], X=['batch_norm_47.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_15.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_15.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_33.tmp_0']} = conv2d(inputs={Filter=['conv2d_33.w_0'], Input=['prelu_15.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_48.w_1'], ReserveSpace=['batch_norm_48.tmp_2'], SavedMean=['batch_norm_48.tmp_0'], SavedVariance=['batch_norm_48.tmp_1'], VarianceOut=['batch_norm_48.w_2'], Y=['batch_norm_48.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_48.b_0'], Mean=['batch_norm_48.w_1'], MomentumTensor=[], Scale=['batch_norm_48.w_0'], Variance=['batch_norm_48.w_2'], X=['conv2d_33.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_14']} = elementwise_add(inputs={X=['batch_norm_48.tmp_3'], Y=['elementwise_add_13']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_49.w_1'], ReserveSpace=['batch_norm_49.tmp_2'], SavedMean=['batch_norm_49.tmp_0'], SavedVariance=['batch_norm_49.tmp_1'], VarianceOut=['batch_norm_49.w_2'], Y=['batch_norm_49.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_49.b_0'], Mean=['batch_norm_49.w_1'], MomentumTensor=[], Scale=['batch_norm_49.w_0'], Variance=['batch_norm_49.w_2'], X=['elementwise_add_14']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_34.tmp_0']} = conv2d(inputs={Filter=['conv2d_34.w_0'], Input=['batch_norm_49.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_50.w_1'], ReserveSpace=['batch_norm_50.tmp_2'], SavedMean=['batch_norm_50.tmp_0'], SavedVariance=['batch_norm_50.tmp_1'], VarianceOut=['batch_norm_50.w_2'], Y=['batch_norm_50.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_50.b_0'], Mean=['batch_norm_50.w_1'], MomentumTensor=[], Scale=['batch_norm_50.w_0'], Variance=['batch_norm_50.w_2'], X=['conv2d_34.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_50.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_50.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_16.w_0.cast_fp32']} = cast(inputs={X=['prelu_16.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_16.tmp_0']} = prelu(inputs={Alpha=['prelu_16.w_0.cast_fp32'], X=['batch_norm_50.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_16.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_16.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_35.tmp_0']} = conv2d(inputs={Filter=['conv2d_35.w_0'], Input=['prelu_16.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_51.w_1'], ReserveSpace=['batch_norm_51.tmp_2'], SavedMean=['batch_norm_51.tmp_0'], SavedVariance=['batch_norm_51.tmp_1'], VarianceOut=['batch_norm_51.w_2'], Y=['batch_norm_51.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_51.b_0'], Mean=['batch_norm_51.w_1'], MomentumTensor=[], Scale=['batch_norm_51.w_0'], Variance=['batch_norm_51.w_2'], X=['conv2d_35.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_15']} = elementwise_add(inputs={X=['batch_norm_51.tmp_3'], Y=['elementwise_add_14']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_52.w_1'], ReserveSpace=['batch_norm_52.tmp_2'], SavedMean=['batch_norm_52.tmp_0'], SavedVariance=['batch_norm_52.tmp_1'], VarianceOut=['batch_norm_52.w_2'], Y=['batch_norm_52.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_52.b_0'], Mean=['batch_norm_52.w_1'], MomentumTensor=[], Scale=['batch_norm_52.w_0'], Variance=['batch_norm_52.w_2'], X=['elementwise_add_15']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_36.tmp_0']} = conv2d(inputs={Filter=['conv2d_36.w_0'], Input=['batch_norm_52.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_53.w_1'], ReserveSpace=['batch_norm_53.tmp_2'], SavedMean=['batch_norm_53.tmp_0'], SavedVariance=['batch_norm_53.tmp_1'], VarianceOut=['batch_norm_53.w_2'], Y=['batch_norm_53.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_53.b_0'], Mean=['batch_norm_53.w_1'], MomentumTensor=[], Scale=['batch_norm_53.w_0'], Variance=['batch_norm_53.w_2'], X=['conv2d_36.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_53.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_53.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_17.w_0.cast_fp32']} = cast(inputs={X=['prelu_17.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_17.tmp_0']} = prelu(inputs={Alpha=['prelu_17.w_0.cast_fp32'], X=['batch_norm_53.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_17.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_17.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_37.tmp_0']} = conv2d(inputs={Filter=['conv2d_37.w_0'], Input=['prelu_17.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_54.w_1'], ReserveSpace=['batch_norm_54.tmp_2'], SavedMean=['batch_norm_54.tmp_0'], SavedVariance=['batch_norm_54.tmp_1'], VarianceOut=['batch_norm_54.w_2'], Y=['batch_norm_54.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_54.b_0'], Mean=['batch_norm_54.w_1'], MomentumTensor=[], Scale=['batch_norm_54.w_0'], Variance=['batch_norm_54.w_2'], X=['conv2d_37.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_16']} = elementwise_add(inputs={X=['batch_norm_54.tmp_3'], Y=['elementwise_add_15']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_55.w_1'], ReserveSpace=['batch_norm_55.tmp_2'], SavedMean=['batch_norm_55.tmp_0'], SavedVariance=['batch_norm_55.tmp_1'], VarianceOut=['batch_norm_55.w_2'], Y=['batch_norm_55.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_55.b_0'], Mean=['batch_norm_55.w_1'], MomentumTensor=[], Scale=['batch_norm_55.w_0'], Variance=['batch_norm_55.w_2'], X=['elementwise_add_16']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_38.tmp_0']} = conv2d(inputs={Filter=['conv2d_38.w_0'], Input=['batch_norm_55.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_56.w_1'], ReserveSpace=['batch_norm_56.tmp_2'], SavedMean=['batch_norm_56.tmp_0'], SavedVariance=['batch_norm_56.tmp_1'], VarianceOut=['batch_norm_56.w_2'], Y=['batch_norm_56.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_56.b_0'], Mean=['batch_norm_56.w_1'], MomentumTensor=[], Scale=['batch_norm_56.w_0'], Variance=['batch_norm_56.w_2'], X=['conv2d_38.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_56.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_56.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_18.w_0.cast_fp32']} = cast(inputs={X=['prelu_18.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_18.tmp_0']} = prelu(inputs={Alpha=['prelu_18.w_0.cast_fp32'], X=['batch_norm_56.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_18.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_18.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_39.tmp_0']} = conv2d(inputs={Filter=['conv2d_39.w_0'], Input=['prelu_18.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_57.w_1'], ReserveSpace=['batch_norm_57.tmp_2'], SavedMean=['batch_norm_57.tmp_0'], SavedVariance=['batch_norm_57.tmp_1'], VarianceOut=['batch_norm_57.w_2'], Y=['batch_norm_57.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_57.b_0'], Mean=['batch_norm_57.w_1'], MomentumTensor=[], Scale=['batch_norm_57.w_0'], Variance=['batch_norm_57.w_2'], X=['conv2d_39.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_17']} = elementwise_add(inputs={X=['batch_norm_57.tmp_3'], Y=['elementwise_add_16']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_58.w_1'], ReserveSpace=['batch_norm_58.tmp_2'], SavedMean=['batch_norm_58.tmp_0'], SavedVariance=['batch_norm_58.tmp_1'], VarianceOut=['batch_norm_58.w_2'], Y=['batch_norm_58.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_58.b_0'], Mean=['batch_norm_58.w_1'], MomentumTensor=[], Scale=['batch_norm_58.w_0'], Variance=['batch_norm_58.w_2'], X=['elementwise_add_17']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_40.tmp_0']} = conv2d(inputs={Filter=['conv2d_40.w_0'], Input=['batch_norm_58.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_59.w_1'], ReserveSpace=['batch_norm_59.tmp_2'], SavedMean=['batch_norm_59.tmp_0'], SavedVariance=['batch_norm_59.tmp_1'], VarianceOut=['batch_norm_59.w_2'], Y=['batch_norm_59.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_59.b_0'], Mean=['batch_norm_59.w_1'], MomentumTensor=[], Scale=['batch_norm_59.w_0'], Variance=['batch_norm_59.w_2'], X=['conv2d_40.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_59.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_59.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_19.w_0.cast_fp32']} = cast(inputs={X=['prelu_19.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_19.tmp_0']} = prelu(inputs={Alpha=['prelu_19.w_0.cast_fp32'], X=['batch_norm_59.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_19.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_19.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_41.tmp_0']} = conv2d(inputs={Filter=['conv2d_41.w_0'], Input=['prelu_19.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_60.w_1'], ReserveSpace=['batch_norm_60.tmp_2'], SavedMean=['batch_norm_60.tmp_0'], SavedVariance=['batch_norm_60.tmp_1'], VarianceOut=['batch_norm_60.w_2'], Y=['batch_norm_60.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_60.b_0'], Mean=['batch_norm_60.w_1'], MomentumTensor=[], Scale=['batch_norm_60.w_0'], Variance=['batch_norm_60.w_2'], X=['conv2d_41.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_18']} = elementwise_add(inputs={X=['batch_norm_60.tmp_3'], Y=['elementwise_add_17']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_61.w_1'], ReserveSpace=['batch_norm_61.tmp_2'], SavedMean=['batch_norm_61.tmp_0'], SavedVariance=['batch_norm_61.tmp_1'], VarianceOut=['batch_norm_61.w_2'], Y=['batch_norm_61.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_61.b_0'], Mean=['batch_norm_61.w_1'], MomentumTensor=[], Scale=['batch_norm_61.w_0'], Variance=['batch_norm_61.w_2'], X=['elementwise_add_18']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_42.tmp_0']} = conv2d(inputs={Filter=['conv2d_42.w_0'], Input=['batch_norm_61.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_62.w_1'], ReserveSpace=['batch_norm_62.tmp_2'], SavedMean=['batch_norm_62.tmp_0'], SavedVariance=['batch_norm_62.tmp_1'], VarianceOut=['batch_norm_62.w_2'], Y=['batch_norm_62.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_62.b_0'], Mean=['batch_norm_62.w_1'], MomentumTensor=[], Scale=['batch_norm_62.w_0'], Variance=['batch_norm_62.w_2'], X=['conv2d_42.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_62.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_62.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_20.w_0.cast_fp32']} = cast(inputs={X=['prelu_20.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_20.tmp_0']} = prelu(inputs={Alpha=['prelu_20.w_0.cast_fp32'], X=['batch_norm_62.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_20.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_20.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_43.tmp_0']} = conv2d(inputs={Filter=['conv2d_43.w_0'], Input=['prelu_20.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_63.w_1'], ReserveSpace=['batch_norm_63.tmp_2'], SavedMean=['batch_norm_63.tmp_0'], SavedVariance=['batch_norm_63.tmp_1'], VarianceOut=['batch_norm_63.w_2'], Y=['batch_norm_63.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_63.b_0'], Mean=['batch_norm_63.w_1'], MomentumTensor=[], Scale=['batch_norm_63.w_0'], Variance=['batch_norm_63.w_2'], X=['conv2d_43.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_19']} = elementwise_add(inputs={X=['batch_norm_63.tmp_3'], Y=['elementwise_add_18']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_64.w_1'], ReserveSpace=['batch_norm_64.tmp_2'], SavedMean=['batch_norm_64.tmp_0'], SavedVariance=['batch_norm_64.tmp_1'], VarianceOut=['batch_norm_64.w_2'], Y=['batch_norm_64.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_64.b_0'], Mean=['batch_norm_64.w_1'], MomentumTensor=[], Scale=['batch_norm_64.w_0'], Variance=['batch_norm_64.w_2'], X=['elementwise_add_19']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_44.tmp_0']} = conv2d(inputs={Filter=['conv2d_44.w_0'], Input=['batch_norm_64.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_65.w_1'], ReserveSpace=['batch_norm_65.tmp_2'], SavedMean=['batch_norm_65.tmp_0'], SavedVariance=['batch_norm_65.tmp_1'], VarianceOut=['batch_norm_65.w_2'], Y=['batch_norm_65.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_65.b_0'], Mean=['batch_norm_65.w_1'], MomentumTensor=[], Scale=['batch_norm_65.w_0'], Variance=['batch_norm_65.w_2'], X=['conv2d_44.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_65.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_65.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_21.w_0.cast_fp32']} = cast(inputs={X=['prelu_21.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_21.tmp_0']} = prelu(inputs={Alpha=['prelu_21.w_0.cast_fp32'], X=['batch_norm_65.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_21.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_21.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_45.tmp_0']} = conv2d(inputs={Filter=['conv2d_45.w_0'], Input=['prelu_21.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_66.w_1'], ReserveSpace=['batch_norm_66.tmp_2'], SavedMean=['batch_norm_66.tmp_0'], SavedVariance=['batch_norm_66.tmp_1'], VarianceOut=['batch_norm_66.w_2'], Y=['batch_norm_66.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_66.b_0'], Mean=['batch_norm_66.w_1'], MomentumTensor=[], Scale=['batch_norm_66.w_0'], Variance=['batch_norm_66.w_2'], X=['conv2d_45.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_20']} = elementwise_add(inputs={X=['batch_norm_66.tmp_3'], Y=['elementwise_add_19']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_67.w_1'], ReserveSpace=['batch_norm_67.tmp_2'], SavedMean=['batch_norm_67.tmp_0'], SavedVariance=['batch_norm_67.tmp_1'], VarianceOut=['batch_norm_67.w_2'], Y=['batch_norm_67.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_67.b_0'], Mean=['batch_norm_67.w_1'], MomentumTensor=[], Scale=['batch_norm_67.w_0'], Variance=['batch_norm_67.w_2'], X=['elementwise_add_20']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_46.tmp_0']} = conv2d(inputs={Filter=['conv2d_46.w_0'], Input=['batch_norm_67.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_68.w_1'], ReserveSpace=['batch_norm_68.tmp_2'], SavedMean=['batch_norm_68.tmp_0'], SavedVariance=['batch_norm_68.tmp_1'], VarianceOut=['batch_norm_68.w_2'], Y=['batch_norm_68.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_68.b_0'], Mean=['batch_norm_68.w_1'], MomentumTensor=[], Scale=['batch_norm_68.w_0'], Variance=['batch_norm_68.w_2'], X=['conv2d_46.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_68.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_68.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_22.w_0.cast_fp32']} = cast(inputs={X=['prelu_22.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_22.tmp_0']} = prelu(inputs={Alpha=['prelu_22.w_0.cast_fp32'], X=['batch_norm_68.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_22.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_22.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_47.tmp_0']} = conv2d(inputs={Filter=['conv2d_47.w_0'], Input=['prelu_22.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [2, 2], with_quant_attr = False)
    {MeanOut=['batch_norm_69.w_1'], ReserveSpace=['batch_norm_69.tmp_2'], SavedMean=['batch_norm_69.tmp_0'], SavedVariance=['batch_norm_69.tmp_1'], VarianceOut=['batch_norm_69.w_2'], Y=['batch_norm_69.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_69.b_0'], Mean=['batch_norm_69.w_1'], MomentumTensor=[], Scale=['batch_norm_69.w_0'], Variance=['batch_norm_69.w_2'], X=['conv2d_47.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_48.tmp_0']} = conv2d(inputs={Filter=['conv2d_48.w_0'], Input=['elementwise_add_20']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [0, 0], strides = [2, 2], with_quant_attr = False)
    {MeanOut=['batch_norm_70.w_1'], ReserveSpace=['batch_norm_70.tmp_2'], SavedMean=['batch_norm_70.tmp_0'], SavedVariance=['batch_norm_70.tmp_1'], VarianceOut=['batch_norm_70.w_2'], Y=['batch_norm_70.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_70.b_0'], Mean=['batch_norm_70.w_1'], MomentumTensor=[], Scale=['batch_norm_70.w_0'], Variance=['batch_norm_70.w_2'], X=['conv2d_48.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_21']} = elementwise_add(inputs={X=['batch_norm_69.tmp_3'], Y=['batch_norm_70.tmp_3']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_71.w_1'], ReserveSpace=['batch_norm_71.tmp_2'], SavedMean=['batch_norm_71.tmp_0'], SavedVariance=['batch_norm_71.tmp_1'], VarianceOut=['batch_norm_71.w_2'], Y=['batch_norm_71.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_71.b_0'], Mean=['batch_norm_71.w_1'], MomentumTensor=[], Scale=['batch_norm_71.w_0'], Variance=['batch_norm_71.w_2'], X=['elementwise_add_21']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_49.tmp_0']} = conv2d(inputs={Filter=['conv2d_49.w_0'], Input=['batch_norm_71.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_72.w_1'], ReserveSpace=['batch_norm_72.tmp_2'], SavedMean=['batch_norm_72.tmp_0'], SavedVariance=['batch_norm_72.tmp_1'], VarianceOut=['batch_norm_72.w_2'], Y=['batch_norm_72.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_72.b_0'], Mean=['batch_norm_72.w_1'], MomentumTensor=[], Scale=['batch_norm_72.w_0'], Variance=['batch_norm_72.w_2'], X=['conv2d_49.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_72.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_72.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_23.w_0.cast_fp32']} = cast(inputs={X=['prelu_23.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_23.tmp_0']} = prelu(inputs={Alpha=['prelu_23.w_0.cast_fp32'], X=['batch_norm_72.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_23.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_23.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_50.tmp_0']} = conv2d(inputs={Filter=['conv2d_50.w_0'], Input=['prelu_23.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_73.w_1'], ReserveSpace=['batch_norm_73.tmp_2'], SavedMean=['batch_norm_73.tmp_0'], SavedVariance=['batch_norm_73.tmp_1'], VarianceOut=['batch_norm_73.w_2'], Y=['batch_norm_73.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_73.b_0'], Mean=['batch_norm_73.w_1'], MomentumTensor=[], Scale=['batch_norm_73.w_0'], Variance=['batch_norm_73.w_2'], X=['conv2d_50.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_22']} = elementwise_add(inputs={X=['batch_norm_73.tmp_3'], Y=['elementwise_add_21']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_74.w_1'], ReserveSpace=['batch_norm_74.tmp_2'], SavedMean=['batch_norm_74.tmp_0'], SavedVariance=['batch_norm_74.tmp_1'], VarianceOut=['batch_norm_74.w_2'], Y=['batch_norm_74.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_74.b_0'], Mean=['batch_norm_74.w_1'], MomentumTensor=[], Scale=['batch_norm_74.w_0'], Variance=['batch_norm_74.w_2'], X=['elementwise_add_22']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Output=['conv2d_51.tmp_0']} = conv2d(inputs={Filter=['conv2d_51.w_0'], Input=['batch_norm_74.tmp_3']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_75.w_1'], ReserveSpace=['batch_norm_75.tmp_2'], SavedMean=['batch_norm_75.tmp_0'], SavedVariance=['batch_norm_75.tmp_1'], VarianceOut=['batch_norm_75.w_2'], Y=['batch_norm_75.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_75.b_0'], Mean=['batch_norm_75.w_1'], MomentumTensor=[], Scale=['batch_norm_75.w_0'], Variance=['batch_norm_75.w_2'], X=['conv2d_51.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['batch_norm_75.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_75.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_24.w_0.cast_fp32']} = cast(inputs={X=['prelu_24.w_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['prelu_24.tmp_0']} = prelu(inputs={Alpha=['prelu_24.w_0.cast_fp32'], X=['batch_norm_75.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_24.tmp_0.cast_fp16']} = cast(inputs={X=['prelu_24.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Output=['conv2d_52.tmp_0']} = conv2d(inputs={Filter=['conv2d_52.w_0'], Input=['prelu_24.tmp_0.cast_fp16']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {MeanOut=['batch_norm_76.w_1'], ReserveSpace=['batch_norm_76.tmp_2'], SavedMean=['batch_norm_76.tmp_0'], SavedVariance=['batch_norm_76.tmp_1'], VarianceOut=['batch_norm_76.w_2'], Y=['batch_norm_76.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_76.b_0'], Mean=['batch_norm_76.w_1'], MomentumTensor=[], Scale=['batch_norm_76.w_0'], Variance=['batch_norm_76.w_2'], X=['conv2d_52.tmp_0']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_23']} = elementwise_add(inputs={X=['batch_norm_76.tmp_3'], Y=['elementwise_add_22']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_77.w_1'], ReserveSpace=['batch_norm_77.tmp_2'], SavedMean=['batch_norm_77.tmp_0'], SavedVariance=['batch_norm_77.tmp_1'], VarianceOut=['batch_norm_77.w_2'], Y=['batch_norm_77.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_77.b_0'], Mean=['batch_norm_77.w_1'], MomentumTensor=[], Scale=['batch_norm_77.w_0'], Variance=['batch_norm_77.w_2'], X=['elementwise_add_23']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['fc_0.tmp_0']} = mul(inputs={X=['batch_norm_77.tmp_3'], Y=['fc_0.w_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False, x_num_col_dims = 1, y_num_col_dims = 1)
    {Out=['fc_0.tmp_1']} = elementwise_add(inputs={X=['fc_0.tmp_0'], Y=['fc_0.b_0']}, axis = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {MeanOut=['batch_norm_78.w_1'], ReserveSpace=['batch_norm_78.tmp_2'], SavedMean=['batch_norm_78.tmp_0'], SavedVariance=['batch_norm_78.tmp_1'], VarianceOut=['batch_norm_78.w_2'], Y=['batch_norm_78.tmp_3']} = batch_norm(inputs={Bias=['batch_norm_78.b_0'], Mean=['batch_norm_78.w_1'], MomentumTensor=[], Scale=['batch_norm_78.w_0'], Variance=['batch_norm_78.w_2'], X=['fc_0.tmp_1']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {RemappedLabel=['class_center_sample_0.tmp_0'], SampledLocalClassCenter=['class_center_sample_0.tmp_1']} = class_center_sample(inputs={Label=['label']}, fix_seed = False, nranks = 1, num_classes = 312, num_samples = 31, op_device = , op_namescope = /, op_role = 0, op_role_var = [], rank = 0, ring_id = 0, seed = 0, with_quant_attr = False)
    {Out=['gather_0.tmp_0']} = gather(inputs={Axis=[], Index=['class_center_sample_0.tmp_1'], X=['dist@fc@rank@00000.w.w_0']}, axis = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['batch_norm_78.tmp_3.cast_fp32']} = cast(inputs={X=['batch_norm_78.tmp_3']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['p_norm_0.tmp_0']} = p_norm(inputs={X=['batch_norm_78.tmp_3.cast_fp32']}, asvector = False, axis = 1, epsilon = 9.999999960041972e-13, keepdim = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], porder = 2.0, with_quant_attr = False)
    {Out=['fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 1e-12, value = 9.999999960041972e-13, with_quant_attr = False)
    {Out=['fill_constant_1.tmp_0.cast_fp32']} = cast(inputs={X=['fill_constant_1.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['elementwise_max_0']} = elementwise_max(inputs={X=['p_norm_0.tmp_0'], Y=['fill_constant_1.tmp_0.cast_fp32']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_div_0']} = elementwise_div(inputs={X=['batch_norm_78.tmp_3.cast_fp32'], Y=['elementwise_max_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gather_0.tmp_0.cast_fp32']} = cast(inputs={X=['gather_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['p_norm_1.tmp_0']} = p_norm(inputs={X=['gather_0.tmp_0.cast_fp32']}, asvector = False, axis = 0, epsilon = 9.999999960041972e-13, keepdim = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], porder = 2.0, with_quant_attr = False)
    {Out=['fill_constant_3.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 1e-12, value = 9.999999960041972e-13, with_quant_attr = False)
    {Out=['fill_constant_3.tmp_0.cast_fp32']} = cast(inputs={X=['fill_constant_3.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['elementwise_max_1']} = elementwise_max(inputs={X=['p_norm_1.tmp_0'], Y=['fill_constant_3.tmp_0.cast_fp32']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_div_1']} = elementwise_div(inputs={X=['gather_0.tmp_0.cast_fp32'], Y=['elementwise_max_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_div_1.cast_fp16']} = cast(inputs={X=['elementwise_div_1']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['elementwise_div_0.cast_fp16']} = cast(inputs={X=['elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['matmul_v2_0.tmp_0']} = matmul_v2(inputs={X=['elementwise_div_0.cast_fp16'], Y=['elementwise_div_1.cast_fp16']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['unsqueeze2_0.tmp_0'], XShape=['unsqueeze2_0.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['class_center_sample_0.tmp_0']}, axes = [-1], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_0.tmp_0.cast_fp32']} = cast(inputs={X=['matmul_v2_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Loss=['margin_cross_entropy_0.tmp_1'], Softmax=['margin_cross_entropy_0.tmp_0']} = margin_cross_entropy(inputs={Label=['unsqueeze2_0.tmp_0'], Logits=['matmul_v2_0.tmp_0.cast_fp32']}, margin1 = 1.0, margin2 = 0.5, margin3 = 0.0, nranks = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], rank = 0, return_softmax = False, ring_id = 0, scale = 64.0, with_quant_attr = False)
    {Out=['mean_0.tmp_0']} = reduce_mean(inputs={X=['margin_cross_entropy_0.tmp_1']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Out=['tmp_0']} = elementwise_mul(inputs={X=['mean_0.tmp_0'], Y=['loss_scaling_0']}, axis = -1, op_device = , op_namescope = /, op_role = 256, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_0@GRAD']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = , op_role = 257, op_role_var = [], place_type = -1, shape = [1], str_value = , value = 1.0, with_quant_attr = False)
    {X@GRAD=['mean_0.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_0@GRAD'], X=['mean_0.tmp_0'], Y=['loss_scaling_0']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['margin_cross_entropy_0.tmp_1@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_0.tmp_0@GRAD'], X=['margin_cross_entropy_0.tmp_1']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Logits@GRAD=['matmul_v2_0.tmp_0.cast_fp32@GRAD']} = margin_cross_entropy_grad(inputs={Label=['unsqueeze2_0.tmp_0'], Logits=['matmul_v2_0.tmp_0.cast_fp32'], Loss@GRAD=['margin_cross_entropy_0.tmp_1@GRAD'], Softmax=['margin_cross_entropy_0.tmp_0']}, margin1 = 1.0, margin2 = 0.5, margin3 = 0.0, nranks = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], rank = 0, return_softmax = False, ring_id = 0, scale = 64.0, with_quant_attr = False)
    {Out=['matmul_v2_0.tmp_0@GRAD']} = cast(inputs={X=['matmul_v2_0.tmp_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['elementwise_div_0.cast_fp16@GRAD'], Y@GRAD=['elementwise_div_1.cast_fp16@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_0.tmp_0@GRAD'], X=['elementwise_div_0.cast_fp16'], Y=['elementwise_div_1.cast_fp16']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['elementwise_div_0@GRAD']} = cast(inputs={X=['elementwise_div_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Out=['elementwise_div_1@GRAD']} = cast(inputs={X=['elementwise_div_1.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['gather_0.tmp_0.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['elementwise_max_1@GRAD']} = elementwise_div_grad(inputs={Out=['elementwise_div_1'], Out@GRAD=['elementwise_div_1@GRAD'], X=['gather_0.tmp_0.cast_fp32'], Y=['elementwise_max_1']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['p_norm_1.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_max_grad(inputs={Out@GRAD=['elementwise_max_1@GRAD'], X=['p_norm_1.tmp_0'], Y=['fill_constant_3.tmp_0.cast_fp32']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['gather_0.tmp_0.cast_fp32@GRAD@RENAME@block0@1']} = p_norm_grad(inputs={Out=['p_norm_1.tmp_0'], Out@GRAD=['p_norm_1.tmp_0@GRAD'], X=['gather_0.tmp_0.cast_fp32']}, asvector = False, axis = 0, epsilon = 9.999999960041972e-13, keepdim = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], porder = 2.0, with_quant_attr = False)
    {Out=['gather_0.tmp_0.cast_fp32@GRAD']} = grad_add(inputs={X=['gather_0.tmp_0.cast_fp32@GRAD@RENAME@block0@0'], Y=['gather_0.tmp_0.cast_fp32@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_0.tmp_0@GRAD']} = cast(inputs={X=['gather_0.tmp_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {X@GRAD=['batch_norm_78.tmp_3.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['elementwise_max_0@GRAD']} = elementwise_div_grad(inputs={Out=['elementwise_div_0'], Out@GRAD=['elementwise_div_0@GRAD'], X=['batch_norm_78.tmp_3.cast_fp32'], Y=['elementwise_max_0']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['p_norm_0.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_max_grad(inputs={Out@GRAD=['elementwise_max_0@GRAD'], X=['p_norm_0.tmp_0'], Y=['fill_constant_1.tmp_0.cast_fp32']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_78.tmp_3.cast_fp32@GRAD@RENAME@block0@1']} = p_norm_grad(inputs={Out=['p_norm_0.tmp_0'], Out@GRAD=['p_norm_0.tmp_0@GRAD'], X=['batch_norm_78.tmp_3.cast_fp32']}, asvector = False, axis = 1, epsilon = 9.999999960041972e-13, keepdim = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], porder = 2.0, with_quant_attr = False)
    {Out=['batch_norm_78.tmp_3.cast_fp32@GRAD']} = grad_add(inputs={X=['batch_norm_78.tmp_3.cast_fp32@GRAD@RENAME@block0@0'], Y=['batch_norm_78.tmp_3.cast_fp32@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['batch_norm_78.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_78.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_78.b_0@GRAD'], Scale@GRAD=['batch_norm_78.w_0@GRAD'], X@GRAD=['fc_0.tmp_1@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_78.b_0'], MeanOut=['batch_norm_78.w_1'], ReserveSpace=['batch_norm_78.tmp_2'], SavedMean=['batch_norm_78.tmp_0'], SavedVariance=['batch_norm_78.tmp_1'], Scale=['batch_norm_78.w_0'], VarianceOut=['batch_norm_78.w_2'], X=['fc_0.tmp_1'], Y@GRAD=['batch_norm_78.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_78.b_0', 'batch_norm_78.b_0@GRAD', 'batch_norm_78.w_0', 'batch_norm_78.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {X@GRAD=['fc_0.tmp_0@GRAD'], Y@GRAD=['fc_0.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_0.tmp_1@GRAD'], X=['fc_0.tmp_0'], Y=['fc_0.b_0']}, axis = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['fc_0.b_0', 'fc_0.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['batch_norm_77.tmp_3@GRAD'], Y@GRAD=['fc_0.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_0.tmp_0@GRAD'], X=['batch_norm_77.tmp_3'], Y=['fc_0.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = ['fc_0.w_0', 'fc_0.w_0@GRAD'], with_quant_attr = False, x_num_col_dims = 1, y_num_col_dims = 1)
    {Bias@GRAD=['batch_norm_77.b_0@GRAD'], Scale@GRAD=['batch_norm_77.w_0@GRAD'], X@GRAD=['elementwise_add_23@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_77.b_0'], MeanOut=['batch_norm_77.w_1'], ReserveSpace=['batch_norm_77.tmp_2'], SavedMean=['batch_norm_77.tmp_0'], SavedVariance=['batch_norm_77.tmp_1'], Scale=['batch_norm_77.w_0'], VarianceOut=['batch_norm_77.w_2'], X=['elementwise_add_23'], Y@GRAD=['batch_norm_77.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_77.b_0', 'batch_norm_77.b_0@GRAD', 'batch_norm_77.w_0', 'batch_norm_77.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {X@GRAD=['batch_norm_76.tmp_3@GRAD'], Y@GRAD=['elementwise_add_22@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_23@GRAD'], X=['batch_norm_76.tmp_3'], Y=['elementwise_add_22']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_76.b_0@GRAD'], Scale@GRAD=['batch_norm_76.w_0@GRAD'], X@GRAD=['conv2d_52.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_76.b_0'], MeanOut=['batch_norm_76.w_1'], ReserveSpace=['batch_norm_76.tmp_2'], SavedMean=['batch_norm_76.tmp_0'], SavedVariance=['batch_norm_76.tmp_1'], Scale=['batch_norm_76.w_0'], VarianceOut=['batch_norm_76.w_2'], X=['conv2d_52.tmp_0'], Y@GRAD=['batch_norm_76.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_76.b_0', 'batch_norm_76.b_0@GRAD', 'batch_norm_76.w_0', 'batch_norm_76.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_52.w_0@GRAD'], Input@GRAD=['prelu_24.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_52.w_0'], Input=['prelu_24.tmp_0.cast_fp16'], Output@GRAD=['conv2d_52.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_52.w_0', 'conv2d_52.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_24.tmp_0@GRAD']} = cast(inputs={X=['prelu_24.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_24.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_75.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_24.w_0.cast_fp32'], Out@GRAD=['prelu_24.tmp_0@GRAD'], X=['batch_norm_75.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_24.w_0@GRAD']} = cast(inputs={X=['prelu_24.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_24.w_0', 'prelu_24.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_75.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_75.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_75.b_0@GRAD'], Scale@GRAD=['batch_norm_75.w_0@GRAD'], X@GRAD=['conv2d_51.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_75.b_0'], MeanOut=['batch_norm_75.w_1'], ReserveSpace=['batch_norm_75.tmp_2'], SavedMean=['batch_norm_75.tmp_0'], SavedVariance=['batch_norm_75.tmp_1'], Scale=['batch_norm_75.w_0'], VarianceOut=['batch_norm_75.w_2'], X=['conv2d_51.tmp_0'], Y@GRAD=['batch_norm_75.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_75.b_0', 'batch_norm_75.b_0@GRAD', 'batch_norm_75.w_0', 'batch_norm_75.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_51.w_0@GRAD'], Input@GRAD=['batch_norm_74.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_51.w_0'], Input=['batch_norm_74.tmp_3'], Output@GRAD=['conv2d_51.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_51.w_0', 'conv2d_51.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_74.b_0@GRAD'], Scale@GRAD=['batch_norm_74.w_0@GRAD'], X@GRAD=['elementwise_add_22@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_74.b_0'], MeanOut=['batch_norm_74.w_1'], ReserveSpace=['batch_norm_74.tmp_2'], SavedMean=['batch_norm_74.tmp_0'], SavedVariance=['batch_norm_74.tmp_1'], Scale=['batch_norm_74.w_0'], VarianceOut=['batch_norm_74.w_2'], X=['elementwise_add_22'], Y@GRAD=['batch_norm_74.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_74.b_0', 'batch_norm_74.b_0@GRAD', 'batch_norm_74.w_0', 'batch_norm_74.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_22@GRAD']} = grad_add(inputs={X=['elementwise_add_22@GRAD@RENAME@block0@0'], Y=['elementwise_add_22@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_73.tmp_3@GRAD'], Y@GRAD=['elementwise_add_21@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_22@GRAD'], X=['batch_norm_73.tmp_3'], Y=['elementwise_add_21']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_73.b_0@GRAD'], Scale@GRAD=['batch_norm_73.w_0@GRAD'], X@GRAD=['conv2d_50.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_73.b_0'], MeanOut=['batch_norm_73.w_1'], ReserveSpace=['batch_norm_73.tmp_2'], SavedMean=['batch_norm_73.tmp_0'], SavedVariance=['batch_norm_73.tmp_1'], Scale=['batch_norm_73.w_0'], VarianceOut=['batch_norm_73.w_2'], X=['conv2d_50.tmp_0'], Y@GRAD=['batch_norm_73.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_73.b_0', 'batch_norm_73.b_0@GRAD', 'batch_norm_73.w_0', 'batch_norm_73.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_50.w_0@GRAD'], Input@GRAD=['prelu_23.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_50.w_0'], Input=['prelu_23.tmp_0.cast_fp16'], Output@GRAD=['conv2d_50.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_50.w_0', 'conv2d_50.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_23.tmp_0@GRAD']} = cast(inputs={X=['prelu_23.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_23.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_72.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_23.w_0.cast_fp32'], Out@GRAD=['prelu_23.tmp_0@GRAD'], X=['batch_norm_72.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_23.w_0@GRAD']} = cast(inputs={X=['prelu_23.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_23.w_0', 'prelu_23.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_72.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_72.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_72.b_0@GRAD'], Scale@GRAD=['batch_norm_72.w_0@GRAD'], X@GRAD=['conv2d_49.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_72.b_0'], MeanOut=['batch_norm_72.w_1'], ReserveSpace=['batch_norm_72.tmp_2'], SavedMean=['batch_norm_72.tmp_0'], SavedVariance=['batch_norm_72.tmp_1'], Scale=['batch_norm_72.w_0'], VarianceOut=['batch_norm_72.w_2'], X=['conv2d_49.tmp_0'], Y@GRAD=['batch_norm_72.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_72.b_0', 'batch_norm_72.b_0@GRAD', 'batch_norm_72.w_0', 'batch_norm_72.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_49.w_0@GRAD'], Input@GRAD=['batch_norm_71.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_49.w_0'], Input=['batch_norm_71.tmp_3'], Output@GRAD=['conv2d_49.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_49.w_0', 'conv2d_49.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_71.b_0@GRAD'], Scale@GRAD=['batch_norm_71.w_0@GRAD'], X@GRAD=['elementwise_add_21@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_71.b_0'], MeanOut=['batch_norm_71.w_1'], ReserveSpace=['batch_norm_71.tmp_2'], SavedMean=['batch_norm_71.tmp_0'], SavedVariance=['batch_norm_71.tmp_1'], Scale=['batch_norm_71.w_0'], VarianceOut=['batch_norm_71.w_2'], X=['elementwise_add_21'], Y@GRAD=['batch_norm_71.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_71.b_0', 'batch_norm_71.b_0@GRAD', 'batch_norm_71.w_0', 'batch_norm_71.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_21@GRAD']} = grad_add(inputs={X=['elementwise_add_21@GRAD@RENAME@block0@0'], Y=['elementwise_add_21@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_69.tmp_3@GRAD'], Y@GRAD=['batch_norm_70.tmp_3@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_21@GRAD'], X=['batch_norm_69.tmp_3'], Y=['batch_norm_70.tmp_3']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_70.b_0@GRAD'], Scale@GRAD=['batch_norm_70.w_0@GRAD'], X@GRAD=['conv2d_48.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_70.b_0'], MeanOut=['batch_norm_70.w_1'], ReserveSpace=['batch_norm_70.tmp_2'], SavedMean=['batch_norm_70.tmp_0'], SavedVariance=['batch_norm_70.tmp_1'], Scale=['batch_norm_70.w_0'], VarianceOut=['batch_norm_70.w_2'], X=['conv2d_48.tmp_0'], Y@GRAD=['batch_norm_70.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_70.b_0', 'batch_norm_70.b_0@GRAD', 'batch_norm_70.w_0', 'batch_norm_70.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_48.w_0@GRAD'], Input@GRAD=['elementwise_add_20@GRAD@RENAME@block0@0']} = conv2d_grad(inputs={Filter=['conv2d_48.w_0'], Input=['elementwise_add_20'], Output@GRAD=['conv2d_48.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_48.w_0', 'conv2d_48.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [0, 0], strides = [2, 2], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_69.b_0@GRAD'], Scale@GRAD=['batch_norm_69.w_0@GRAD'], X@GRAD=['conv2d_47.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_69.b_0'], MeanOut=['batch_norm_69.w_1'], ReserveSpace=['batch_norm_69.tmp_2'], SavedMean=['batch_norm_69.tmp_0'], SavedVariance=['batch_norm_69.tmp_1'], Scale=['batch_norm_69.w_0'], VarianceOut=['batch_norm_69.w_2'], X=['conv2d_47.tmp_0'], Y@GRAD=['batch_norm_69.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_69.b_0', 'batch_norm_69.b_0@GRAD', 'batch_norm_69.w_0', 'batch_norm_69.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_47.w_0@GRAD'], Input@GRAD=['prelu_22.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_47.w_0'], Input=['prelu_22.tmp_0.cast_fp16'], Output@GRAD=['conv2d_47.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_47.w_0', 'conv2d_47.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [2, 2], with_quant_attr = False)
    {Out=['prelu_22.tmp_0@GRAD']} = cast(inputs={X=['prelu_22.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_22.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_68.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_22.w_0.cast_fp32'], Out@GRAD=['prelu_22.tmp_0@GRAD'], X=['batch_norm_68.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_22.w_0@GRAD']} = cast(inputs={X=['prelu_22.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_22.w_0', 'prelu_22.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_68.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_68.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_68.b_0@GRAD'], Scale@GRAD=['batch_norm_68.w_0@GRAD'], X@GRAD=['conv2d_46.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_68.b_0'], MeanOut=['batch_norm_68.w_1'], ReserveSpace=['batch_norm_68.tmp_2'], SavedMean=['batch_norm_68.tmp_0'], SavedVariance=['batch_norm_68.tmp_1'], Scale=['batch_norm_68.w_0'], VarianceOut=['batch_norm_68.w_2'], X=['conv2d_46.tmp_0'], Y@GRAD=['batch_norm_68.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_68.b_0', 'batch_norm_68.b_0@GRAD', 'batch_norm_68.w_0', 'batch_norm_68.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_46.w_0@GRAD'], Input@GRAD=['batch_norm_67.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_46.w_0'], Input=['batch_norm_67.tmp_3'], Output@GRAD=['conv2d_46.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_46.w_0', 'conv2d_46.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_67.b_0@GRAD'], Scale@GRAD=['batch_norm_67.w_0@GRAD'], X@GRAD=['elementwise_add_20@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_67.b_0'], MeanOut=['batch_norm_67.w_1'], ReserveSpace=['batch_norm_67.tmp_2'], SavedMean=['batch_norm_67.tmp_0'], SavedVariance=['batch_norm_67.tmp_1'], Scale=['batch_norm_67.w_0'], VarianceOut=['batch_norm_67.w_2'], X=['elementwise_add_20'], Y@GRAD=['batch_norm_67.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_67.b_0', 'batch_norm_67.b_0@GRAD', 'batch_norm_67.w_0', 'batch_norm_67.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_20@GRAD']} = grad_add(inputs={X=['elementwise_add_20@GRAD@RENAME@block0@0'], Y=['elementwise_add_20@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_66.tmp_3@GRAD'], Y@GRAD=['elementwise_add_19@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_20@GRAD'], X=['batch_norm_66.tmp_3'], Y=['elementwise_add_19']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_66.b_0@GRAD'], Scale@GRAD=['batch_norm_66.w_0@GRAD'], X@GRAD=['conv2d_45.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_66.b_0'], MeanOut=['batch_norm_66.w_1'], ReserveSpace=['batch_norm_66.tmp_2'], SavedMean=['batch_norm_66.tmp_0'], SavedVariance=['batch_norm_66.tmp_1'], Scale=['batch_norm_66.w_0'], VarianceOut=['batch_norm_66.w_2'], X=['conv2d_45.tmp_0'], Y@GRAD=['batch_norm_66.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_66.b_0', 'batch_norm_66.b_0@GRAD', 'batch_norm_66.w_0', 'batch_norm_66.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_45.w_0@GRAD'], Input@GRAD=['prelu_21.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_45.w_0'], Input=['prelu_21.tmp_0.cast_fp16'], Output@GRAD=['conv2d_45.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_45.w_0', 'conv2d_45.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_21.tmp_0@GRAD']} = cast(inputs={X=['prelu_21.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_21.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_65.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_21.w_0.cast_fp32'], Out@GRAD=['prelu_21.tmp_0@GRAD'], X=['batch_norm_65.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_21.w_0@GRAD']} = cast(inputs={X=['prelu_21.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_21.w_0', 'prelu_21.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_65.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_65.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_65.b_0@GRAD'], Scale@GRAD=['batch_norm_65.w_0@GRAD'], X@GRAD=['conv2d_44.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_65.b_0'], MeanOut=['batch_norm_65.w_1'], ReserveSpace=['batch_norm_65.tmp_2'], SavedMean=['batch_norm_65.tmp_0'], SavedVariance=['batch_norm_65.tmp_1'], Scale=['batch_norm_65.w_0'], VarianceOut=['batch_norm_65.w_2'], X=['conv2d_44.tmp_0'], Y@GRAD=['batch_norm_65.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_65.b_0', 'batch_norm_65.b_0@GRAD', 'batch_norm_65.w_0', 'batch_norm_65.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_44.w_0@GRAD'], Input@GRAD=['batch_norm_64.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_44.w_0'], Input=['batch_norm_64.tmp_3'], Output@GRAD=['conv2d_44.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_44.w_0', 'conv2d_44.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_64.b_0@GRAD'], Scale@GRAD=['batch_norm_64.w_0@GRAD'], X@GRAD=['elementwise_add_19@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_64.b_0'], MeanOut=['batch_norm_64.w_1'], ReserveSpace=['batch_norm_64.tmp_2'], SavedMean=['batch_norm_64.tmp_0'], SavedVariance=['batch_norm_64.tmp_1'], Scale=['batch_norm_64.w_0'], VarianceOut=['batch_norm_64.w_2'], X=['elementwise_add_19'], Y@GRAD=['batch_norm_64.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_64.b_0', 'batch_norm_64.b_0@GRAD', 'batch_norm_64.w_0', 'batch_norm_64.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_19@GRAD']} = grad_add(inputs={X=['elementwise_add_19@GRAD@RENAME@block0@0'], Y=['elementwise_add_19@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_63.tmp_3@GRAD'], Y@GRAD=['elementwise_add_18@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_19@GRAD'], X=['batch_norm_63.tmp_3'], Y=['elementwise_add_18']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_63.b_0@GRAD'], Scale@GRAD=['batch_norm_63.w_0@GRAD'], X@GRAD=['conv2d_43.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_63.b_0'], MeanOut=['batch_norm_63.w_1'], ReserveSpace=['batch_norm_63.tmp_2'], SavedMean=['batch_norm_63.tmp_0'], SavedVariance=['batch_norm_63.tmp_1'], Scale=['batch_norm_63.w_0'], VarianceOut=['batch_norm_63.w_2'], X=['conv2d_43.tmp_0'], Y@GRAD=['batch_norm_63.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_63.b_0', 'batch_norm_63.b_0@GRAD', 'batch_norm_63.w_0', 'batch_norm_63.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_43.w_0@GRAD'], Input@GRAD=['prelu_20.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_43.w_0'], Input=['prelu_20.tmp_0.cast_fp16'], Output@GRAD=['conv2d_43.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_43.w_0', 'conv2d_43.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_20.tmp_0@GRAD']} = cast(inputs={X=['prelu_20.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_20.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_62.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_20.w_0.cast_fp32'], Out@GRAD=['prelu_20.tmp_0@GRAD'], X=['batch_norm_62.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_20.w_0@GRAD']} = cast(inputs={X=['prelu_20.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_20.w_0', 'prelu_20.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_62.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_62.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_62.b_0@GRAD'], Scale@GRAD=['batch_norm_62.w_0@GRAD'], X@GRAD=['conv2d_42.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_62.b_0'], MeanOut=['batch_norm_62.w_1'], ReserveSpace=['batch_norm_62.tmp_2'], SavedMean=['batch_norm_62.tmp_0'], SavedVariance=['batch_norm_62.tmp_1'], Scale=['batch_norm_62.w_0'], VarianceOut=['batch_norm_62.w_2'], X=['conv2d_42.tmp_0'], Y@GRAD=['batch_norm_62.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_62.b_0', 'batch_norm_62.b_0@GRAD', 'batch_norm_62.w_0', 'batch_norm_62.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_42.w_0@GRAD'], Input@GRAD=['batch_norm_61.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_42.w_0'], Input=['batch_norm_61.tmp_3'], Output@GRAD=['conv2d_42.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_42.w_0', 'conv2d_42.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_61.b_0@GRAD'], Scale@GRAD=['batch_norm_61.w_0@GRAD'], X@GRAD=['elementwise_add_18@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_61.b_0'], MeanOut=['batch_norm_61.w_1'], ReserveSpace=['batch_norm_61.tmp_2'], SavedMean=['batch_norm_61.tmp_0'], SavedVariance=['batch_norm_61.tmp_1'], Scale=['batch_norm_61.w_0'], VarianceOut=['batch_norm_61.w_2'], X=['elementwise_add_18'], Y@GRAD=['batch_norm_61.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_61.b_0', 'batch_norm_61.b_0@GRAD', 'batch_norm_61.w_0', 'batch_norm_61.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_18@GRAD']} = grad_add(inputs={X=['elementwise_add_18@GRAD@RENAME@block0@0'], Y=['elementwise_add_18@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_60.tmp_3@GRAD'], Y@GRAD=['elementwise_add_17@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_18@GRAD'], X=['batch_norm_60.tmp_3'], Y=['elementwise_add_17']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_60.b_0@GRAD'], Scale@GRAD=['batch_norm_60.w_0@GRAD'], X@GRAD=['conv2d_41.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_60.b_0'], MeanOut=['batch_norm_60.w_1'], ReserveSpace=['batch_norm_60.tmp_2'], SavedMean=['batch_norm_60.tmp_0'], SavedVariance=['batch_norm_60.tmp_1'], Scale=['batch_norm_60.w_0'], VarianceOut=['batch_norm_60.w_2'], X=['conv2d_41.tmp_0'], Y@GRAD=['batch_norm_60.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_60.b_0', 'batch_norm_60.b_0@GRAD', 'batch_norm_60.w_0', 'batch_norm_60.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_41.w_0@GRAD'], Input@GRAD=['prelu_19.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_41.w_0'], Input=['prelu_19.tmp_0.cast_fp16'], Output@GRAD=['conv2d_41.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_41.w_0', 'conv2d_41.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_19.tmp_0@GRAD']} = cast(inputs={X=['prelu_19.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_19.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_59.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_19.w_0.cast_fp32'], Out@GRAD=['prelu_19.tmp_0@GRAD'], X=['batch_norm_59.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_19.w_0@GRAD']} = cast(inputs={X=['prelu_19.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_19.w_0', 'prelu_19.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_59.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_59.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_59.b_0@GRAD'], Scale@GRAD=['batch_norm_59.w_0@GRAD'], X@GRAD=['conv2d_40.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_59.b_0'], MeanOut=['batch_norm_59.w_1'], ReserveSpace=['batch_norm_59.tmp_2'], SavedMean=['batch_norm_59.tmp_0'], SavedVariance=['batch_norm_59.tmp_1'], Scale=['batch_norm_59.w_0'], VarianceOut=['batch_norm_59.w_2'], X=['conv2d_40.tmp_0'], Y@GRAD=['batch_norm_59.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_59.b_0', 'batch_norm_59.b_0@GRAD', 'batch_norm_59.w_0', 'batch_norm_59.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_40.w_0@GRAD'], Input@GRAD=['batch_norm_58.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_40.w_0'], Input=['batch_norm_58.tmp_3'], Output@GRAD=['conv2d_40.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_40.w_0', 'conv2d_40.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_58.b_0@GRAD'], Scale@GRAD=['batch_norm_58.w_0@GRAD'], X@GRAD=['elementwise_add_17@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_58.b_0'], MeanOut=['batch_norm_58.w_1'], ReserveSpace=['batch_norm_58.tmp_2'], SavedMean=['batch_norm_58.tmp_0'], SavedVariance=['batch_norm_58.tmp_1'], Scale=['batch_norm_58.w_0'], VarianceOut=['batch_norm_58.w_2'], X=['elementwise_add_17'], Y@GRAD=['batch_norm_58.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_58.b_0', 'batch_norm_58.b_0@GRAD', 'batch_norm_58.w_0', 'batch_norm_58.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_17@GRAD']} = grad_add(inputs={X=['elementwise_add_17@GRAD@RENAME@block0@0'], Y=['elementwise_add_17@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_57.tmp_3@GRAD'], Y@GRAD=['elementwise_add_16@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_17@GRAD'], X=['batch_norm_57.tmp_3'], Y=['elementwise_add_16']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_57.b_0@GRAD'], Scale@GRAD=['batch_norm_57.w_0@GRAD'], X@GRAD=['conv2d_39.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_57.b_0'], MeanOut=['batch_norm_57.w_1'], ReserveSpace=['batch_norm_57.tmp_2'], SavedMean=['batch_norm_57.tmp_0'], SavedVariance=['batch_norm_57.tmp_1'], Scale=['batch_norm_57.w_0'], VarianceOut=['batch_norm_57.w_2'], X=['conv2d_39.tmp_0'], Y@GRAD=['batch_norm_57.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_57.b_0', 'batch_norm_57.b_0@GRAD', 'batch_norm_57.w_0', 'batch_norm_57.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_39.w_0@GRAD'], Input@GRAD=['prelu_18.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_39.w_0'], Input=['prelu_18.tmp_0.cast_fp16'], Output@GRAD=['conv2d_39.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_39.w_0', 'conv2d_39.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_18.tmp_0@GRAD']} = cast(inputs={X=['prelu_18.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_18.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_56.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_18.w_0.cast_fp32'], Out@GRAD=['prelu_18.tmp_0@GRAD'], X=['batch_norm_56.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_18.w_0@GRAD']} = cast(inputs={X=['prelu_18.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_18.w_0', 'prelu_18.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_56.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_56.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_56.b_0@GRAD'], Scale@GRAD=['batch_norm_56.w_0@GRAD'], X@GRAD=['conv2d_38.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_56.b_0'], MeanOut=['batch_norm_56.w_1'], ReserveSpace=['batch_norm_56.tmp_2'], SavedMean=['batch_norm_56.tmp_0'], SavedVariance=['batch_norm_56.tmp_1'], Scale=['batch_norm_56.w_0'], VarianceOut=['batch_norm_56.w_2'], X=['conv2d_38.tmp_0'], Y@GRAD=['batch_norm_56.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_56.b_0', 'batch_norm_56.b_0@GRAD', 'batch_norm_56.w_0', 'batch_norm_56.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_38.w_0@GRAD'], Input@GRAD=['batch_norm_55.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_38.w_0'], Input=['batch_norm_55.tmp_3'], Output@GRAD=['conv2d_38.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_38.w_0', 'conv2d_38.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_55.b_0@GRAD'], Scale@GRAD=['batch_norm_55.w_0@GRAD'], X@GRAD=['elementwise_add_16@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_55.b_0'], MeanOut=['batch_norm_55.w_1'], ReserveSpace=['batch_norm_55.tmp_2'], SavedMean=['batch_norm_55.tmp_0'], SavedVariance=['batch_norm_55.tmp_1'], Scale=['batch_norm_55.w_0'], VarianceOut=['batch_norm_55.w_2'], X=['elementwise_add_16'], Y@GRAD=['batch_norm_55.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_55.b_0', 'batch_norm_55.b_0@GRAD', 'batch_norm_55.w_0', 'batch_norm_55.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_16@GRAD']} = grad_add(inputs={X=['elementwise_add_16@GRAD@RENAME@block0@0'], Y=['elementwise_add_16@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_54.tmp_3@GRAD'], Y@GRAD=['elementwise_add_15@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_16@GRAD'], X=['batch_norm_54.tmp_3'], Y=['elementwise_add_15']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_54.b_0@GRAD'], Scale@GRAD=['batch_norm_54.w_0@GRAD'], X@GRAD=['conv2d_37.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_54.b_0'], MeanOut=['batch_norm_54.w_1'], ReserveSpace=['batch_norm_54.tmp_2'], SavedMean=['batch_norm_54.tmp_0'], SavedVariance=['batch_norm_54.tmp_1'], Scale=['batch_norm_54.w_0'], VarianceOut=['batch_norm_54.w_2'], X=['conv2d_37.tmp_0'], Y@GRAD=['batch_norm_54.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_54.b_0', 'batch_norm_54.b_0@GRAD', 'batch_norm_54.w_0', 'batch_norm_54.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_37.w_0@GRAD'], Input@GRAD=['prelu_17.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_37.w_0'], Input=['prelu_17.tmp_0.cast_fp16'], Output@GRAD=['conv2d_37.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_37.w_0', 'conv2d_37.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_17.tmp_0@GRAD']} = cast(inputs={X=['prelu_17.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_17.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_53.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_17.w_0.cast_fp32'], Out@GRAD=['prelu_17.tmp_0@GRAD'], X=['batch_norm_53.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_17.w_0@GRAD']} = cast(inputs={X=['prelu_17.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_17.w_0', 'prelu_17.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_53.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_53.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_53.b_0@GRAD'], Scale@GRAD=['batch_norm_53.w_0@GRAD'], X@GRAD=['conv2d_36.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_53.b_0'], MeanOut=['batch_norm_53.w_1'], ReserveSpace=['batch_norm_53.tmp_2'], SavedMean=['batch_norm_53.tmp_0'], SavedVariance=['batch_norm_53.tmp_1'], Scale=['batch_norm_53.w_0'], VarianceOut=['batch_norm_53.w_2'], X=['conv2d_36.tmp_0'], Y@GRAD=['batch_norm_53.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_53.b_0', 'batch_norm_53.b_0@GRAD', 'batch_norm_53.w_0', 'batch_norm_53.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_36.w_0@GRAD'], Input@GRAD=['batch_norm_52.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_36.w_0'], Input=['batch_norm_52.tmp_3'], Output@GRAD=['conv2d_36.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_36.w_0', 'conv2d_36.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_52.b_0@GRAD'], Scale@GRAD=['batch_norm_52.w_0@GRAD'], X@GRAD=['elementwise_add_15@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_52.b_0'], MeanOut=['batch_norm_52.w_1'], ReserveSpace=['batch_norm_52.tmp_2'], SavedMean=['batch_norm_52.tmp_0'], SavedVariance=['batch_norm_52.tmp_1'], Scale=['batch_norm_52.w_0'], VarianceOut=['batch_norm_52.w_2'], X=['elementwise_add_15'], Y@GRAD=['batch_norm_52.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_52.b_0', 'batch_norm_52.b_0@GRAD', 'batch_norm_52.w_0', 'batch_norm_52.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_15@GRAD']} = grad_add(inputs={X=['elementwise_add_15@GRAD@RENAME@block0@0'], Y=['elementwise_add_15@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_51.tmp_3@GRAD'], Y@GRAD=['elementwise_add_14@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_15@GRAD'], X=['batch_norm_51.tmp_3'], Y=['elementwise_add_14']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_51.b_0@GRAD'], Scale@GRAD=['batch_norm_51.w_0@GRAD'], X@GRAD=['conv2d_35.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_51.b_0'], MeanOut=['batch_norm_51.w_1'], ReserveSpace=['batch_norm_51.tmp_2'], SavedMean=['batch_norm_51.tmp_0'], SavedVariance=['batch_norm_51.tmp_1'], Scale=['batch_norm_51.w_0'], VarianceOut=['batch_norm_51.w_2'], X=['conv2d_35.tmp_0'], Y@GRAD=['batch_norm_51.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_51.b_0', 'batch_norm_51.b_0@GRAD', 'batch_norm_51.w_0', 'batch_norm_51.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_35.w_0@GRAD'], Input@GRAD=['prelu_16.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_35.w_0'], Input=['prelu_16.tmp_0.cast_fp16'], Output@GRAD=['conv2d_35.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_35.w_0', 'conv2d_35.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_16.tmp_0@GRAD']} = cast(inputs={X=['prelu_16.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_16.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_50.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_16.w_0.cast_fp32'], Out@GRAD=['prelu_16.tmp_0@GRAD'], X=['batch_norm_50.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_16.w_0@GRAD']} = cast(inputs={X=['prelu_16.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_16.w_0', 'prelu_16.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_50.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_50.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_50.b_0@GRAD'], Scale@GRAD=['batch_norm_50.w_0@GRAD'], X@GRAD=['conv2d_34.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_50.b_0'], MeanOut=['batch_norm_50.w_1'], ReserveSpace=['batch_norm_50.tmp_2'], SavedMean=['batch_norm_50.tmp_0'], SavedVariance=['batch_norm_50.tmp_1'], Scale=['batch_norm_50.w_0'], VarianceOut=['batch_norm_50.w_2'], X=['conv2d_34.tmp_0'], Y@GRAD=['batch_norm_50.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_50.b_0', 'batch_norm_50.b_0@GRAD', 'batch_norm_50.w_0', 'batch_norm_50.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_34.w_0@GRAD'], Input@GRAD=['batch_norm_49.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_34.w_0'], Input=['batch_norm_49.tmp_3'], Output@GRAD=['conv2d_34.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_34.w_0', 'conv2d_34.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_49.b_0@GRAD'], Scale@GRAD=['batch_norm_49.w_0@GRAD'], X@GRAD=['elementwise_add_14@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_49.b_0'], MeanOut=['batch_norm_49.w_1'], ReserveSpace=['batch_norm_49.tmp_2'], SavedMean=['batch_norm_49.tmp_0'], SavedVariance=['batch_norm_49.tmp_1'], Scale=['batch_norm_49.w_0'], VarianceOut=['batch_norm_49.w_2'], X=['elementwise_add_14'], Y@GRAD=['batch_norm_49.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_49.b_0', 'batch_norm_49.b_0@GRAD', 'batch_norm_49.w_0', 'batch_norm_49.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_14@GRAD']} = grad_add(inputs={X=['elementwise_add_14@GRAD@RENAME@block0@0'], Y=['elementwise_add_14@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_48.tmp_3@GRAD'], Y@GRAD=['elementwise_add_13@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_14@GRAD'], X=['batch_norm_48.tmp_3'], Y=['elementwise_add_13']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_48.b_0@GRAD'], Scale@GRAD=['batch_norm_48.w_0@GRAD'], X@GRAD=['conv2d_33.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_48.b_0'], MeanOut=['batch_norm_48.w_1'], ReserveSpace=['batch_norm_48.tmp_2'], SavedMean=['batch_norm_48.tmp_0'], SavedVariance=['batch_norm_48.tmp_1'], Scale=['batch_norm_48.w_0'], VarianceOut=['batch_norm_48.w_2'], X=['conv2d_33.tmp_0'], Y@GRAD=['batch_norm_48.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_48.b_0', 'batch_norm_48.b_0@GRAD', 'batch_norm_48.w_0', 'batch_norm_48.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_33.w_0@GRAD'], Input@GRAD=['prelu_15.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_33.w_0'], Input=['prelu_15.tmp_0.cast_fp16'], Output@GRAD=['conv2d_33.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_33.w_0', 'conv2d_33.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_15.tmp_0@GRAD']} = cast(inputs={X=['prelu_15.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_15.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_47.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_15.w_0.cast_fp32'], Out@GRAD=['prelu_15.tmp_0@GRAD'], X=['batch_norm_47.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_15.w_0@GRAD']} = cast(inputs={X=['prelu_15.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_15.w_0', 'prelu_15.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_47.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_47.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_47.b_0@GRAD'], Scale@GRAD=['batch_norm_47.w_0@GRAD'], X@GRAD=['conv2d_32.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_47.b_0'], MeanOut=['batch_norm_47.w_1'], ReserveSpace=['batch_norm_47.tmp_2'], SavedMean=['batch_norm_47.tmp_0'], SavedVariance=['batch_norm_47.tmp_1'], Scale=['batch_norm_47.w_0'], VarianceOut=['batch_norm_47.w_2'], X=['conv2d_32.tmp_0'], Y@GRAD=['batch_norm_47.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_47.b_0', 'batch_norm_47.b_0@GRAD', 'batch_norm_47.w_0', 'batch_norm_47.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_32.w_0@GRAD'], Input@GRAD=['batch_norm_46.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_32.w_0'], Input=['batch_norm_46.tmp_3'], Output@GRAD=['conv2d_32.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_32.w_0', 'conv2d_32.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_46.b_0@GRAD'], Scale@GRAD=['batch_norm_46.w_0@GRAD'], X@GRAD=['elementwise_add_13@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_46.b_0'], MeanOut=['batch_norm_46.w_1'], ReserveSpace=['batch_norm_46.tmp_2'], SavedMean=['batch_norm_46.tmp_0'], SavedVariance=['batch_norm_46.tmp_1'], Scale=['batch_norm_46.w_0'], VarianceOut=['batch_norm_46.w_2'], X=['elementwise_add_13'], Y@GRAD=['batch_norm_46.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_46.b_0', 'batch_norm_46.b_0@GRAD', 'batch_norm_46.w_0', 'batch_norm_46.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_13@GRAD']} = grad_add(inputs={X=['elementwise_add_13@GRAD@RENAME@block0@0'], Y=['elementwise_add_13@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_45.tmp_3@GRAD'], Y@GRAD=['elementwise_add_12@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_13@GRAD'], X=['batch_norm_45.tmp_3'], Y=['elementwise_add_12']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_45.b_0@GRAD'], Scale@GRAD=['batch_norm_45.w_0@GRAD'], X@GRAD=['conv2d_31.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_45.b_0'], MeanOut=['batch_norm_45.w_1'], ReserveSpace=['batch_norm_45.tmp_2'], SavedMean=['batch_norm_45.tmp_0'], SavedVariance=['batch_norm_45.tmp_1'], Scale=['batch_norm_45.w_0'], VarianceOut=['batch_norm_45.w_2'], X=['conv2d_31.tmp_0'], Y@GRAD=['batch_norm_45.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_45.b_0', 'batch_norm_45.b_0@GRAD', 'batch_norm_45.w_0', 'batch_norm_45.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_31.w_0@GRAD'], Input@GRAD=['prelu_14.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_31.w_0'], Input=['prelu_14.tmp_0.cast_fp16'], Output@GRAD=['conv2d_31.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_31.w_0', 'conv2d_31.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_14.tmp_0@GRAD']} = cast(inputs={X=['prelu_14.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_14.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_44.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_14.w_0.cast_fp32'], Out@GRAD=['prelu_14.tmp_0@GRAD'], X=['batch_norm_44.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_14.w_0@GRAD']} = cast(inputs={X=['prelu_14.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_14.w_0', 'prelu_14.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_44.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_44.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_44.b_0@GRAD'], Scale@GRAD=['batch_norm_44.w_0@GRAD'], X@GRAD=['conv2d_30.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_44.b_0'], MeanOut=['batch_norm_44.w_1'], ReserveSpace=['batch_norm_44.tmp_2'], SavedMean=['batch_norm_44.tmp_0'], SavedVariance=['batch_norm_44.tmp_1'], Scale=['batch_norm_44.w_0'], VarianceOut=['batch_norm_44.w_2'], X=['conv2d_30.tmp_0'], Y@GRAD=['batch_norm_44.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_44.b_0', 'batch_norm_44.b_0@GRAD', 'batch_norm_44.w_0', 'batch_norm_44.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_30.w_0@GRAD'], Input@GRAD=['batch_norm_43.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_30.w_0'], Input=['batch_norm_43.tmp_3'], Output@GRAD=['conv2d_30.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_30.w_0', 'conv2d_30.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_43.b_0@GRAD'], Scale@GRAD=['batch_norm_43.w_0@GRAD'], X@GRAD=['elementwise_add_12@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_43.b_0'], MeanOut=['batch_norm_43.w_1'], ReserveSpace=['batch_norm_43.tmp_2'], SavedMean=['batch_norm_43.tmp_0'], SavedVariance=['batch_norm_43.tmp_1'], Scale=['batch_norm_43.w_0'], VarianceOut=['batch_norm_43.w_2'], X=['elementwise_add_12'], Y@GRAD=['batch_norm_43.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_43.b_0', 'batch_norm_43.b_0@GRAD', 'batch_norm_43.w_0', 'batch_norm_43.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_12@GRAD']} = grad_add(inputs={X=['elementwise_add_12@GRAD@RENAME@block0@0'], Y=['elementwise_add_12@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_42.tmp_3@GRAD'], Y@GRAD=['elementwise_add_11@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_12@GRAD'], X=['batch_norm_42.tmp_3'], Y=['elementwise_add_11']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_42.b_0@GRAD'], Scale@GRAD=['batch_norm_42.w_0@GRAD'], X@GRAD=['conv2d_29.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_42.b_0'], MeanOut=['batch_norm_42.w_1'], ReserveSpace=['batch_norm_42.tmp_2'], SavedMean=['batch_norm_42.tmp_0'], SavedVariance=['batch_norm_42.tmp_1'], Scale=['batch_norm_42.w_0'], VarianceOut=['batch_norm_42.w_2'], X=['conv2d_29.tmp_0'], Y@GRAD=['batch_norm_42.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_42.b_0', 'batch_norm_42.b_0@GRAD', 'batch_norm_42.w_0', 'batch_norm_42.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_29.w_0@GRAD'], Input@GRAD=['prelu_13.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_29.w_0'], Input=['prelu_13.tmp_0.cast_fp16'], Output@GRAD=['conv2d_29.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_29.w_0', 'conv2d_29.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_13.tmp_0@GRAD']} = cast(inputs={X=['prelu_13.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_13.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_41.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_13.w_0.cast_fp32'], Out@GRAD=['prelu_13.tmp_0@GRAD'], X=['batch_norm_41.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_13.w_0@GRAD']} = cast(inputs={X=['prelu_13.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_13.w_0', 'prelu_13.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_41.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_41.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_41.b_0@GRAD'], Scale@GRAD=['batch_norm_41.w_0@GRAD'], X@GRAD=['conv2d_28.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_41.b_0'], MeanOut=['batch_norm_41.w_1'], ReserveSpace=['batch_norm_41.tmp_2'], SavedMean=['batch_norm_41.tmp_0'], SavedVariance=['batch_norm_41.tmp_1'], Scale=['batch_norm_41.w_0'], VarianceOut=['batch_norm_41.w_2'], X=['conv2d_28.tmp_0'], Y@GRAD=['batch_norm_41.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_41.b_0', 'batch_norm_41.b_0@GRAD', 'batch_norm_41.w_0', 'batch_norm_41.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_28.w_0@GRAD'], Input@GRAD=['batch_norm_40.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_28.w_0'], Input=['batch_norm_40.tmp_3'], Output@GRAD=['conv2d_28.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_28.w_0', 'conv2d_28.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_40.b_0@GRAD'], Scale@GRAD=['batch_norm_40.w_0@GRAD'], X@GRAD=['elementwise_add_11@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_40.b_0'], MeanOut=['batch_norm_40.w_1'], ReserveSpace=['batch_norm_40.tmp_2'], SavedMean=['batch_norm_40.tmp_0'], SavedVariance=['batch_norm_40.tmp_1'], Scale=['batch_norm_40.w_0'], VarianceOut=['batch_norm_40.w_2'], X=['elementwise_add_11'], Y@GRAD=['batch_norm_40.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_40.b_0', 'batch_norm_40.b_0@GRAD', 'batch_norm_40.w_0', 'batch_norm_40.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_11@GRAD']} = grad_add(inputs={X=['elementwise_add_11@GRAD@RENAME@block0@0'], Y=['elementwise_add_11@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_39.tmp_3@GRAD'], Y@GRAD=['elementwise_add_10@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_11@GRAD'], X=['batch_norm_39.tmp_3'], Y=['elementwise_add_10']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_39.b_0@GRAD'], Scale@GRAD=['batch_norm_39.w_0@GRAD'], X@GRAD=['conv2d_27.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_39.b_0'], MeanOut=['batch_norm_39.w_1'], ReserveSpace=['batch_norm_39.tmp_2'], SavedMean=['batch_norm_39.tmp_0'], SavedVariance=['batch_norm_39.tmp_1'], Scale=['batch_norm_39.w_0'], VarianceOut=['batch_norm_39.w_2'], X=['conv2d_27.tmp_0'], Y@GRAD=['batch_norm_39.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_39.b_0', 'batch_norm_39.b_0@GRAD', 'batch_norm_39.w_0', 'batch_norm_39.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_27.w_0@GRAD'], Input@GRAD=['prelu_12.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_27.w_0'], Input=['prelu_12.tmp_0.cast_fp16'], Output@GRAD=['conv2d_27.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_27.w_0', 'conv2d_27.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_12.tmp_0@GRAD']} = cast(inputs={X=['prelu_12.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_12.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_38.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_12.w_0.cast_fp32'], Out@GRAD=['prelu_12.tmp_0@GRAD'], X=['batch_norm_38.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_12.w_0@GRAD']} = cast(inputs={X=['prelu_12.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_12.w_0', 'prelu_12.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_38.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_38.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_38.b_0@GRAD'], Scale@GRAD=['batch_norm_38.w_0@GRAD'], X@GRAD=['conv2d_26.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_38.b_0'], MeanOut=['batch_norm_38.w_1'], ReserveSpace=['batch_norm_38.tmp_2'], SavedMean=['batch_norm_38.tmp_0'], SavedVariance=['batch_norm_38.tmp_1'], Scale=['batch_norm_38.w_0'], VarianceOut=['batch_norm_38.w_2'], X=['conv2d_26.tmp_0'], Y@GRAD=['batch_norm_38.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_38.b_0', 'batch_norm_38.b_0@GRAD', 'batch_norm_38.w_0', 'batch_norm_38.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_26.w_0@GRAD'], Input@GRAD=['batch_norm_37.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_26.w_0'], Input=['batch_norm_37.tmp_3'], Output@GRAD=['conv2d_26.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_26.w_0', 'conv2d_26.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_37.b_0@GRAD'], Scale@GRAD=['batch_norm_37.w_0@GRAD'], X@GRAD=['elementwise_add_10@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_37.b_0'], MeanOut=['batch_norm_37.w_1'], ReserveSpace=['batch_norm_37.tmp_2'], SavedMean=['batch_norm_37.tmp_0'], SavedVariance=['batch_norm_37.tmp_1'], Scale=['batch_norm_37.w_0'], VarianceOut=['batch_norm_37.w_2'], X=['elementwise_add_10'], Y@GRAD=['batch_norm_37.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_37.b_0', 'batch_norm_37.b_0@GRAD', 'batch_norm_37.w_0', 'batch_norm_37.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_10@GRAD']} = grad_add(inputs={X=['elementwise_add_10@GRAD@RENAME@block0@0'], Y=['elementwise_add_10@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_36.tmp_3@GRAD'], Y@GRAD=['elementwise_add_9@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_10@GRAD'], X=['batch_norm_36.tmp_3'], Y=['elementwise_add_9']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_36.b_0@GRAD'], Scale@GRAD=['batch_norm_36.w_0@GRAD'], X@GRAD=['conv2d_25.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_36.b_0'], MeanOut=['batch_norm_36.w_1'], ReserveSpace=['batch_norm_36.tmp_2'], SavedMean=['batch_norm_36.tmp_0'], SavedVariance=['batch_norm_36.tmp_1'], Scale=['batch_norm_36.w_0'], VarianceOut=['batch_norm_36.w_2'], X=['conv2d_25.tmp_0'], Y@GRAD=['batch_norm_36.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_36.b_0', 'batch_norm_36.b_0@GRAD', 'batch_norm_36.w_0', 'batch_norm_36.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_25.w_0@GRAD'], Input@GRAD=['prelu_11.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_25.w_0'], Input=['prelu_11.tmp_0.cast_fp16'], Output@GRAD=['conv2d_25.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_25.w_0', 'conv2d_25.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_11.tmp_0@GRAD']} = cast(inputs={X=['prelu_11.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_11.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_35.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_11.w_0.cast_fp32'], Out@GRAD=['prelu_11.tmp_0@GRAD'], X=['batch_norm_35.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_11.w_0@GRAD']} = cast(inputs={X=['prelu_11.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_11.w_0', 'prelu_11.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_35.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_35.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_35.b_0@GRAD'], Scale@GRAD=['batch_norm_35.w_0@GRAD'], X@GRAD=['conv2d_24.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_35.b_0'], MeanOut=['batch_norm_35.w_1'], ReserveSpace=['batch_norm_35.tmp_2'], SavedMean=['batch_norm_35.tmp_0'], SavedVariance=['batch_norm_35.tmp_1'], Scale=['batch_norm_35.w_0'], VarianceOut=['batch_norm_35.w_2'], X=['conv2d_24.tmp_0'], Y@GRAD=['batch_norm_35.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_35.b_0', 'batch_norm_35.b_0@GRAD', 'batch_norm_35.w_0', 'batch_norm_35.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_24.w_0@GRAD'], Input@GRAD=['batch_norm_34.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_24.w_0'], Input=['batch_norm_34.tmp_3'], Output@GRAD=['conv2d_24.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_24.w_0', 'conv2d_24.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_34.b_0@GRAD'], Scale@GRAD=['batch_norm_34.w_0@GRAD'], X@GRAD=['elementwise_add_9@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_34.b_0'], MeanOut=['batch_norm_34.w_1'], ReserveSpace=['batch_norm_34.tmp_2'], SavedMean=['batch_norm_34.tmp_0'], SavedVariance=['batch_norm_34.tmp_1'], Scale=['batch_norm_34.w_0'], VarianceOut=['batch_norm_34.w_2'], X=['elementwise_add_9'], Y@GRAD=['batch_norm_34.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_34.b_0', 'batch_norm_34.b_0@GRAD', 'batch_norm_34.w_0', 'batch_norm_34.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_9@GRAD']} = grad_add(inputs={X=['elementwise_add_9@GRAD@RENAME@block0@0'], Y=['elementwise_add_9@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_33.tmp_3@GRAD'], Y@GRAD=['elementwise_add_8@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_9@GRAD'], X=['batch_norm_33.tmp_3'], Y=['elementwise_add_8']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_33.b_0@GRAD'], Scale@GRAD=['batch_norm_33.w_0@GRAD'], X@GRAD=['conv2d_23.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_33.b_0'], MeanOut=['batch_norm_33.w_1'], ReserveSpace=['batch_norm_33.tmp_2'], SavedMean=['batch_norm_33.tmp_0'], SavedVariance=['batch_norm_33.tmp_1'], Scale=['batch_norm_33.w_0'], VarianceOut=['batch_norm_33.w_2'], X=['conv2d_23.tmp_0'], Y@GRAD=['batch_norm_33.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_33.b_0', 'batch_norm_33.b_0@GRAD', 'batch_norm_33.w_0', 'batch_norm_33.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_23.w_0@GRAD'], Input@GRAD=['prelu_10.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_23.w_0'], Input=['prelu_10.tmp_0.cast_fp16'], Output@GRAD=['conv2d_23.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_23.w_0', 'conv2d_23.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_10.tmp_0@GRAD']} = cast(inputs={X=['prelu_10.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_10.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_32.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_10.w_0.cast_fp32'], Out@GRAD=['prelu_10.tmp_0@GRAD'], X=['batch_norm_32.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_10.w_0@GRAD']} = cast(inputs={X=['prelu_10.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_10.w_0', 'prelu_10.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_32.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_32.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_32.b_0@GRAD'], Scale@GRAD=['batch_norm_32.w_0@GRAD'], X@GRAD=['conv2d_22.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_32.b_0'], MeanOut=['batch_norm_32.w_1'], ReserveSpace=['batch_norm_32.tmp_2'], SavedMean=['batch_norm_32.tmp_0'], SavedVariance=['batch_norm_32.tmp_1'], Scale=['batch_norm_32.w_0'], VarianceOut=['batch_norm_32.w_2'], X=['conv2d_22.tmp_0'], Y@GRAD=['batch_norm_32.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_32.b_0', 'batch_norm_32.b_0@GRAD', 'batch_norm_32.w_0', 'batch_norm_32.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_22.w_0@GRAD'], Input@GRAD=['batch_norm_31.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_22.w_0'], Input=['batch_norm_31.tmp_3'], Output@GRAD=['conv2d_22.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_22.w_0', 'conv2d_22.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_31.b_0@GRAD'], Scale@GRAD=['batch_norm_31.w_0@GRAD'], X@GRAD=['elementwise_add_8@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_31.b_0'], MeanOut=['batch_norm_31.w_1'], ReserveSpace=['batch_norm_31.tmp_2'], SavedMean=['batch_norm_31.tmp_0'], SavedVariance=['batch_norm_31.tmp_1'], Scale=['batch_norm_31.w_0'], VarianceOut=['batch_norm_31.w_2'], X=['elementwise_add_8'], Y@GRAD=['batch_norm_31.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_31.b_0', 'batch_norm_31.b_0@GRAD', 'batch_norm_31.w_0', 'batch_norm_31.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_8@GRAD']} = grad_add(inputs={X=['elementwise_add_8@GRAD@RENAME@block0@0'], Y=['elementwise_add_8@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_30.tmp_3@GRAD'], Y@GRAD=['elementwise_add_7@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_8@GRAD'], X=['batch_norm_30.tmp_3'], Y=['elementwise_add_7']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_30.b_0@GRAD'], Scale@GRAD=['batch_norm_30.w_0@GRAD'], X@GRAD=['conv2d_21.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_30.b_0'], MeanOut=['batch_norm_30.w_1'], ReserveSpace=['batch_norm_30.tmp_2'], SavedMean=['batch_norm_30.tmp_0'], SavedVariance=['batch_norm_30.tmp_1'], Scale=['batch_norm_30.w_0'], VarianceOut=['batch_norm_30.w_2'], X=['conv2d_21.tmp_0'], Y@GRAD=['batch_norm_30.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_30.b_0', 'batch_norm_30.b_0@GRAD', 'batch_norm_30.w_0', 'batch_norm_30.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_21.w_0@GRAD'], Input@GRAD=['prelu_9.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_21.w_0'], Input=['prelu_9.tmp_0.cast_fp16'], Output@GRAD=['conv2d_21.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_21.w_0', 'conv2d_21.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_9.tmp_0@GRAD']} = cast(inputs={X=['prelu_9.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_9.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_29.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_9.w_0.cast_fp32'], Out@GRAD=['prelu_9.tmp_0@GRAD'], X=['batch_norm_29.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_9.w_0@GRAD']} = cast(inputs={X=['prelu_9.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_9.w_0', 'prelu_9.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_29.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_29.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_29.b_0@GRAD'], Scale@GRAD=['batch_norm_29.w_0@GRAD'], X@GRAD=['conv2d_20.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_29.b_0'], MeanOut=['batch_norm_29.w_1'], ReserveSpace=['batch_norm_29.tmp_2'], SavedMean=['batch_norm_29.tmp_0'], SavedVariance=['batch_norm_29.tmp_1'], Scale=['batch_norm_29.w_0'], VarianceOut=['batch_norm_29.w_2'], X=['conv2d_20.tmp_0'], Y@GRAD=['batch_norm_29.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_29.b_0', 'batch_norm_29.b_0@GRAD', 'batch_norm_29.w_0', 'batch_norm_29.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_20.w_0@GRAD'], Input@GRAD=['batch_norm_28.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_20.w_0'], Input=['batch_norm_28.tmp_3'], Output@GRAD=['conv2d_20.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_20.w_0', 'conv2d_20.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_28.b_0@GRAD'], Scale@GRAD=['batch_norm_28.w_0@GRAD'], X@GRAD=['elementwise_add_7@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_28.b_0'], MeanOut=['batch_norm_28.w_1'], ReserveSpace=['batch_norm_28.tmp_2'], SavedMean=['batch_norm_28.tmp_0'], SavedVariance=['batch_norm_28.tmp_1'], Scale=['batch_norm_28.w_0'], VarianceOut=['batch_norm_28.w_2'], X=['elementwise_add_7'], Y@GRAD=['batch_norm_28.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_28.b_0', 'batch_norm_28.b_0@GRAD', 'batch_norm_28.w_0', 'batch_norm_28.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_7@GRAD']} = grad_add(inputs={X=['elementwise_add_7@GRAD@RENAME@block0@0'], Y=['elementwise_add_7@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_26.tmp_3@GRAD'], Y@GRAD=['batch_norm_27.tmp_3@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_7@GRAD'], X=['batch_norm_26.tmp_3'], Y=['batch_norm_27.tmp_3']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_27.b_0@GRAD'], Scale@GRAD=['batch_norm_27.w_0@GRAD'], X@GRAD=['conv2d_19.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_27.b_0'], MeanOut=['batch_norm_27.w_1'], ReserveSpace=['batch_norm_27.tmp_2'], SavedMean=['batch_norm_27.tmp_0'], SavedVariance=['batch_norm_27.tmp_1'], Scale=['batch_norm_27.w_0'], VarianceOut=['batch_norm_27.w_2'], X=['conv2d_19.tmp_0'], Y@GRAD=['batch_norm_27.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_27.b_0', 'batch_norm_27.b_0@GRAD', 'batch_norm_27.w_0', 'batch_norm_27.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_19.w_0@GRAD'], Input@GRAD=['elementwise_add_6@GRAD@RENAME@block0@0']} = conv2d_grad(inputs={Filter=['conv2d_19.w_0'], Input=['elementwise_add_6'], Output@GRAD=['conv2d_19.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_19.w_0', 'conv2d_19.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [0, 0], strides = [2, 2], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_26.b_0@GRAD'], Scale@GRAD=['batch_norm_26.w_0@GRAD'], X@GRAD=['conv2d_18.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_26.b_0'], MeanOut=['batch_norm_26.w_1'], ReserveSpace=['batch_norm_26.tmp_2'], SavedMean=['batch_norm_26.tmp_0'], SavedVariance=['batch_norm_26.tmp_1'], Scale=['batch_norm_26.w_0'], VarianceOut=['batch_norm_26.w_2'], X=['conv2d_18.tmp_0'], Y@GRAD=['batch_norm_26.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_26.b_0', 'batch_norm_26.b_0@GRAD', 'batch_norm_26.w_0', 'batch_norm_26.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_18.w_0@GRAD'], Input@GRAD=['prelu_8.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_18.w_0'], Input=['prelu_8.tmp_0.cast_fp16'], Output@GRAD=['conv2d_18.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_18.w_0', 'conv2d_18.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [2, 2], with_quant_attr = False)
    {Out=['prelu_8.tmp_0@GRAD']} = cast(inputs={X=['prelu_8.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_8.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_25.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_8.w_0.cast_fp32'], Out@GRAD=['prelu_8.tmp_0@GRAD'], X=['batch_norm_25.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_8.w_0@GRAD']} = cast(inputs={X=['prelu_8.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_8.w_0', 'prelu_8.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_25.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_25.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_25.b_0@GRAD'], Scale@GRAD=['batch_norm_25.w_0@GRAD'], X@GRAD=['conv2d_17.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_25.b_0'], MeanOut=['batch_norm_25.w_1'], ReserveSpace=['batch_norm_25.tmp_2'], SavedMean=['batch_norm_25.tmp_0'], SavedVariance=['batch_norm_25.tmp_1'], Scale=['batch_norm_25.w_0'], VarianceOut=['batch_norm_25.w_2'], X=['conv2d_17.tmp_0'], Y@GRAD=['batch_norm_25.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_25.b_0', 'batch_norm_25.b_0@GRAD', 'batch_norm_25.w_0', 'batch_norm_25.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_17.w_0@GRAD'], Input@GRAD=['batch_norm_24.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_17.w_0'], Input=['batch_norm_24.tmp_3'], Output@GRAD=['conv2d_17.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_17.w_0', 'conv2d_17.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_24.b_0@GRAD'], Scale@GRAD=['batch_norm_24.w_0@GRAD'], X@GRAD=['elementwise_add_6@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_24.b_0'], MeanOut=['batch_norm_24.w_1'], ReserveSpace=['batch_norm_24.tmp_2'], SavedMean=['batch_norm_24.tmp_0'], SavedVariance=['batch_norm_24.tmp_1'], Scale=['batch_norm_24.w_0'], VarianceOut=['batch_norm_24.w_2'], X=['elementwise_add_6'], Y@GRAD=['batch_norm_24.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_24.b_0', 'batch_norm_24.b_0@GRAD', 'batch_norm_24.w_0', 'batch_norm_24.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_6@GRAD']} = grad_add(inputs={X=['elementwise_add_6@GRAD@RENAME@block0@0'], Y=['elementwise_add_6@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_23.tmp_3@GRAD'], Y@GRAD=['elementwise_add_5@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_6@GRAD'], X=['batch_norm_23.tmp_3'], Y=['elementwise_add_5']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_23.b_0@GRAD'], Scale@GRAD=['batch_norm_23.w_0@GRAD'], X@GRAD=['conv2d_16.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_23.b_0'], MeanOut=['batch_norm_23.w_1'], ReserveSpace=['batch_norm_23.tmp_2'], SavedMean=['batch_norm_23.tmp_0'], SavedVariance=['batch_norm_23.tmp_1'], Scale=['batch_norm_23.w_0'], VarianceOut=['batch_norm_23.w_2'], X=['conv2d_16.tmp_0'], Y@GRAD=['batch_norm_23.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_23.b_0', 'batch_norm_23.b_0@GRAD', 'batch_norm_23.w_0', 'batch_norm_23.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_16.w_0@GRAD'], Input@GRAD=['prelu_7.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_16.w_0'], Input=['prelu_7.tmp_0.cast_fp16'], Output@GRAD=['conv2d_16.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_16.w_0', 'conv2d_16.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_7.tmp_0@GRAD']} = cast(inputs={X=['prelu_7.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_7.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_22.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_7.w_0.cast_fp32'], Out@GRAD=['prelu_7.tmp_0@GRAD'], X=['batch_norm_22.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_7.w_0@GRAD']} = cast(inputs={X=['prelu_7.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_7.w_0', 'prelu_7.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_22.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_22.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_22.b_0@GRAD'], Scale@GRAD=['batch_norm_22.w_0@GRAD'], X@GRAD=['conv2d_15.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_22.b_0'], MeanOut=['batch_norm_22.w_1'], ReserveSpace=['batch_norm_22.tmp_2'], SavedMean=['batch_norm_22.tmp_0'], SavedVariance=['batch_norm_22.tmp_1'], Scale=['batch_norm_22.w_0'], VarianceOut=['batch_norm_22.w_2'], X=['conv2d_15.tmp_0'], Y@GRAD=['batch_norm_22.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_22.b_0', 'batch_norm_22.b_0@GRAD', 'batch_norm_22.w_0', 'batch_norm_22.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_15.w_0@GRAD'], Input@GRAD=['batch_norm_21.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_15.w_0'], Input=['batch_norm_21.tmp_3'], Output@GRAD=['conv2d_15.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_15.w_0', 'conv2d_15.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_21.b_0@GRAD'], Scale@GRAD=['batch_norm_21.w_0@GRAD'], X@GRAD=['elementwise_add_5@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_21.b_0'], MeanOut=['batch_norm_21.w_1'], ReserveSpace=['batch_norm_21.tmp_2'], SavedMean=['batch_norm_21.tmp_0'], SavedVariance=['batch_norm_21.tmp_1'], Scale=['batch_norm_21.w_0'], VarianceOut=['batch_norm_21.w_2'], X=['elementwise_add_5'], Y@GRAD=['batch_norm_21.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_21.b_0', 'batch_norm_21.b_0@GRAD', 'batch_norm_21.w_0', 'batch_norm_21.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_5@GRAD']} = grad_add(inputs={X=['elementwise_add_5@GRAD@RENAME@block0@0'], Y=['elementwise_add_5@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_20.tmp_3@GRAD'], Y@GRAD=['elementwise_add_4@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_5@GRAD'], X=['batch_norm_20.tmp_3'], Y=['elementwise_add_4']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_20.b_0@GRAD'], Scale@GRAD=['batch_norm_20.w_0@GRAD'], X@GRAD=['conv2d_14.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_20.b_0'], MeanOut=['batch_norm_20.w_1'], ReserveSpace=['batch_norm_20.tmp_2'], SavedMean=['batch_norm_20.tmp_0'], SavedVariance=['batch_norm_20.tmp_1'], Scale=['batch_norm_20.w_0'], VarianceOut=['batch_norm_20.w_2'], X=['conv2d_14.tmp_0'], Y@GRAD=['batch_norm_20.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_20.b_0', 'batch_norm_20.b_0@GRAD', 'batch_norm_20.w_0', 'batch_norm_20.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_14.w_0@GRAD'], Input@GRAD=['prelu_6.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_14.w_0'], Input=['prelu_6.tmp_0.cast_fp16'], Output@GRAD=['conv2d_14.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_14.w_0', 'conv2d_14.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_6.tmp_0@GRAD']} = cast(inputs={X=['prelu_6.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_6.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_19.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_6.w_0.cast_fp32'], Out@GRAD=['prelu_6.tmp_0@GRAD'], X=['batch_norm_19.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_6.w_0@GRAD']} = cast(inputs={X=['prelu_6.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_6.w_0', 'prelu_6.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_19.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_19.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_19.b_0@GRAD'], Scale@GRAD=['batch_norm_19.w_0@GRAD'], X@GRAD=['conv2d_13.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_19.b_0'], MeanOut=['batch_norm_19.w_1'], ReserveSpace=['batch_norm_19.tmp_2'], SavedMean=['batch_norm_19.tmp_0'], SavedVariance=['batch_norm_19.tmp_1'], Scale=['batch_norm_19.w_0'], VarianceOut=['batch_norm_19.w_2'], X=['conv2d_13.tmp_0'], Y@GRAD=['batch_norm_19.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_19.b_0', 'batch_norm_19.b_0@GRAD', 'batch_norm_19.w_0', 'batch_norm_19.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_13.w_0@GRAD'], Input@GRAD=['batch_norm_18.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_13.w_0'], Input=['batch_norm_18.tmp_3'], Output@GRAD=['conv2d_13.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_13.w_0', 'conv2d_13.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_18.b_0@GRAD'], Scale@GRAD=['batch_norm_18.w_0@GRAD'], X@GRAD=['elementwise_add_4@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_18.b_0'], MeanOut=['batch_norm_18.w_1'], ReserveSpace=['batch_norm_18.tmp_2'], SavedMean=['batch_norm_18.tmp_0'], SavedVariance=['batch_norm_18.tmp_1'], Scale=['batch_norm_18.w_0'], VarianceOut=['batch_norm_18.w_2'], X=['elementwise_add_4'], Y@GRAD=['batch_norm_18.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_18.b_0', 'batch_norm_18.b_0@GRAD', 'batch_norm_18.w_0', 'batch_norm_18.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_4@GRAD']} = grad_add(inputs={X=['elementwise_add_4@GRAD@RENAME@block0@0'], Y=['elementwise_add_4@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_17.tmp_3@GRAD'], Y@GRAD=['elementwise_add_3@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_4@GRAD'], X=['batch_norm_17.tmp_3'], Y=['elementwise_add_3']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_17.b_0@GRAD'], Scale@GRAD=['batch_norm_17.w_0@GRAD'], X@GRAD=['conv2d_12.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_17.b_0'], MeanOut=['batch_norm_17.w_1'], ReserveSpace=['batch_norm_17.tmp_2'], SavedMean=['batch_norm_17.tmp_0'], SavedVariance=['batch_norm_17.tmp_1'], Scale=['batch_norm_17.w_0'], VarianceOut=['batch_norm_17.w_2'], X=['conv2d_12.tmp_0'], Y@GRAD=['batch_norm_17.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_17.b_0', 'batch_norm_17.b_0@GRAD', 'batch_norm_17.w_0', 'batch_norm_17.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_12.w_0@GRAD'], Input@GRAD=['prelu_5.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_12.w_0'], Input=['prelu_5.tmp_0.cast_fp16'], Output@GRAD=['conv2d_12.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_12.w_0', 'conv2d_12.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_5.tmp_0@GRAD']} = cast(inputs={X=['prelu_5.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_5.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_16.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_5.w_0.cast_fp32'], Out@GRAD=['prelu_5.tmp_0@GRAD'], X=['batch_norm_16.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_5.w_0@GRAD']} = cast(inputs={X=['prelu_5.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_5.w_0', 'prelu_5.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_16.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_16.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_16.b_0@GRAD'], Scale@GRAD=['batch_norm_16.w_0@GRAD'], X@GRAD=['conv2d_11.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_16.b_0'], MeanOut=['batch_norm_16.w_1'], ReserveSpace=['batch_norm_16.tmp_2'], SavedMean=['batch_norm_16.tmp_0'], SavedVariance=['batch_norm_16.tmp_1'], Scale=['batch_norm_16.w_0'], VarianceOut=['batch_norm_16.w_2'], X=['conv2d_11.tmp_0'], Y@GRAD=['batch_norm_16.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_16.b_0', 'batch_norm_16.b_0@GRAD', 'batch_norm_16.w_0', 'batch_norm_16.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_11.w_0@GRAD'], Input@GRAD=['batch_norm_15.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_11.w_0'], Input=['batch_norm_15.tmp_3'], Output@GRAD=['conv2d_11.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_11.w_0', 'conv2d_11.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_15.b_0@GRAD'], Scale@GRAD=['batch_norm_15.w_0@GRAD'], X@GRAD=['elementwise_add_3@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_15.b_0'], MeanOut=['batch_norm_15.w_1'], ReserveSpace=['batch_norm_15.tmp_2'], SavedMean=['batch_norm_15.tmp_0'], SavedVariance=['batch_norm_15.tmp_1'], Scale=['batch_norm_15.w_0'], VarianceOut=['batch_norm_15.w_2'], X=['elementwise_add_3'], Y@GRAD=['batch_norm_15.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_15.b_0', 'batch_norm_15.b_0@GRAD', 'batch_norm_15.w_0', 'batch_norm_15.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_3@GRAD']} = grad_add(inputs={X=['elementwise_add_3@GRAD@RENAME@block0@0'], Y=['elementwise_add_3@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_13.tmp_3@GRAD'], Y@GRAD=['batch_norm_14.tmp_3@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_3@GRAD'], X=['batch_norm_13.tmp_3'], Y=['batch_norm_14.tmp_3']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_14.b_0@GRAD'], Scale@GRAD=['batch_norm_14.w_0@GRAD'], X@GRAD=['conv2d_10.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_14.b_0'], MeanOut=['batch_norm_14.w_1'], ReserveSpace=['batch_norm_14.tmp_2'], SavedMean=['batch_norm_14.tmp_0'], SavedVariance=['batch_norm_14.tmp_1'], Scale=['batch_norm_14.w_0'], VarianceOut=['batch_norm_14.w_2'], X=['conv2d_10.tmp_0'], Y@GRAD=['batch_norm_14.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_14.b_0', 'batch_norm_14.b_0@GRAD', 'batch_norm_14.w_0', 'batch_norm_14.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_10.w_0@GRAD'], Input@GRAD=['elementwise_add_2@GRAD@RENAME@block0@0']} = conv2d_grad(inputs={Filter=['conv2d_10.w_0'], Input=['elementwise_add_2'], Output@GRAD=['conv2d_10.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_10.w_0', 'conv2d_10.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [0, 0], strides = [2, 2], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_13.b_0@GRAD'], Scale@GRAD=['batch_norm_13.w_0@GRAD'], X@GRAD=['conv2d_9.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_13.b_0'], MeanOut=['batch_norm_13.w_1'], ReserveSpace=['batch_norm_13.tmp_2'], SavedMean=['batch_norm_13.tmp_0'], SavedVariance=['batch_norm_13.tmp_1'], Scale=['batch_norm_13.w_0'], VarianceOut=['batch_norm_13.w_2'], X=['conv2d_9.tmp_0'], Y@GRAD=['batch_norm_13.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_13.b_0', 'batch_norm_13.b_0@GRAD', 'batch_norm_13.w_0', 'batch_norm_13.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_9.w_0@GRAD'], Input@GRAD=['prelu_4.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_9.w_0'], Input=['prelu_4.tmp_0.cast_fp16'], Output@GRAD=['conv2d_9.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_9.w_0', 'conv2d_9.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [2, 2], with_quant_attr = False)
    {Out=['prelu_4.tmp_0@GRAD']} = cast(inputs={X=['prelu_4.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_4.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_12.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_4.w_0.cast_fp32'], Out@GRAD=['prelu_4.tmp_0@GRAD'], X=['batch_norm_12.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_4.w_0@GRAD']} = cast(inputs={X=['prelu_4.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_4.w_0', 'prelu_4.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_12.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_12.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_12.b_0@GRAD'], Scale@GRAD=['batch_norm_12.w_0@GRAD'], X@GRAD=['conv2d_8.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_12.b_0'], MeanOut=['batch_norm_12.w_1'], ReserveSpace=['batch_norm_12.tmp_2'], SavedMean=['batch_norm_12.tmp_0'], SavedVariance=['batch_norm_12.tmp_1'], Scale=['batch_norm_12.w_0'], VarianceOut=['batch_norm_12.w_2'], X=['conv2d_8.tmp_0'], Y@GRAD=['batch_norm_12.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_12.b_0', 'batch_norm_12.b_0@GRAD', 'batch_norm_12.w_0', 'batch_norm_12.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_8.w_0@GRAD'], Input@GRAD=['batch_norm_11.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_8.w_0'], Input=['batch_norm_11.tmp_3'], Output@GRAD=['conv2d_8.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_8.w_0', 'conv2d_8.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_11.b_0@GRAD'], Scale@GRAD=['batch_norm_11.w_0@GRAD'], X@GRAD=['elementwise_add_2@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_11.b_0'], MeanOut=['batch_norm_11.w_1'], ReserveSpace=['batch_norm_11.tmp_2'], SavedMean=['batch_norm_11.tmp_0'], SavedVariance=['batch_norm_11.tmp_1'], Scale=['batch_norm_11.w_0'], VarianceOut=['batch_norm_11.w_2'], X=['elementwise_add_2'], Y@GRAD=['batch_norm_11.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_11.b_0', 'batch_norm_11.b_0@GRAD', 'batch_norm_11.w_0', 'batch_norm_11.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_2@GRAD']} = grad_add(inputs={X=['elementwise_add_2@GRAD@RENAME@block0@0'], Y=['elementwise_add_2@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_10.tmp_3@GRAD'], Y@GRAD=['elementwise_add_1@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_2@GRAD'], X=['batch_norm_10.tmp_3'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_10.b_0@GRAD'], Scale@GRAD=['batch_norm_10.w_0@GRAD'], X@GRAD=['conv2d_7.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_10.b_0'], MeanOut=['batch_norm_10.w_1'], ReserveSpace=['batch_norm_10.tmp_2'], SavedMean=['batch_norm_10.tmp_0'], SavedVariance=['batch_norm_10.tmp_1'], Scale=['batch_norm_10.w_0'], VarianceOut=['batch_norm_10.w_2'], X=['conv2d_7.tmp_0'], Y@GRAD=['batch_norm_10.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_10.b_0', 'batch_norm_10.b_0@GRAD', 'batch_norm_10.w_0', 'batch_norm_10.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_7.w_0@GRAD'], Input@GRAD=['prelu_3.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_7.w_0'], Input=['prelu_3.tmp_0.cast_fp16'], Output@GRAD=['conv2d_7.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_7.w_0', 'conv2d_7.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_3.tmp_0@GRAD']} = cast(inputs={X=['prelu_3.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_3.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_9.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_3.w_0.cast_fp32'], Out@GRAD=['prelu_3.tmp_0@GRAD'], X=['batch_norm_9.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_3.w_0@GRAD']} = cast(inputs={X=['prelu_3.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_3.w_0', 'prelu_3.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_9.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_9.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_9.b_0@GRAD'], Scale@GRAD=['batch_norm_9.w_0@GRAD'], X@GRAD=['conv2d_6.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_9.b_0'], MeanOut=['batch_norm_9.w_1'], ReserveSpace=['batch_norm_9.tmp_2'], SavedMean=['batch_norm_9.tmp_0'], SavedVariance=['batch_norm_9.tmp_1'], Scale=['batch_norm_9.w_0'], VarianceOut=['batch_norm_9.w_2'], X=['conv2d_6.tmp_0'], Y@GRAD=['batch_norm_9.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_9.b_0', 'batch_norm_9.b_0@GRAD', 'batch_norm_9.w_0', 'batch_norm_9.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_6.w_0@GRAD'], Input@GRAD=['batch_norm_8.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_6.w_0'], Input=['batch_norm_8.tmp_3'], Output@GRAD=['conv2d_6.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_6.w_0', 'conv2d_6.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_8.b_0@GRAD'], Scale@GRAD=['batch_norm_8.w_0@GRAD'], X@GRAD=['elementwise_add_1@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_8.b_0'], MeanOut=['batch_norm_8.w_1'], ReserveSpace=['batch_norm_8.tmp_2'], SavedMean=['batch_norm_8.tmp_0'], SavedVariance=['batch_norm_8.tmp_1'], Scale=['batch_norm_8.w_0'], VarianceOut=['batch_norm_8.w_2'], X=['elementwise_add_1'], Y@GRAD=['batch_norm_8.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_8.b_0', 'batch_norm_8.b_0@GRAD', 'batch_norm_8.w_0', 'batch_norm_8.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_1@GRAD']} = grad_add(inputs={X=['elementwise_add_1@GRAD@RENAME@block0@0'], Y=['elementwise_add_1@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_7.tmp_3@GRAD'], Y@GRAD=['elementwise_add_0@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_1@GRAD'], X=['batch_norm_7.tmp_3'], Y=['elementwise_add_0']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_7.b_0@GRAD'], Scale@GRAD=['batch_norm_7.w_0@GRAD'], X@GRAD=['conv2d_5.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_7.b_0'], MeanOut=['batch_norm_7.w_1'], ReserveSpace=['batch_norm_7.tmp_2'], SavedMean=['batch_norm_7.tmp_0'], SavedVariance=['batch_norm_7.tmp_1'], Scale=['batch_norm_7.w_0'], VarianceOut=['batch_norm_7.w_2'], X=['conv2d_5.tmp_0'], Y@GRAD=['batch_norm_7.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_7.b_0', 'batch_norm_7.b_0@GRAD', 'batch_norm_7.w_0', 'batch_norm_7.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_5.w_0@GRAD'], Input@GRAD=['prelu_2.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_5.w_0'], Input=['prelu_2.tmp_0.cast_fp16'], Output@GRAD=['conv2d_5.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_5.w_0', 'conv2d_5.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['prelu_2.tmp_0@GRAD']} = cast(inputs={X=['prelu_2.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_2.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_6.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_2.w_0.cast_fp32'], Out@GRAD=['prelu_2.tmp_0@GRAD'], X=['batch_norm_6.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_2.w_0@GRAD']} = cast(inputs={X=['prelu_2.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_2.w_0', 'prelu_2.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_6.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_6.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_6.b_0@GRAD'], Scale@GRAD=['batch_norm_6.w_0@GRAD'], X@GRAD=['conv2d_4.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_6.b_0'], MeanOut=['batch_norm_6.w_1'], ReserveSpace=['batch_norm_6.tmp_2'], SavedMean=['batch_norm_6.tmp_0'], SavedVariance=['batch_norm_6.tmp_1'], Scale=['batch_norm_6.w_0'], VarianceOut=['batch_norm_6.w_2'], X=['conv2d_4.tmp_0'], Y@GRAD=['batch_norm_6.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_6.b_0', 'batch_norm_6.b_0@GRAD', 'batch_norm_6.w_0', 'batch_norm_6.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_4.w_0@GRAD'], Input@GRAD=['batch_norm_5.tmp_3@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_4.w_0'], Input=['batch_norm_5.tmp_3'], Output@GRAD=['conv2d_4.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_4.w_0', 'conv2d_4.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_5.b_0@GRAD'], Scale@GRAD=['batch_norm_5.w_0@GRAD'], X@GRAD=['elementwise_add_0@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_5.b_0'], MeanOut=['batch_norm_5.w_1'], ReserveSpace=['batch_norm_5.tmp_2'], SavedMean=['batch_norm_5.tmp_0'], SavedVariance=['batch_norm_5.tmp_1'], Scale=['batch_norm_5.w_0'], VarianceOut=['batch_norm_5.w_2'], X=['elementwise_add_0'], Y@GRAD=['batch_norm_5.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_5.b_0', 'batch_norm_5.b_0@GRAD', 'batch_norm_5.w_0', 'batch_norm_5.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['elementwise_add_0@GRAD']} = grad_add(inputs={X=['elementwise_add_0@GRAD@RENAME@block0@0'], Y=['elementwise_add_0@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['batch_norm_3.tmp_3@GRAD'], Y@GRAD=['batch_norm_4.tmp_3@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['elementwise_add_0@GRAD'], X=['batch_norm_3.tmp_3'], Y=['batch_norm_4.tmp_3']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Bias@GRAD=['batch_norm_4.b_0@GRAD'], Scale@GRAD=['batch_norm_4.w_0@GRAD'], X@GRAD=['conv2d_3.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_4.b_0'], MeanOut=['batch_norm_4.w_1'], ReserveSpace=['batch_norm_4.tmp_2'], SavedMean=['batch_norm_4.tmp_0'], SavedVariance=['batch_norm_4.tmp_1'], Scale=['batch_norm_4.w_0'], VarianceOut=['batch_norm_4.w_2'], X=['conv2d_3.tmp_0'], Y@GRAD=['batch_norm_4.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_4.b_0', 'batch_norm_4.b_0@GRAD', 'batch_norm_4.w_0', 'batch_norm_4.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_3.w_0@GRAD'], Input@GRAD=['prelu_0.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_3.w_0'], Input=['prelu_0.tmp_0.cast_fp16'], Output@GRAD=['conv2d_3.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_3.w_0', 'conv2d_3.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [0, 0], strides = [2, 2], with_quant_attr = False)
    {Out=['prelu_0.tmp_0@GRAD@RENAME@block0@0']} = cast(inputs={X=['prelu_0.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_3.b_0@GRAD'], Scale@GRAD=['batch_norm_3.w_0@GRAD'], X@GRAD=['conv2d_2.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_3.b_0'], MeanOut=['batch_norm_3.w_1'], ReserveSpace=['batch_norm_3.tmp_2'], SavedMean=['batch_norm_3.tmp_0'], SavedVariance=['batch_norm_3.tmp_1'], Scale=['batch_norm_3.w_0'], VarianceOut=['batch_norm_3.w_2'], X=['conv2d_2.tmp_0'], Y@GRAD=['batch_norm_3.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_3.b_0', 'batch_norm_3.b_0@GRAD', 'batch_norm_3.w_0', 'batch_norm_3.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_2.w_0@GRAD'], Input@GRAD=['prelu_1.tmp_0.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_2.w_0'], Input=['prelu_1.tmp_0.cast_fp16'], Output@GRAD=['conv2d_2.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_2.w_0', 'conv2d_2.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [2, 2], with_quant_attr = False)
    {Out=['prelu_1.tmp_0@GRAD']} = cast(inputs={X=['prelu_1.tmp_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Alpha@GRAD=['prelu_1.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_2.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_1.w_0.cast_fp32'], Out@GRAD=['prelu_1.tmp_0@GRAD'], X=['batch_norm_2.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_1.w_0@GRAD']} = cast(inputs={X=['prelu_1.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_1.w_0', 'prelu_1.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_2.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_2.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_2.b_0@GRAD'], Scale@GRAD=['batch_norm_2.w_0@GRAD'], X@GRAD=['conv2d_1.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_2.b_0'], MeanOut=['batch_norm_2.w_1'], ReserveSpace=['batch_norm_2.tmp_2'], SavedMean=['batch_norm_2.tmp_0'], SavedVariance=['batch_norm_2.tmp_1'], Scale=['batch_norm_2.w_0'], VarianceOut=['batch_norm_2.w_2'], X=['conv2d_1.tmp_0'], Y@GRAD=['batch_norm_2.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_2.b_0', 'batch_norm_2.b_0@GRAD', 'batch_norm_2.w_0', 'batch_norm_2.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_1.w_0@GRAD'], Input@GRAD=['batch_norm_1.tmp_3.cast_fp16@GRAD']} = conv2d_grad(inputs={Filter=['conv2d_1.w_0'], Input=['batch_norm_1.tmp_3.cast_fp16'], Output@GRAD=['conv2d_1.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_1.w_0', 'conv2d_1.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {Out=['batch_norm_1.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_1.tmp_3.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_1.b_0@GRAD'], Scale@GRAD=['batch_norm_1.w_0@GRAD'], X@GRAD=['prelu_0.tmp_0@GRAD@RENAME@block0@1']} = batch_norm_grad(inputs={Bias=['batch_norm_1.b_0'], MeanOut=['batch_norm_1.w_1'], ReserveSpace=['batch_norm_1.tmp_2'], SavedMean=['batch_norm_1.tmp_0'], SavedVariance=['batch_norm_1.tmp_1'], Scale=['batch_norm_1.w_0'], VarianceOut=['batch_norm_1.w_2'], X=['prelu_0.tmp_0'], Y@GRAD=['batch_norm_1.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_1.b_0', 'batch_norm_1.b_0@GRAD', 'batch_norm_1.w_0', 'batch_norm_1.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Out=['prelu_0.tmp_0@GRAD']} = grad_add(inputs={X=['prelu_0.tmp_0@GRAD@RENAME@block0@0'], Y=['prelu_0.tmp_0@GRAD@RENAME@block0@1']}, axis = -1, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {Alpha@GRAD=['prelu_0.w_0.cast_fp32@GRAD'], X@GRAD=['batch_norm_0.tmp_3.cast_fp32@GRAD']} = prelu_grad(inputs={Alpha=['prelu_0.w_0.cast_fp32'], Out@GRAD=['prelu_0.tmp_0@GRAD'], X=['batch_norm_0.tmp_3.cast_fp32']}, data_format = NCHW, mode = all, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['prelu_0.w_0@GRAD']} = cast(inputs={X=['prelu_0.w_0.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = ['prelu_0.w_0', 'prelu_0.w_0@GRAD'], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Out=['batch_norm_0.tmp_3@GRAD']} = cast(inputs={X=['batch_norm_0.tmp_3.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False, with_quant_attr = False)
    {Bias@GRAD=['batch_norm_0.b_0@GRAD'], Scale@GRAD=['batch_norm_0.w_0@GRAD'], X@GRAD=['conv2d_0.tmp_0@GRAD']} = batch_norm_grad(inputs={Bias=['batch_norm_0.b_0'], MeanOut=['batch_norm_0.w_1'], ReserveSpace=['batch_norm_0.tmp_2'], SavedMean=['batch_norm_0.tmp_0'], SavedVariance=['batch_norm_0.tmp_1'], Scale=['batch_norm_0.w_0'], VarianceOut=['batch_norm_0.w_2'], X=['conv2d_0.tmp_0'], Y@GRAD=['batch_norm_0.tmp_3@GRAD']}, data_layout = NCHW, epsilon = 9.999999747378752e-06, is_test = False, momentum = 0.8999999761581421, op_device = , op_namescope = /, op_role = 1, op_role_var = ['batch_norm_0.b_0', 'batch_norm_0.b_0@GRAD', 'batch_norm_0.w_0', 'batch_norm_0.w_0@GRAD'], trainable_statistics = False, use_global_stats = False, with_quant_attr = False)
    {Filter@GRAD=['conv2d_0.w_0@GRAD'], Input@GRAD=[]} = conv2d_grad(inputs={Filter=['conv2d_0.w_0'], Input=['image'], Output@GRAD=['conv2d_0.tmp_0@GRAD']}, data_format = NCHW, dilations = [1, 1], groups = 1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['conv2d_0.w_0', 'conv2d_0.w_0@GRAD'], padding_algorithm = EXPLICIT, paddings = [1, 1], strides = [1, 1], with_quant_attr = False)
    {FoundInfinite=['find_infinite_scale.tmp_0'], Out=['conv2d_0.w_0@GRAD', 'batch_norm_0.w_0@GRAD', 'batch_norm_0.b_0@GRAD', 'prelu_0.w_0@GRAD', 'batch_norm_1.w_0@GRAD', 'batch_norm_1.b_0@GRAD', 'conv2d_1.w_0@GRAD', 'batch_norm_2.w_0@GRAD', 'batch_norm_2.b_0@GRAD', 'prelu_1.w_0@GRAD', 'conv2d_2.w_0@GRAD', 'batch_norm_3.w_0@GRAD', 'batch_norm_3.b_0@GRAD', 'conv2d_3.w_0@GRAD', 'batch_norm_4.w_0@GRAD', 'batch_norm_4.b_0@GRAD', 'batch_norm_5.w_0@GRAD', 'batch_norm_5.b_0@GRAD', 'conv2d_4.w_0@GRAD', 'batch_norm_6.w_0@GRAD', 'batch_norm_6.b_0@GRAD', 'prelu_2.w_0@GRAD', 'conv2d_5.w_0@GRAD', 'batch_norm_7.w_0@GRAD', 'batch_norm_7.b_0@GRAD', 'batch_norm_8.w_0@GRAD', 'batch_norm_8.b_0@GRAD', 'conv2d_6.w_0@GRAD', 'batch_norm_9.w_0@GRAD', 'batch_norm_9.b_0@GRAD', 'prelu_3.w_0@GRAD', 'conv2d_7.w_0@GRAD', 'batch_norm_10.w_0@GRAD', 'batch_norm_10.b_0@GRAD', 'batch_norm_11.w_0@GRAD', 'batch_norm_11.b_0@GRAD', 'conv2d_8.w_0@GRAD', 'batch_norm_12.w_0@GRAD', 'batch_norm_12.b_0@GRAD', 'prelu_4.w_0@GRAD', 'conv2d_9.w_0@GRAD', 'batch_norm_13.w_0@GRAD', 'batch_norm_13.b_0@GRAD', 'conv2d_10.w_0@GRAD', 'batch_norm_14.w_0@GRAD', 'batch_norm_14.b_0@GRAD', 'batch_norm_15.w_0@GRAD', 'batch_norm_15.b_0@GRAD', 'conv2d_11.w_0@GRAD', 'batch_norm_16.w_0@GRAD', 'batch_norm_16.b_0@GRAD', 'prelu_5.w_0@GRAD', 'conv2d_12.w_0@GRAD', 'batch_norm_17.w_0@GRAD', 'batch_norm_17.b_0@GRAD', 'batch_norm_18.w_0@GRAD', 'batch_norm_18.b_0@GRAD', 'conv2d_13.w_0@GRAD', 'batch_norm_19.w_0@GRAD', 'batch_norm_19.b_0@GRAD', 'prelu_6.w_0@GRAD', 'conv2d_14.w_0@GRAD', 'batch_norm_20.w_0@GRAD', 'batch_norm_20.b_0@GRAD', 'batch_norm_21.w_0@GRAD', 'batch_norm_21.b_0@GRAD', 'conv2d_15.w_0@GRAD', 'batch_norm_22.w_0@GRAD', 'batch_norm_22.b_0@GRAD', 'prelu_7.w_0@GRAD', 'conv2d_16.w_0@GRAD', 'batch_norm_23.w_0@GRAD', 'batch_norm_23.b_0@GRAD', 'batch_norm_24.w_0@GRAD', 'batch_norm_24.b_0@GRAD', 'conv2d_17.w_0@GRAD', 'batch_norm_25.w_0@GRAD', 'batch_norm_25.b_0@GRAD', 'prelu_8.w_0@GRAD', 'conv2d_18.w_0@GRAD', 'batch_norm_26.w_0@GRAD', 'batch_norm_26.b_0@GRAD', 'conv2d_19.w_0@GRAD', 'batch_norm_27.w_0@GRAD', 'batch_norm_27.b_0@GRAD', 'batch_norm_28.w_0@GRAD', 'batch_norm_28.b_0@GRAD', 'conv2d_20.w_0@GRAD', 'batch_norm_29.w_0@GRAD', 'batch_norm_29.b_0@GRAD', 'prelu_9.w_0@GRAD', 'conv2d_21.w_0@GRAD', 'batch_norm_30.w_0@GRAD', 'batch_norm_30.b_0@GRAD', 'batch_norm_31.w_0@GRAD', 'batch_norm_31.b_0@GRAD', 'conv2d_22.w_0@GRAD', 'batch_norm_32.w_0@GRAD', 'batch_norm_32.b_0@GRAD', 'prelu_10.w_0@GRAD', 'conv2d_23.w_0@GRAD', 'batch_norm_33.w_0@GRAD', 'batch_norm_33.b_0@GRAD', 'batch_norm_34.w_0@GRAD', 'batch_norm_34.b_0@GRAD', 'conv2d_24.w_0@GRAD', 'batch_norm_35.w_0@GRAD', 'batch_norm_35.b_0@GRAD', 'prelu_11.w_0@GRAD', 'conv2d_25.w_0@GRAD', 'batch_norm_36.w_0@GRAD', 'batch_norm_36.b_0@GRAD', 'batch_norm_37.w_0@GRAD', 'batch_norm_37.b_0@GRAD', 'conv2d_26.w_0@GRAD', 'batch_norm_38.w_0@GRAD', 'batch_norm_38.b_0@GRAD', 'prelu_12.w_0@GRAD', 'conv2d_27.w_0@GRAD', 'batch_norm_39.w_0@GRAD', 'batch_norm_39.b_0@GRAD', 'batch_norm_40.w_0@GRAD', 'batch_norm_40.b_0@GRAD', 'conv2d_28.w_0@GRAD', 'batch_norm_41.w_0@GRAD', 'batch_norm_41.b_0@GRAD', 'prelu_13.w_0@GRAD', 'conv2d_29.w_0@GRAD', 'batch_norm_42.w_0@GRAD', 'batch_norm_42.b_0@GRAD', 'batch_norm_43.w_0@GRAD', 'batch_norm_43.b_0@GRAD', 'conv2d_30.w_0@GRAD', 'batch_norm_44.w_0@GRAD', 'batch_norm_44.b_0@GRAD', 'prelu_14.w_0@GRAD', 'conv2d_31.w_0@GRAD', 'batch_norm_45.w_0@GRAD', 'batch_norm_45.b_0@GRAD', 'batch_norm_46.w_0@GRAD', 'batch_norm_46.b_0@GRAD', 'conv2d_32.w_0@GRAD', 'batch_norm_47.w_0@GRAD', 'batch_norm_47.b_0@GRAD', 'prelu_15.w_0@GRAD', 'conv2d_33.w_0@GRAD', 'batch_norm_48.w_0@GRAD', 'batch_norm_48.b_0@GRAD', 'batch_norm_49.w_0@GRAD', 'batch_norm_49.b_0@GRAD', 'conv2d_34.w_0@GRAD', 'batch_norm_50.w_0@GRAD', 'batch_norm_50.b_0@GRAD', 'prelu_16.w_0@GRAD', 'conv2d_35.w_0@GRAD', 'batch_norm_51.w_0@GRAD', 'batch_norm_51.b_0@GRAD', 'batch_norm_52.w_0@GRAD', 'batch_norm_52.b_0@GRAD', 'conv2d_36.w_0@GRAD', 'batch_norm_53.w_0@GRAD', 'batch_norm_53.b_0@GRAD', 'prelu_17.w_0@GRAD', 'conv2d_37.w_0@GRAD', 'batch_norm_54.w_0@GRAD', 'batch_norm_54.b_0@GRAD', 'batch_norm_55.w_0@GRAD', 'batch_norm_55.b_0@GRAD', 'conv2d_38.w_0@GRAD', 'batch_norm_56.w_0@GRAD', 'batch_norm_56.b_0@GRAD', 'prelu_18.w_0@GRAD', 'conv2d_39.w_0@GRAD', 'batch_norm_57.w_0@GRAD', 'batch_norm_57.b_0@GRAD', 'batch_norm_58.w_0@GRAD', 'batch_norm_58.b_0@GRAD', 'conv2d_40.w_0@GRAD', 'batch_norm_59.w_0@GRAD', 'batch_norm_59.b_0@GRAD', 'prelu_19.w_0@GRAD', 'conv2d_41.w_0@GRAD', 'batch_norm_60.w_0@GRAD', 'batch_norm_60.b_0@GRAD', 'batch_norm_61.w_0@GRAD', 'batch_norm_61.b_0@GRAD', 'conv2d_42.w_0@GRAD', 'batch_norm_62.w_0@GRAD', 'batch_norm_62.b_0@GRAD', 'prelu_20.w_0@GRAD', 'conv2d_43.w_0@GRAD', 'batch_norm_63.w_0@GRAD', 'batch_norm_63.b_0@GRAD', 'batch_norm_64.w_0@GRAD', 'batch_norm_64.b_0@GRAD', 'conv2d_44.w_0@GRAD', 'batch_norm_65.w_0@GRAD', 'batch_norm_65.b_0@GRAD', 'prelu_21.w_0@GRAD', 'conv2d_45.w_0@GRAD', 'batch_norm_66.w_0@GRAD', 'batch_norm_66.b_0@GRAD', 'batch_norm_67.w_0@GRAD', 'batch_norm_67.b_0@GRAD', 'conv2d_46.w_0@GRAD', 'batch_norm_68.w_0@GRAD', 'batch_norm_68.b_0@GRAD', 'prelu_22.w_0@GRAD', 'conv2d_47.w_0@GRAD', 'batch_norm_69.w_0@GRAD', 'batch_norm_69.b_0@GRAD', 'conv2d_48.w_0@GRAD', 'batch_norm_70.w_0@GRAD', 'batch_norm_70.b_0@GRAD', 'batch_norm_71.w_0@GRAD', 'batch_norm_71.b_0@GRAD', 'conv2d_49.w_0@GRAD', 'batch_norm_72.w_0@GRAD', 'batch_norm_72.b_0@GRAD', 'prelu_23.w_0@GRAD', 'conv2d_50.w_0@GRAD', 'batch_norm_73.w_0@GRAD', 'batch_norm_73.b_0@GRAD', 'batch_norm_74.w_0@GRAD', 'batch_norm_74.b_0@GRAD', 'conv2d_51.w_0@GRAD', 'batch_norm_75.w_0@GRAD', 'batch_norm_75.b_0@GRAD', 'prelu_24.w_0@GRAD', 'conv2d_52.w_0@GRAD', 'batch_norm_76.w_0@GRAD', 'batch_norm_76.b_0@GRAD', 'batch_norm_77.w_0@GRAD', 'batch_norm_77.b_0@GRAD', 'fc_0.w_0@GRAD', 'fc_0.b_0@GRAD', 'batch_norm_78.w_0@GRAD', 'batch_norm_78.b_0@GRAD']} = check_finite_and_unscale(inputs={Scale=['loss_scaling_0'], X=['conv2d_0.w_0@GRAD', 'batch_norm_0.w_0@GRAD', 'batch_norm_0.b_0@GRAD', 'prelu_0.w_0@GRAD', 'batch_norm_1.w_0@GRAD', 'batch_norm_1.b_0@GRAD', 'conv2d_1.w_0@GRAD', 'batch_norm_2.w_0@GRAD', 'batch_norm_2.b_0@GRAD', 'prelu_1.w_0@GRAD', 'conv2d_2.w_0@GRAD', 'batch_norm_3.w_0@GRAD', 'batch_norm_3.b_0@GRAD', 'conv2d_3.w_0@GRAD', 'batch_norm_4.w_0@GRAD', 'batch_norm_4.b_0@GRAD', 'batch_norm_5.w_0@GRAD', 'batch_norm_5.b_0@GRAD', 'conv2d_4.w_0@GRAD', 'batch_norm_6.w_0@GRAD', 'batch_norm_6.b_0@GRAD', 'prelu_2.w_0@GRAD', 'conv2d_5.w_0@GRAD', 'batch_norm_7.w_0@GRAD', 'batch_norm_7.b_0@GRAD', 'batch_norm_8.w_0@GRAD', 'batch_norm_8.b_0@GRAD', 'conv2d_6.w_0@GRAD', 'batch_norm_9.w_0@GRAD', 'batch_norm_9.b_0@GRAD', 'prelu_3.w_0@GRAD', 'conv2d_7.w_0@GRAD', 'batch_norm_10.w_0@GRAD', 'batch_norm_10.b_0@GRAD', 'batch_norm_11.w_0@GRAD', 'batch_norm_11.b_0@GRAD', 'conv2d_8.w_0@GRAD', 'batch_norm_12.w_0@GRAD', 'batch_norm_12.b_0@GRAD', 'prelu_4.w_0@GRAD', 'conv2d_9.w_0@GRAD', 'batch_norm_13.w_0@GRAD', 'batch_norm_13.b_0@GRAD', 'conv2d_10.w_0@GRAD', 'batch_norm_14.w_0@GRAD', 'batch_norm_14.b_0@GRAD', 'batch_norm_15.w_0@GRAD', 'batch_norm_15.b_0@GRAD', 'conv2d_11.w_0@GRAD', 'batch_norm_16.w_0@GRAD', 'batch_norm_16.b_0@GRAD', 'prelu_5.w_0@GRAD', 'conv2d_12.w_0@GRAD', 'batch_norm_17.w_0@GRAD', 'batch_norm_17.b_0@GRAD', 'batch_norm_18.w_0@GRAD', 'batch_norm_18.b_0@GRAD', 'conv2d_13.w_0@GRAD', 'batch_norm_19.w_0@GRAD', 'batch_norm_19.b_0@GRAD', 'prelu_6.w_0@GRAD', 'conv2d_14.w_0@GRAD', 'batch_norm_20.w_0@GRAD', 'batch_norm_20.b_0@GRAD', 'batch_norm_21.w_0@GRAD', 'batch_norm_21.b_0@GRAD', 'conv2d_15.w_0@GRAD', 'batch_norm_22.w_0@GRAD', 'batch_norm_22.b_0@GRAD', 'prelu_7.w_0@GRAD', 'conv2d_16.w_0@GRAD', 'batch_norm_23.w_0@GRAD', 'batch_norm_23.b_0@GRAD', 'batch_norm_24.w_0@GRAD', 'batch_norm_24.b_0@GRAD', 'conv2d_17.w_0@GRAD', 'batch_norm_25.w_0@GRAD', 'batch_norm_25.b_0@GRAD', 'prelu_8.w_0@GRAD', 'conv2d_18.w_0@GRAD', 'batch_norm_26.w_0@GRAD', 'batch_norm_26.b_0@GRAD', 'conv2d_19.w_0@GRAD', 'batch_norm_27.w_0@GRAD', 'batch_norm_27.b_0@GRAD', 'batch_norm_28.w_0@GRAD', 'batch_norm_28.b_0@GRAD', 'conv2d_20.w_0@GRAD', 'batch_norm_29.w_0@GRAD', 'batch_norm_29.b_0@GRAD', 'prelu_9.w_0@GRAD', 'conv2d_21.w_0@GRAD', 'batch_norm_30.w_0@GRAD', 'batch_norm_30.b_0@GRAD', 'batch_norm_31.w_0@GRAD', 'batch_norm_31.b_0@GRAD', 'conv2d_22.w_0@GRAD', 'batch_norm_32.w_0@GRAD', 'batch_norm_32.b_0@GRAD', 'prelu_10.w_0@GRAD', 'conv2d_23.w_0@GRAD', 'batch_norm_33.w_0@GRAD', 'batch_norm_33.b_0@GRAD', 'batch_norm_34.w_0@GRAD', 'batch_norm_34.b_0@GRAD', 'conv2d_24.w_0@GRAD', 'batch_norm_35.w_0@GRAD', 'batch_norm_35.b_0@GRAD', 'prelu_11.w_0@GRAD', 'conv2d_25.w_0@GRAD', 'batch_norm_36.w_0@GRAD', 'batch_norm_36.b_0@GRAD', 'batch_norm_37.w_0@GRAD', 'batch_norm_37.b_0@GRAD', 'conv2d_26.w_0@GRAD', 'batch_norm_38.w_0@GRAD', 'batch_norm_38.b_0@GRAD', 'prelu_12.w_0@GRAD', 'conv2d_27.w_0@GRAD', 'batch_norm_39.w_0@GRAD', 'batch_norm_39.b_0@GRAD', 'batch_norm_40.w_0@GRAD', 'batch_norm_40.b_0@GRAD', 'conv2d_28.w_0@GRAD', 'batch_norm_41.w_0@GRAD', 'batch_norm_41.b_0@GRAD', 'prelu_13.w_0@GRAD', 'conv2d_29.w_0@GRAD', 'batch_norm_42.w_0@GRAD', 'batch_norm_42.b_0@GRAD', 'batch_norm_43.w_0@GRAD', 'batch_norm_43.b_0@GRAD', 'conv2d_30.w_0@GRAD', 'batch_norm_44.w_0@GRAD', 'batch_norm_44.b_0@GRAD', 'prelu_14.w_0@GRAD', 'conv2d_31.w_0@GRAD', 'batch_norm_45.w_0@GRAD', 'batch_norm_45.b_0@GRAD', 'batch_norm_46.w_0@GRAD', 'batch_norm_46.b_0@GRAD', 'conv2d_32.w_0@GRAD', 'batch_norm_47.w_0@GRAD', 'batch_norm_47.b_0@GRAD', 'prelu_15.w_0@GRAD', 'conv2d_33.w_0@GRAD', 'batch_norm_48.w_0@GRAD', 'batch_norm_48.b_0@GRAD', 'batch_norm_49.w_0@GRAD', 'batch_norm_49.b_0@GRAD', 'conv2d_34.w_0@GRAD', 'batch_norm_50.w_0@GRAD', 'batch_norm_50.b_0@GRAD', 'prelu_16.w_0@GRAD', 'conv2d_35.w_0@GRAD', 'batch_norm_51.w_0@GRAD', 'batch_norm_51.b_0@GRAD', 'batch_norm_52.w_0@GRAD', 'batch_norm_52.b_0@GRAD', 'conv2d_36.w_0@GRAD', 'batch_norm_53.w_0@GRAD', 'batch_norm_53.b_0@GRAD', 'prelu_17.w_0@GRAD', 'conv2d_37.w_0@GRAD', 'batch_norm_54.w_0@GRAD', 'batch_norm_54.b_0@GRAD', 'batch_norm_55.w_0@GRAD', 'batch_norm_55.b_0@GRAD', 'conv2d_38.w_0@GRAD', 'batch_norm_56.w_0@GRAD', 'batch_norm_56.b_0@GRAD', 'prelu_18.w_0@GRAD', 'conv2d_39.w_0@GRAD', 'batch_norm_57.w_0@GRAD', 'batch_norm_57.b_0@GRAD', 'batch_norm_58.w_0@GRAD', 'batch_norm_58.b_0@GRAD', 'conv2d_40.w_0@GRAD', 'batch_norm_59.w_0@GRAD', 'batch_norm_59.b_0@GRAD', 'prelu_19.w_0@GRAD', 'conv2d_41.w_0@GRAD', 'batch_norm_60.w_0@GRAD', 'batch_norm_60.b_0@GRAD', 'batch_norm_61.w_0@GRAD', 'batch_norm_61.b_0@GRAD', 'conv2d_42.w_0@GRAD', 'batch_norm_62.w_0@GRAD', 'batch_norm_62.b_0@GRAD', 'prelu_20.w_0@GRAD', 'conv2d_43.w_0@GRAD', 'batch_norm_63.w_0@GRAD', 'batch_norm_63.b_0@GRAD', 'batch_norm_64.w_0@GRAD', 'batch_norm_64.b_0@GRAD', 'conv2d_44.w_0@GRAD', 'batch_norm_65.w_0@GRAD', 'batch_norm_65.b_0@GRAD', 'prelu_21.w_0@GRAD', 'conv2d_45.w_0@GRAD', 'batch_norm_66.w_0@GRAD', 'batch_norm_66.b_0@GRAD', 'batch_norm_67.w_0@GRAD', 'batch_norm_67.b_0@GRAD', 'conv2d_46.w_0@GRAD', 'batch_norm_68.w_0@GRAD', 'batch_norm_68.b_0@GRAD', 'prelu_22.w_0@GRAD', 'conv2d_47.w_0@GRAD', 'batch_norm_69.w_0@GRAD', 'batch_norm_69.b_0@GRAD', 'conv2d_48.w_0@GRAD', 'batch_norm_70.w_0@GRAD', 'batch_norm_70.b_0@GRAD', 'batch_norm_71.w_0@GRAD', 'batch_norm_71.b_0@GRAD', 'conv2d_49.w_0@GRAD', 'batch_norm_72.w_0@GRAD', 'batch_norm_72.b_0@GRAD', 'prelu_23.w_0@GRAD', 'conv2d_50.w_0@GRAD', 'batch_norm_73.w_0@GRAD', 'batch_norm_73.b_0@GRAD', 'batch_norm_74.w_0@GRAD', 'batch_norm_74.b_0@GRAD', 'conv2d_51.w_0@GRAD', 'batch_norm_75.w_0@GRAD', 'batch_norm_75.b_0@GRAD', 'prelu_24.w_0@GRAD', 'conv2d_52.w_0@GRAD', 'batch_norm_76.w_0@GRAD', 'batch_norm_76.b_0@GRAD', 'batch_norm_77.w_0@GRAD', 'batch_norm_77.b_0@GRAD', 'fc_0.w_0@GRAD', 'fc_0.b_0@GRAD', 'batch_norm_78.w_0@GRAD', 'batch_norm_78.b_0@GRAD']}, op_device = , op_namescope = /, op_role = 2, op_role_var = ['conv2d_0.w_0@GRAD', 'batch_norm_0.w_0@GRAD', 'batch_norm_0.b_0@GRAD', 'prelu_0.w_0@GRAD', 'batch_norm_1.w_0@GRAD', 'batch_norm_1.b_0@GRAD', 'conv2d_1.w_0@GRAD', 'batch_norm_2.w_0@GRAD', 'batch_norm_2.b_0@GRAD', 'prelu_1.w_0@GRAD', 'conv2d_2.w_0@GRAD', 'batch_norm_3.w_0@GRAD', 'batch_norm_3.b_0@GRAD', 'conv2d_3.w_0@GRAD', 'batch_norm_4.w_0@GRAD', 'batch_norm_4.b_0@GRAD', 'batch_norm_5.w_0@GRAD', 'batch_norm_5.b_0@GRAD', 'conv2d_4.w_0@GRAD', 'batch_norm_6.w_0@GRAD', 'batch_norm_6.b_0@GRAD', 'prelu_2.w_0@GRAD', 'conv2d_5.w_0@GRAD', 'batch_norm_7.w_0@GRAD', 'batch_norm_7.b_0@GRAD', 'batch_norm_8.w_0@GRAD', 'batch_norm_8.b_0@GRAD', 'conv2d_6.w_0@GRAD', 'batch_norm_9.w_0@GRAD', 'batch_norm_9.b_0@GRAD', 'prelu_3.w_0@GRAD', 'conv2d_7.w_0@GRAD', 'batch_norm_10.w_0@GRAD', 'batch_norm_10.b_0@GRAD', 'batch_norm_11.w_0@GRAD', 'batch_norm_11.b_0@GRAD', 'conv2d_8.w_0@GRAD', 'batch_norm_12.w_0@GRAD', 'batch_norm_12.b_0@GRAD', 'prelu_4.w_0@GRAD', 'conv2d_9.w_0@GRAD', 'batch_norm_13.w_0@GRAD', 'batch_norm_13.b_0@GRAD', 'conv2d_10.w_0@GRAD', 'batch_norm_14.w_0@GRAD', 'batch_norm_14.b_0@GRAD', 'batch_norm_15.w_0@GRAD', 'batch_norm_15.b_0@GRAD', 'conv2d_11.w_0@GRAD', 'batch_norm_16.w_0@GRAD', 'batch_norm_16.b_0@GRAD', 'prelu_5.w_0@GRAD', 'conv2d_12.w_0@GRAD', 'batch_norm_17.w_0@GRAD', 'batch_norm_17.b_0@GRAD', 'batch_norm_18.w_0@GRAD', 'batch_norm_18.b_0@GRAD', 'conv2d_13.w_0@GRAD', 'batch_norm_19.w_0@GRAD', 'batch_norm_19.b_0@GRAD', 'prelu_6.w_0@GRAD', 'conv2d_14.w_0@GRAD', 'batch_norm_20.w_0@GRAD', 'batch_norm_20.b_0@GRAD', 'batch_norm_21.w_0@GRAD', 'batch_norm_21.b_0@GRAD', 'conv2d_15.w_0@GRAD', 'batch_norm_22.w_0@GRAD', 'batch_norm_22.b_0@GRAD', 'prelu_7.w_0@GRAD', 'conv2d_16.w_0@GRAD', 'batch_norm_23.w_0@GRAD', 'batch_norm_23.b_0@GRAD', 'batch_norm_24.w_0@GRAD', 'batch_norm_24.b_0@GRAD', 'conv2d_17.w_0@GRAD', 'batch_norm_25.w_0@GRAD', 'batch_norm_25.b_0@GRAD', 'prelu_8.w_0@GRAD', 'conv2d_18.w_0@GRAD', 'batch_norm_26.w_0@GRAD', 'batch_norm_26.b_0@GRAD', 'conv2d_19.w_0@GRAD', 'batch_norm_27.w_0@GRAD', 'batch_norm_27.b_0@GRAD', 'batch_norm_28.w_0@GRAD', 'batch_norm_28.b_0@GRAD', 'conv2d_20.w_0@GRAD', 'batch_norm_29.w_0@GRAD', 'batch_norm_29.b_0@GRAD', 'prelu_9.w_0@GRAD', 'conv2d_21.w_0@GRAD', 'batch_norm_30.w_0@GRAD', 'batch_norm_30.b_0@GRAD', 'batch_norm_31.w_0@GRAD', 'batch_norm_31.b_0@GRAD', 'conv2d_22.w_0@GRAD', 'batch_norm_32.w_0@GRAD', 'batch_norm_32.b_0@GRAD', 'prelu_10.w_0@GRAD', 'conv2d_23.w_0@GRAD', 'batch_norm_33.w_0@GRAD', 'batch_norm_33.b_0@GRAD', 'batch_norm_34.w_0@GRAD', 'batch_norm_34.b_0@GRAD', 'conv2d_24.w_0@GRAD', 'batch_norm_35.w_0@GRAD', 'batch_norm_35.b_0@GRAD', 'prelu_11.w_0@GRAD', 'conv2d_25.w_0@GRAD', 'batch_norm_36.w_0@GRAD', 'batch_norm_36.b_0@GRAD', 'batch_norm_37.w_0@GRAD', 'batch_norm_37.b_0@GRAD', 'conv2d_26.w_0@GRAD', 'batch_norm_38.w_0@GRAD', 'batch_norm_38.b_0@GRAD', 'prelu_12.w_0@GRAD', 'conv2d_27.w_0@GRAD', 'batch_norm_39.w_0@GRAD', 'batch_norm_39.b_0@GRAD', 'batch_norm_40.w_0@GRAD', 'batch_norm_40.b_0@GRAD', 'conv2d_28.w_0@GRAD', 'batch_norm_41.w_0@GRAD', 'batch_norm_41.b_0@GRAD', 'prelu_13.w_0@GRAD', 'conv2d_29.w_0@GRAD', 'batch_norm_42.w_0@GRAD', 'batch_norm_42.b_0@GRAD', 'batch_norm_43.w_0@GRAD', 'batch_norm_43.b_0@GRAD', 'conv2d_30.w_0@GRAD', 'batch_norm_44.w_0@GRAD', 'batch_norm_44.b_0@GRAD', 'prelu_14.w_0@GRAD', 'conv2d_31.w_0@GRAD', 'batch_norm_45.w_0@GRAD', 'batch_norm_45.b_0@GRAD', 'batch_norm_46.w_0@GRAD', 'batch_norm_46.b_0@GRAD', 'conv2d_32.w_0@GRAD', 'batch_norm_47.w_0@GRAD', 'batch_norm_47.b_0@GRAD', 'prelu_15.w_0@GRAD', 'conv2d_33.w_0@GRAD', 'batch_norm_48.w_0@GRAD', 'batch_norm_48.b_0@GRAD', 'batch_norm_49.w_0@GRAD', 'batch_norm_49.b_0@GRAD', 'conv2d_34.w_0@GRAD', 'batch_norm_50.w_0@GRAD', 'batch_norm_50.b_0@GRAD', 'prelu_16.w_0@GRAD', 'conv2d_35.w_0@GRAD', 'batch_norm_51.w_0@GRAD', 'batch_norm_51.b_0@GRAD', 'batch_norm_52.w_0@GRAD', 'batch_norm_52.b_0@GRAD', 'conv2d_36.w_0@GRAD', 'batch_norm_53.w_0@GRAD', 'batch_norm_53.b_0@GRAD', 'prelu_17.w_0@GRAD', 'conv2d_37.w_0@GRAD', 'batch_norm_54.w_0@GRAD', 'batch_norm_54.b_0@GRAD', 'batch_norm_55.w_0@GRAD', 'batch_norm_55.b_0@GRAD', 'conv2d_38.w_0@GRAD', 'batch_norm_56.w_0@GRAD', 'batch_norm_56.b_0@GRAD', 'prelu_18.w_0@GRAD', 'conv2d_39.w_0@GRAD', 'batch_norm_57.w_0@GRAD', 'batch_norm_57.b_0@GRAD', 'batch_norm_58.w_0@GRAD', 'batch_norm_58.b_0@GRAD', 'conv2d_40.w_0@GRAD', 'batch_norm_59.w_0@GRAD', 'batch_norm_59.b_0@GRAD', 'prelu_19.w_0@GRAD', 'conv2d_41.w_0@GRAD', 'batch_norm_60.w_0@GRAD', 'batch_norm_60.b_0@GRAD', 'batch_norm_61.w_0@GRAD', 'batch_norm_61.b_0@GRAD', 'conv2d_42.w_0@GRAD', 'batch_norm_62.w_0@GRAD', 'batch_norm_62.b_0@GRAD', 'prelu_20.w_0@GRAD', 'conv2d_43.w_0@GRAD', 'batch_norm_63.w_0@GRAD', 'batch_norm_63.b_0@GRAD', 'batch_norm_64.w_0@GRAD', 'batch_norm_64.b_0@GRAD', 'conv2d_44.w_0@GRAD', 'batch_norm_65.w_0@GRAD', 'batch_norm_65.b_0@GRAD', 'prelu_21.w_0@GRAD', 'conv2d_45.w_0@GRAD', 'batch_norm_66.w_0@GRAD', 'batch_norm_66.b_0@GRAD', 'batch_norm_67.w_0@GRAD', 'batch_norm_67.b_0@GRAD', 'conv2d_46.w_0@GRAD', 'batch_norm_68.w_0@GRAD', 'batch_norm_68.b_0@GRAD', 'prelu_22.w_0@GRAD', 'conv2d_47.w_0@GRAD', 'batch_norm_69.w_0@GRAD', 'batch_norm_69.b_0@GRAD', 'conv2d_48.w_0@GRAD', 'batch_norm_70.w_0@GRAD', 'batch_norm_70.b_0@GRAD', 'batch_norm_71.w_0@GRAD', 'batch_norm_71.b_0@GRAD', 'conv2d_49.w_0@GRAD', 'batch_norm_72.w_0@GRAD', 'batch_norm_72.b_0@GRAD', 'prelu_23.w_0@GRAD', 'conv2d_50.w_0@GRAD', 'batch_norm_73.w_0@GRAD', 'batch_norm_73.b_0@GRAD', 'batch_norm_74.w_0@GRAD', 'batch_norm_74.b_0@GRAD', 'conv2d_51.w_0@GRAD', 'batch_norm_75.w_0@GRAD', 'batch_norm_75.b_0@GRAD', 'prelu_24.w_0@GRAD', 'conv2d_52.w_0@GRAD', 'batch_norm_76.w_0@GRAD', 'batch_norm_76.b_0@GRAD', 'batch_norm_77.w_0@GRAD', 'batch_norm_77.b_0@GRAD', 'fc_0.w_0@GRAD', 'fc_0.b_0@GRAD', 'batch_norm_78.w_0@GRAD', 'batch_norm_78.b_0@GRAD'], with_quant_attr = False)
    {LossScaling=['loss_scaling_0'], Out=['conv2d_0.w_0@GRAD', 'batch_norm_0.w_0@GRAD', 'batch_norm_0.b_0@GRAD', 'prelu_0.w_0@GRAD', 'batch_norm_1.w_0@GRAD', 'batch_norm_1.b_0@GRAD', 'conv2d_1.w_0@GRAD', 'batch_norm_2.w_0@GRAD', 'batch_norm_2.b_0@GRAD', 'prelu_1.w_0@GRAD', 'conv2d_2.w_0@GRAD', 'batch_norm_3.w_0@GRAD', 'batch_norm_3.b_0@GRAD', 'conv2d_3.w_0@GRAD', 'batch_norm_4.w_0@GRAD', 'batch_norm_4.b_0@GRAD', 'batch_norm_5.w_0@GRAD', 'batch_norm_5.b_0@GRAD', 'conv2d_4.w_0@GRAD', 'batch_norm_6.w_0@GRAD', 'batch_norm_6.b_0@GRAD', 'prelu_2.w_0@GRAD', 'conv2d_5.w_0@GRAD', 'batch_norm_7.w_0@GRAD', 'batch_norm_7.b_0@GRAD', 'batch_norm_8.w_0@GRAD', 'batch_norm_8.b_0@GRAD', 'conv2d_6.w_0@GRAD', 'batch_norm_9.w_0@GRAD', 'batch_norm_9.b_0@GRAD', 'prelu_3.w_0@GRAD', 'conv2d_7.w_0@GRAD', 'batch_norm_10.w_0@GRAD', 'batch_norm_10.b_0@GRAD', 'batch_norm_11.w_0@GRAD', 'batch_norm_11.b_0@GRAD', 'conv2d_8.w_0@GRAD', 'batch_norm_12.w_0@GRAD', 'batch_norm_12.b_0@GRAD', 'prelu_4.w_0@GRAD', 'conv2d_9.w_0@GRAD', 'batch_norm_13.w_0@GRAD', 'batch_norm_13.b_0@GRAD', 'conv2d_10.w_0@GRAD', 'batch_norm_14.w_0@GRAD', 'batch_norm_14.b_0@GRAD', 'batch_norm_15.w_0@GRAD', 'batch_norm_15.b_0@GRAD', 'conv2d_11.w_0@GRAD', 'batch_norm_16.w_0@GRAD', 'batch_norm_16.b_0@GRAD', 'prelu_5.w_0@GRAD', 'conv2d_12.w_0@GRAD', 'batch_norm_17.w_0@GRAD', 'batch_norm_17.b_0@GRAD', 'batch_norm_18.w_0@GRAD', 'batch_norm_18.b_0@GRAD', 'conv2d_13.w_0@GRAD', 'batch_norm_19.w_0@GRAD', 'batch_norm_19.b_0@GRAD', 'prelu_6.w_0@GRAD', 'conv2d_14.w_0@GRAD', 'batch_norm_20.w_0@GRAD', 'batch_norm_20.b_0@GRAD', 'batch_norm_21.w_0@GRAD', 'batch_norm_21.b_0@GRAD', 'conv2d_15.w_0@GRAD', 'batch_norm_22.w_0@GRAD', 'batch_norm_22.b_0@GRAD', 'prelu_7.w_0@GRAD', 'conv2d_16.w_0@GRAD', 'batch_norm_23.w_0@GRAD', 'batch_norm_23.b_0@GRAD', 'batch_norm_24.w_0@GRAD', 'batch_norm_24.b_0@GRAD', 'conv2d_17.w_0@GRAD', 'batch_norm_25.w_0@GRAD', 'batch_norm_25.b_0@GRAD', 'prelu_8.w_0@GRAD', 'conv2d_18.w_0@GRAD', 'batch_norm_26.w_0@GRAD', 'batch_norm_26.b_0@GRAD', 'conv2d_19.w_0@GRAD', 'batch_norm_27.w_0@GRAD', 'batch_norm_27.b_0@GRAD', 'batch_norm_28.w_0@GRAD', 'batch_norm_28.b_0@GRAD', 'conv2d_20.w_0@GRAD', 'batch_norm_29.w_0@GRAD', 'batch_norm_29.b_0@GRAD', 'prelu_9.w_0@GRAD', 'conv2d_21.w_0@GRAD', 'batch_norm_30.w_0@GRAD', 'batch_norm_30.b_0@GRAD', 'batch_norm_31.w_0@GRAD', 'batch_norm_31.b_0@GRAD', 'conv2d_22.w_0@GRAD', 'batch_norm_32.w_0@GRAD', 'batch_norm_32.b_0@GRAD', 'prelu_10.w_0@GRAD', 'conv2d_23.w_0@GRAD', 'batch_norm_33.w_0@GRAD', 'batch_norm_33.b_0@GRAD', 'batch_norm_34.w_0@GRAD', 'batch_norm_34.b_0@GRAD', 'conv2d_24.w_0@GRAD', 'batch_norm_35.w_0@GRAD', 'batch_norm_35.b_0@GRAD', 'prelu_11.w_0@GRAD', 'conv2d_25.w_0@GRAD', 'batch_norm_36.w_0@GRAD', 'batch_norm_36.b_0@GRAD', 'batch_norm_37.w_0@GRAD', 'batch_norm_37.b_0@GRAD', 'conv2d_26.w_0@GRAD', 'batch_norm_38.w_0@GRAD', 'batch_norm_38.b_0@GRAD', 'prelu_12.w_0@GRAD', 'conv2d_27.w_0@GRAD', 'batch_norm_39.w_0@GRAD', 'batch_norm_39.b_0@GRAD', 'batch_norm_40.w_0@GRAD', 'batch_norm_40.b_0@GRAD', 'conv2d_28.w_0@GRAD', 'batch_norm_41.w_0@GRAD', 'batch_norm_41.b_0@GRAD', 'prelu_13.w_0@GRAD', 'conv2d_29.w_0@GRAD', 'batch_norm_42.w_0@GRAD', 'batch_norm_42.b_0@GRAD', 'batch_norm_43.w_0@GRAD', 'batch_norm_43.b_0@GRAD', 'conv2d_30.w_0@GRAD', 'batch_norm_44.w_0@GRAD', 'batch_norm_44.b_0@GRAD', 'prelu_14.w_0@GRAD', 'conv2d_31.w_0@GRAD', 'batch_norm_45.w_0@GRAD', 'batch_norm_45.b_0@GRAD', 'batch_norm_46.w_0@GRAD', 'batch_norm_46.b_0@GRAD', 'conv2d_32.w_0@GRAD', 'batch_norm_47.w_0@GRAD', 'batch_norm_47.b_0@GRAD', 'prelu_15.w_0@GRAD', 'conv2d_33.w_0@GRAD', 'batch_norm_48.w_0@GRAD', 'batch_norm_48.b_0@GRAD', 'batch_norm_49.w_0@GRAD', 'batch_norm_49.b_0@GRAD', 'conv2d_34.w_0@GRAD', 'batch_norm_50.w_0@GRAD', 'batch_norm_50.b_0@GRAD', 'prelu_16.w_0@GRAD', 'conv2d_35.w_0@GRAD', 'batch_norm_51.w_0@GRAD', 'batch_norm_51.b_0@GRAD', 'batch_norm_52.w_0@GRAD', 'batch_norm_52.b_0@GRAD', 'conv2d_36.w_0@GRAD', 'batch_norm_53.w_0@GRAD', 'batch_norm_53.b_0@GRAD', 'prelu_17.w_0@GRAD', 'conv2d_37.w_0@GRAD', 'batch_norm_54.w_0@GRAD', 'batch_norm_54.b_0@GRAD', 'batch_norm_55.w_0@GRAD', 'batch_norm_55.b_0@GRAD', 'conv2d_38.w_0@GRAD', 'batch_norm_56.w_0@GRAD', 'batch_norm_56.b_0@GRAD', 'prelu_18.w_0@GRAD', 'conv2d_39.w_0@GRAD', 'batch_norm_57.w_0@GRAD', 'batch_norm_57.b_0@GRAD', 'batch_norm_58.w_0@GRAD', 'batch_norm_58.b_0@GRAD', 'conv2d_40.w_0@GRAD', 'batch_norm_59.w_0@GRAD', 'batch_norm_59.b_0@GRAD', 'prelu_19.w_0@GRAD', 'conv2d_41.w_0@GRAD', 'batch_norm_60.w_0@GRAD', 'batch_norm_60.b_0@GRAD', 'batch_norm_61.w_0@GRAD', 'batch_norm_61.b_0@GRAD', 'conv2d_42.w_0@GRAD', 'batch_norm_62.w_0@GRAD', 'batch_norm_62.b_0@GRAD', 'prelu_20.w_0@GRAD', 'conv2d_43.w_0@GRAD', 'batch_norm_63.w_0@GRAD', 'batch_norm_63.b_0@GRAD', 'batch_norm_64.w_0@GRAD', 'batch_norm_64.b_0@GRAD', 'conv2d_44.w_0@GRAD', 'batch_norm_65.w_0@GRAD', 'batch_norm_65.b_0@GRAD', 'prelu_21.w_0@GRAD', 'conv2d_45.w_0@GRAD', 'batch_norm_66.w_0@GRAD', 'batch_norm_66.b_0@GRAD', 'batch_norm_67.w_0@GRAD', 'batch_norm_67.b_0@GRAD', 'conv2d_46.w_0@GRAD', 'batch_norm_68.w_0@GRAD', 'batch_norm_68.b_0@GRAD', 'prelu_22.w_0@GRAD', 'conv2d_47.w_0@GRAD', 'batch_norm_69.w_0@GRAD', 'batch_norm_69.b_0@GRAD', 'conv2d_48.w_0@GRAD', 'batch_norm_70.w_0@GRAD', 'batch_norm_70.b_0@GRAD', 'batch_norm_71.w_0@GRAD', 'batch_norm_71.b_0@GRAD', 'conv2d_49.w_0@GRAD', 'batch_norm_72.w_0@GRAD', 'batch_norm_72.b_0@GRAD', 'prelu_23.w_0@GRAD', 'conv2d_50.w_0@GRAD', 'batch_norm_73.w_0@GRAD', 'batch_norm_73.b_0@GRAD', 'batch_norm_74.w_0@GRAD', 'batch_norm_74.b_0@GRAD', 'conv2d_51.w_0@GRAD', 'batch_norm_75.w_0@GRAD', 'batch_norm_75.b_0@GRAD', 'prelu_24.w_0@GRAD', 'conv2d_52.w_0@GRAD', 'batch_norm_76.w_0@GRAD', 'batch_norm_76.b_0@GRAD', 'batch_norm_77.w_0@GRAD', 'batch_norm_77.b_0@GRAD', 'fc_0.w_0@GRAD', 'fc_0.b_0@GRAD', 'batch_norm_78.w_0@GRAD', 'batch_norm_78.b_0@GRAD'], OutBadSteps=['num_bad_steps_0'], OutGoodSteps=['num_good_steps_0']} = update_loss_scaling(inputs={FoundInfinite=['find_infinite_scale.tmp_0'], InBadSteps=['num_bad_steps_0'], InGoodSteps=['num_good_steps_0'], PrevLossScaling=['loss_scaling_0'], StopUpdate=[], X=['conv2d_0.w_0@GRAD', 'batch_norm_0.w_0@GRAD', 'batch_norm_0.b_0@GRAD', 'prelu_0.w_0@GRAD', 'batch_norm_1.w_0@GRAD', 'batch_norm_1.b_0@GRAD', 'conv2d_1.w_0@GRAD', 'batch_norm_2.w_0@GRAD', 'batch_norm_2.b_0@GRAD', 'prelu_1.w_0@GRAD', 'conv2d_2.w_0@GRAD', 'batch_norm_3.w_0@GRAD', 'batch_norm_3.b_0@GRAD', 'conv2d_3.w_0@GRAD', 'batch_norm_4.w_0@GRAD', 'batch_norm_4.b_0@GRAD', 'batch_norm_5.w_0@GRAD', 'batch_norm_5.b_0@GRAD', 'conv2d_4.w_0@GRAD', 'batch_norm_6.w_0@GRAD', 'batch_norm_6.b_0@GRAD', 'prelu_2.w_0@GRAD', 'conv2d_5.w_0@GRAD', 'batch_norm_7.w_0@GRAD', 'batch_norm_7.b_0@GRAD', 'batch_norm_8.w_0@GRAD', 'batch_norm_8.b_0@GRAD', 'conv2d_6.w_0@GRAD', 'batch_norm_9.w_0@GRAD', 'batch_norm_9.b_0@GRAD', 'prelu_3.w_0@GRAD', 'conv2d_7.w_0@GRAD', 'batch_norm_10.w_0@GRAD', 'batch_norm_10.b_0@GRAD', 'batch_norm_11.w_0@GRAD', 'batch_norm_11.b_0@GRAD', 'conv2d_8.w_0@GRAD', 'batch_norm_12.w_0@GRAD', 'batch_norm_12.b_0@GRAD', 'prelu_4.w_0@GRAD', 'conv2d_9.w_0@GRAD', 'batch_norm_13.w_0@GRAD', 'batch_norm_13.b_0@GRAD', 'conv2d_10.w_0@GRAD', 'batch_norm_14.w_0@GRAD', 'batch_norm_14.b_0@GRAD', 'batch_norm_15.w_0@GRAD', 'batch_norm_15.b_0@GRAD', 'conv2d_11.w_0@GRAD', 'batch_norm_16.w_0@GRAD', 'batch_norm_16.b_0@GRAD', 'prelu_5.w_0@GRAD', 'conv2d_12.w_0@GRAD', 'batch_norm_17.w_0@GRAD', 'batch_norm_17.b_0@GRAD', 'batch_norm_18.w_0@GRAD', 'batch_norm_18.b_0@GRAD', 'conv2d_13.w_0@GRAD', 'batch_norm_19.w_0@GRAD', 'batch_norm_19.b_0@GRAD', 'prelu_6.w_0@GRAD', 'conv2d_14.w_0@GRAD', 'batch_norm_20.w_0@GRAD', 'batch_norm_20.b_0@GRAD', 'batch_norm_21.w_0@GRAD', 'batch_norm_21.b_0@GRAD', 'conv2d_15.w_0@GRAD', 'batch_norm_22.w_0@GRAD', 'batch_norm_22.b_0@GRAD', 'prelu_7.w_0@GRAD', 'conv2d_16.w_0@GRAD', 'batch_norm_23.w_0@GRAD', 'batch_norm_23.b_0@GRAD', 'batch_norm_24.w_0@GRAD', 'batch_norm_24.b_0@GRAD', 'conv2d_17.w_0@GRAD', 'batch_norm_25.w_0@GRAD', 'batch_norm_25.b_0@GRAD', 'prelu_8.w_0@GRAD', 'conv2d_18.w_0@GRAD', 'batch_norm_26.w_0@GRAD', 'batch_norm_26.b_0@GRAD', 'conv2d_19.w_0@GRAD', 'batch_norm_27.w_0@GRAD', 'batch_norm_27.b_0@GRAD', 'batch_norm_28.w_0@GRAD', 'batch_norm_28.b_0@GRAD', 'conv2d_20.w_0@GRAD', 'batch_norm_29.w_0@GRAD', 'batch_norm_29.b_0@GRAD', 'prelu_9.w_0@GRAD', 'conv2d_21.w_0@GRAD', 'batch_norm_30.w_0@GRAD', 'batch_norm_30.b_0@GRAD', 'batch_norm_31.w_0@GRAD', 'batch_norm_31.b_0@GRAD', 'conv2d_22.w_0@GRAD', 'batch_norm_32.w_0@GRAD', 'batch_norm_32.b_0@GRAD', 'prelu_10.w_0@GRAD', 'conv2d_23.w_0@GRAD', 'batch_norm_33.w_0@GRAD', 'batch_norm_33.b_0@GRAD', 'batch_norm_34.w_0@GRAD', 'batch_norm_34.b_0@GRAD', 'conv2d_24.w_0@GRAD', 'batch_norm_35.w_0@GRAD', 'batch_norm_35.b_0@GRAD', 'prelu_11.w_0@GRAD', 'conv2d_25.w_0@GRAD', 'batch_norm_36.w_0@GRAD', 'batch_norm_36.b_0@GRAD', 'batch_norm_37.w_0@GRAD', 'batch_norm_37.b_0@GRAD', 'conv2d_26.w_0@GRAD', 'batch_norm_38.w_0@GRAD', 'batch_norm_38.b_0@GRAD', 'prelu_12.w_0@GRAD', 'conv2d_27.w_0@GRAD', 'batch_norm_39.w_0@GRAD', 'batch_norm_39.b_0@GRAD', 'batch_norm_40.w_0@GRAD', 'batch_norm_40.b_0@GRAD', 'conv2d_28.w_0@GRAD', 'batch_norm_41.w_0@GRAD', 'batch_norm_41.b_0@GRAD', 'prelu_13.w_0@GRAD', 'conv2d_29.w_0@GRAD', 'batch_norm_42.w_0@GRAD', 'batch_norm_42.b_0@GRAD', 'batch_norm_43.w_0@GRAD', 'batch_norm_43.b_0@GRAD', 'conv2d_30.w_0@GRAD', 'batch_norm_44.w_0@GRAD', 'batch_norm_44.b_0@GRAD', 'prelu_14.w_0@GRAD', 'conv2d_31.w_0@GRAD', 'batch_norm_45.w_0@GRAD', 'batch_norm_45.b_0@GRAD', 'batch_norm_46.w_0@GRAD', 'batch_norm_46.b_0@GRAD', 'conv2d_32.w_0@GRAD', 'batch_norm_47.w_0@GRAD', 'batch_norm_47.b_0@GRAD', 'prelu_15.w_0@GRAD', 'conv2d_33.w_0@GRAD', 'batch_norm_48.w_0@GRAD', 'batch_norm_48.b_0@GRAD', 'batch_norm_49.w_0@GRAD', 'batch_norm_49.b_0@GRAD', 'conv2d_34.w_0@GRAD', 'batch_norm_50.w_0@GRAD', 'batch_norm_50.b_0@GRAD', 'prelu_16.w_0@GRAD', 'conv2d_35.w_0@GRAD', 'batch_norm_51.w_0@GRAD', 'batch_norm_51.b_0@GRAD', 'batch_norm_52.w_0@GRAD', 'batch_norm_52.b_0@GRAD', 'conv2d_36.w_0@GRAD', 'batch_norm_53.w_0@GRAD', 'batch_norm_53.b_0@GRAD', 'prelu_17.w_0@GRAD', 'conv2d_37.w_0@GRAD', 'batch_norm_54.w_0@GRAD', 'batch_norm_54.b_0@GRAD', 'batch_norm_55.w_0@GRAD', 'batch_norm_55.b_0@GRAD', 'conv2d_38.w_0@GRAD', 'batch_norm_56.w_0@GRAD', 'batch_norm_56.b_0@GRAD', 'prelu_18.w_0@GRAD', 'conv2d_39.w_0@GRAD', 'batch_norm_57.w_0@GRAD', 'batch_norm_57.b_0@GRAD', 'batch_norm_58.w_0@GRAD', 'batch_norm_58.b_0@GRAD', 'conv2d_40.w_0@GRAD', 'batch_norm_59.w_0@GRAD', 'batch_norm_59.b_0@GRAD', 'prelu_19.w_0@GRAD', 'conv2d_41.w_0@GRAD', 'batch_norm_60.w_0@GRAD', 'batch_norm_60.b_0@GRAD', 'batch_norm_61.w_0@GRAD', 'batch_norm_61.b_0@GRAD', 'conv2d_42.w_0@GRAD', 'batch_norm_62.w_0@GRAD', 'batch_norm_62.b_0@GRAD', 'prelu_20.w_0@GRAD', 'conv2d_43.w_0@GRAD', 'batch_norm_63.w_0@GRAD', 'batch_norm_63.b_0@GRAD', 'batch_norm_64.w_0@GRAD', 'batch_norm_64.b_0@GRAD', 'conv2d_44.w_0@GRAD', 'batch_norm_65.w_0@GRAD', 'batch_norm_65.b_0@GRAD', 'prelu_21.w_0@GRAD', 'conv2d_45.w_0@GRAD', 'batch_norm_66.w_0@GRAD', 'batch_norm_66.b_0@GRAD', 'batch_norm_67.w_0@GRAD', 'batch_norm_67.b_0@GRAD', 'conv2d_46.w_0@GRAD', 'batch_norm_68.w_0@GRAD', 'batch_norm_68.b_0@GRAD', 'prelu_22.w_0@GRAD', 'conv2d_47.w_0@GRAD', 'batch_norm_69.w_0@GRAD', 'batch_norm_69.b_0@GRAD', 'conv2d_48.w_0@GRAD', 'batch_norm_70.w_0@GRAD', 'batch_norm_70.b_0@GRAD', 'batch_norm_71.w_0@GRAD', 'batch_norm_71.b_0@GRAD', 'conv2d_49.w_0@GRAD', 'batch_norm_72.w_0@GRAD', 'batch_norm_72.b_0@GRAD', 'prelu_23.w_0@GRAD', 'conv2d_50.w_0@GRAD', 'batch_norm_73.w_0@GRAD', 'batch_norm_73.b_0@GRAD', 'batch_norm_74.w_0@GRAD', 'batch_norm_74.b_0@GRAD', 'conv2d_51.w_0@GRAD', 'batch_norm_75.w_0@GRAD', 'batch_norm_75.b_0@GRAD', 'prelu_24.w_0@GRAD', 'conv2d_52.w_0@GRAD', 'batch_norm_76.w_0@GRAD', 'batch_norm_76.b_0@GRAD', 'batch_norm_77.w_0@GRAD', 'batch_norm_77.b_0@GRAD', 'fc_0.w_0@GRAD', 'fc_0.b_0@GRAD', 'batch_norm_78.w_0@GRAD', 'batch_norm_78.b_0@GRAD']}, decr_every_n_nan_or_inf = 1, decr_ratio = 0.5, incr_every_n_steps = 2000, incr_ratio = 2.0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], stop_update = False, with_quant_attr = False)
    {ParamOut=['batch_norm_0.b_0'], VelocityOut=['batch_norm_0.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_0.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_0.b_0'], Velocity=['batch_norm_0.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer/, op_role = 2, op_role_var = ['batch_norm_0.b_0', 'batch_norm_0.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_0.w_0'], VelocityOut=['batch_norm_0.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_0.w_0'], Velocity=['batch_norm_0.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_1/, op_role = 2, op_role_var = ['batch_norm_0.w_0', 'batch_norm_0.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_1.b_0'], VelocityOut=['batch_norm_1.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_1.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_1.b_0'], Velocity=['batch_norm_1.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_2/, op_role = 2, op_role_var = ['batch_norm_1.b_0', 'batch_norm_1.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_1.w_0'], VelocityOut=['batch_norm_1.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_1.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_1.w_0'], Velocity=['batch_norm_1.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_3/, op_role = 2, op_role_var = ['batch_norm_1.w_0', 'batch_norm_1.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_10.b_0'], VelocityOut=['batch_norm_10.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_10.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_10.b_0'], Velocity=['batch_norm_10.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_4/, op_role = 2, op_role_var = ['batch_norm_10.b_0', 'batch_norm_10.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_10.w_0'], VelocityOut=['batch_norm_10.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_10.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_10.w_0'], Velocity=['batch_norm_10.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_5/, op_role = 2, op_role_var = ['batch_norm_10.w_0', 'batch_norm_10.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_11.b_0'], VelocityOut=['batch_norm_11.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_11.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_11.b_0'], Velocity=['batch_norm_11.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_6/, op_role = 2, op_role_var = ['batch_norm_11.b_0', 'batch_norm_11.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_11.w_0'], VelocityOut=['batch_norm_11.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_11.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_11.w_0'], Velocity=['batch_norm_11.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_7/, op_role = 2, op_role_var = ['batch_norm_11.w_0', 'batch_norm_11.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_12.b_0'], VelocityOut=['batch_norm_12.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_12.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_12.b_0'], Velocity=['batch_norm_12.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_8/, op_role = 2, op_role_var = ['batch_norm_12.b_0', 'batch_norm_12.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_12.w_0'], VelocityOut=['batch_norm_12.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_12.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_12.w_0'], Velocity=['batch_norm_12.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_9/, op_role = 2, op_role_var = ['batch_norm_12.w_0', 'batch_norm_12.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_13.b_0'], VelocityOut=['batch_norm_13.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_13.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_13.b_0'], Velocity=['batch_norm_13.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_10/, op_role = 2, op_role_var = ['batch_norm_13.b_0', 'batch_norm_13.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_13.w_0'], VelocityOut=['batch_norm_13.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_13.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_13.w_0'], Velocity=['batch_norm_13.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_11/, op_role = 2, op_role_var = ['batch_norm_13.w_0', 'batch_norm_13.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_14.b_0'], VelocityOut=['batch_norm_14.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_14.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_14.b_0'], Velocity=['batch_norm_14.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_12/, op_role = 2, op_role_var = ['batch_norm_14.b_0', 'batch_norm_14.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_14.w_0'], VelocityOut=['batch_norm_14.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_14.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_14.w_0'], Velocity=['batch_norm_14.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_13/, op_role = 2, op_role_var = ['batch_norm_14.w_0', 'batch_norm_14.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_15.b_0'], VelocityOut=['batch_norm_15.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_15.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_15.b_0'], Velocity=['batch_norm_15.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_14/, op_role = 2, op_role_var = ['batch_norm_15.b_0', 'batch_norm_15.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_15.w_0'], VelocityOut=['batch_norm_15.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_15.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_15.w_0'], Velocity=['batch_norm_15.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_15/, op_role = 2, op_role_var = ['batch_norm_15.w_0', 'batch_norm_15.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_16.b_0'], VelocityOut=['batch_norm_16.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_16.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_16.b_0'], Velocity=['batch_norm_16.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_16/, op_role = 2, op_role_var = ['batch_norm_16.b_0', 'batch_norm_16.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_16.w_0'], VelocityOut=['batch_norm_16.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_16.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_16.w_0'], Velocity=['batch_norm_16.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_17/, op_role = 2, op_role_var = ['batch_norm_16.w_0', 'batch_norm_16.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_17.b_0'], VelocityOut=['batch_norm_17.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_17.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_17.b_0'], Velocity=['batch_norm_17.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_18/, op_role = 2, op_role_var = ['batch_norm_17.b_0', 'batch_norm_17.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_17.w_0'], VelocityOut=['batch_norm_17.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_17.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_17.w_0'], Velocity=['batch_norm_17.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_19/, op_role = 2, op_role_var = ['batch_norm_17.w_0', 'batch_norm_17.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_18.b_0'], VelocityOut=['batch_norm_18.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_18.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_18.b_0'], Velocity=['batch_norm_18.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_20/, op_role = 2, op_role_var = ['batch_norm_18.b_0', 'batch_norm_18.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_18.w_0'], VelocityOut=['batch_norm_18.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_18.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_18.w_0'], Velocity=['batch_norm_18.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_21/, op_role = 2, op_role_var = ['batch_norm_18.w_0', 'batch_norm_18.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_19.b_0'], VelocityOut=['batch_norm_19.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_19.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_19.b_0'], Velocity=['batch_norm_19.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_22/, op_role = 2, op_role_var = ['batch_norm_19.b_0', 'batch_norm_19.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_19.w_0'], VelocityOut=['batch_norm_19.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_19.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_19.w_0'], Velocity=['batch_norm_19.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_23/, op_role = 2, op_role_var = ['batch_norm_19.w_0', 'batch_norm_19.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_2.b_0'], VelocityOut=['batch_norm_2.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_2.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_2.b_0'], Velocity=['batch_norm_2.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_24/, op_role = 2, op_role_var = ['batch_norm_2.b_0', 'batch_norm_2.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_2.w_0'], VelocityOut=['batch_norm_2.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_2.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_2.w_0'], Velocity=['batch_norm_2.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_25/, op_role = 2, op_role_var = ['batch_norm_2.w_0', 'batch_norm_2.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_20.b_0'], VelocityOut=['batch_norm_20.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_20.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_20.b_0'], Velocity=['batch_norm_20.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_26/, op_role = 2, op_role_var = ['batch_norm_20.b_0', 'batch_norm_20.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_20.w_0'], VelocityOut=['batch_norm_20.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_20.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_20.w_0'], Velocity=['batch_norm_20.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_27/, op_role = 2, op_role_var = ['batch_norm_20.w_0', 'batch_norm_20.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_21.b_0'], VelocityOut=['batch_norm_21.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_21.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_21.b_0'], Velocity=['batch_norm_21.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_28/, op_role = 2, op_role_var = ['batch_norm_21.b_0', 'batch_norm_21.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_21.w_0'], VelocityOut=['batch_norm_21.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_21.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_21.w_0'], Velocity=['batch_norm_21.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_29/, op_role = 2, op_role_var = ['batch_norm_21.w_0', 'batch_norm_21.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_22.b_0'], VelocityOut=['batch_norm_22.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_22.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_22.b_0'], Velocity=['batch_norm_22.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_30/, op_role = 2, op_role_var = ['batch_norm_22.b_0', 'batch_norm_22.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_22.w_0'], VelocityOut=['batch_norm_22.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_22.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_22.w_0'], Velocity=['batch_norm_22.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_31/, op_role = 2, op_role_var = ['batch_norm_22.w_0', 'batch_norm_22.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_23.b_0'], VelocityOut=['batch_norm_23.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_23.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_23.b_0'], Velocity=['batch_norm_23.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_32/, op_role = 2, op_role_var = ['batch_norm_23.b_0', 'batch_norm_23.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_23.w_0'], VelocityOut=['batch_norm_23.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_23.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_23.w_0'], Velocity=['batch_norm_23.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_33/, op_role = 2, op_role_var = ['batch_norm_23.w_0', 'batch_norm_23.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_24.b_0'], VelocityOut=['batch_norm_24.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_24.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_24.b_0'], Velocity=['batch_norm_24.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_34/, op_role = 2, op_role_var = ['batch_norm_24.b_0', 'batch_norm_24.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_24.w_0'], VelocityOut=['batch_norm_24.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_24.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_24.w_0'], Velocity=['batch_norm_24.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_35/, op_role = 2, op_role_var = ['batch_norm_24.w_0', 'batch_norm_24.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_25.b_0'], VelocityOut=['batch_norm_25.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_25.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_25.b_0'], Velocity=['batch_norm_25.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_36/, op_role = 2, op_role_var = ['batch_norm_25.b_0', 'batch_norm_25.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_25.w_0'], VelocityOut=['batch_norm_25.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_25.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_25.w_0'], Velocity=['batch_norm_25.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_37/, op_role = 2, op_role_var = ['batch_norm_25.w_0', 'batch_norm_25.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_26.b_0'], VelocityOut=['batch_norm_26.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_26.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_26.b_0'], Velocity=['batch_norm_26.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_38/, op_role = 2, op_role_var = ['batch_norm_26.b_0', 'batch_norm_26.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_26.w_0'], VelocityOut=['batch_norm_26.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_26.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_26.w_0'], Velocity=['batch_norm_26.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_39/, op_role = 2, op_role_var = ['batch_norm_26.w_0', 'batch_norm_26.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_27.b_0'], VelocityOut=['batch_norm_27.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_27.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_27.b_0'], Velocity=['batch_norm_27.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_40/, op_role = 2, op_role_var = ['batch_norm_27.b_0', 'batch_norm_27.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_27.w_0'], VelocityOut=['batch_norm_27.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_27.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_27.w_0'], Velocity=['batch_norm_27.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_41/, op_role = 2, op_role_var = ['batch_norm_27.w_0', 'batch_norm_27.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_28.b_0'], VelocityOut=['batch_norm_28.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_28.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_28.b_0'], Velocity=['batch_norm_28.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_42/, op_role = 2, op_role_var = ['batch_norm_28.b_0', 'batch_norm_28.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_28.w_0'], VelocityOut=['batch_norm_28.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_28.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_28.w_0'], Velocity=['batch_norm_28.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_43/, op_role = 2, op_role_var = ['batch_norm_28.w_0', 'batch_norm_28.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_29.b_0'], VelocityOut=['batch_norm_29.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_29.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_29.b_0'], Velocity=['batch_norm_29.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_44/, op_role = 2, op_role_var = ['batch_norm_29.b_0', 'batch_norm_29.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_29.w_0'], VelocityOut=['batch_norm_29.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_29.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_29.w_0'], Velocity=['batch_norm_29.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_45/, op_role = 2, op_role_var = ['batch_norm_29.w_0', 'batch_norm_29.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_3.b_0'], VelocityOut=['batch_norm_3.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_3.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_3.b_0'], Velocity=['batch_norm_3.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_46/, op_role = 2, op_role_var = ['batch_norm_3.b_0', 'batch_norm_3.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_3.w_0'], VelocityOut=['batch_norm_3.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_3.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_3.w_0'], Velocity=['batch_norm_3.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_47/, op_role = 2, op_role_var = ['batch_norm_3.w_0', 'batch_norm_3.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_30.b_0'], VelocityOut=['batch_norm_30.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_30.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_30.b_0'], Velocity=['batch_norm_30.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_48/, op_role = 2, op_role_var = ['batch_norm_30.b_0', 'batch_norm_30.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_30.w_0'], VelocityOut=['batch_norm_30.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_30.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_30.w_0'], Velocity=['batch_norm_30.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_49/, op_role = 2, op_role_var = ['batch_norm_30.w_0', 'batch_norm_30.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_31.b_0'], VelocityOut=['batch_norm_31.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_31.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_31.b_0'], Velocity=['batch_norm_31.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_50/, op_role = 2, op_role_var = ['batch_norm_31.b_0', 'batch_norm_31.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_31.w_0'], VelocityOut=['batch_norm_31.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_31.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_31.w_0'], Velocity=['batch_norm_31.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_51/, op_role = 2, op_role_var = ['batch_norm_31.w_0', 'batch_norm_31.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_32.b_0'], VelocityOut=['batch_norm_32.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_32.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_32.b_0'], Velocity=['batch_norm_32.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_52/, op_role = 2, op_role_var = ['batch_norm_32.b_0', 'batch_norm_32.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_32.w_0'], VelocityOut=['batch_norm_32.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_32.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_32.w_0'], Velocity=['batch_norm_32.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_53/, op_role = 2, op_role_var = ['batch_norm_32.w_0', 'batch_norm_32.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_33.b_0'], VelocityOut=['batch_norm_33.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_33.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_33.b_0'], Velocity=['batch_norm_33.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_54/, op_role = 2, op_role_var = ['batch_norm_33.b_0', 'batch_norm_33.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_33.w_0'], VelocityOut=['batch_norm_33.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_33.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_33.w_0'], Velocity=['batch_norm_33.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_55/, op_role = 2, op_role_var = ['batch_norm_33.w_0', 'batch_norm_33.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_34.b_0'], VelocityOut=['batch_norm_34.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_34.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_34.b_0'], Velocity=['batch_norm_34.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_56/, op_role = 2, op_role_var = ['batch_norm_34.b_0', 'batch_norm_34.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_34.w_0'], VelocityOut=['batch_norm_34.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_34.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_34.w_0'], Velocity=['batch_norm_34.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_57/, op_role = 2, op_role_var = ['batch_norm_34.w_0', 'batch_norm_34.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_35.b_0'], VelocityOut=['batch_norm_35.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_35.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_35.b_0'], Velocity=['batch_norm_35.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_58/, op_role = 2, op_role_var = ['batch_norm_35.b_0', 'batch_norm_35.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_35.w_0'], VelocityOut=['batch_norm_35.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_35.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_35.w_0'], Velocity=['batch_norm_35.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_59/, op_role = 2, op_role_var = ['batch_norm_35.w_0', 'batch_norm_35.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_36.b_0'], VelocityOut=['batch_norm_36.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_36.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_36.b_0'], Velocity=['batch_norm_36.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_60/, op_role = 2, op_role_var = ['batch_norm_36.b_0', 'batch_norm_36.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_36.w_0'], VelocityOut=['batch_norm_36.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_36.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_36.w_0'], Velocity=['batch_norm_36.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_61/, op_role = 2, op_role_var = ['batch_norm_36.w_0', 'batch_norm_36.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_37.b_0'], VelocityOut=['batch_norm_37.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_37.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_37.b_0'], Velocity=['batch_norm_37.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_62/, op_role = 2, op_role_var = ['batch_norm_37.b_0', 'batch_norm_37.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_37.w_0'], VelocityOut=['batch_norm_37.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_37.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_37.w_0'], Velocity=['batch_norm_37.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_63/, op_role = 2, op_role_var = ['batch_norm_37.w_0', 'batch_norm_37.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_38.b_0'], VelocityOut=['batch_norm_38.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_38.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_38.b_0'], Velocity=['batch_norm_38.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_64/, op_role = 2, op_role_var = ['batch_norm_38.b_0', 'batch_norm_38.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_38.w_0'], VelocityOut=['batch_norm_38.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_38.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_38.w_0'], Velocity=['batch_norm_38.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_65/, op_role = 2, op_role_var = ['batch_norm_38.w_0', 'batch_norm_38.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_39.b_0'], VelocityOut=['batch_norm_39.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_39.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_39.b_0'], Velocity=['batch_norm_39.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_66/, op_role = 2, op_role_var = ['batch_norm_39.b_0', 'batch_norm_39.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_39.w_0'], VelocityOut=['batch_norm_39.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_39.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_39.w_0'], Velocity=['batch_norm_39.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_67/, op_role = 2, op_role_var = ['batch_norm_39.w_0', 'batch_norm_39.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_4.b_0'], VelocityOut=['batch_norm_4.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_4.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_4.b_0'], Velocity=['batch_norm_4.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_68/, op_role = 2, op_role_var = ['batch_norm_4.b_0', 'batch_norm_4.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_4.w_0'], VelocityOut=['batch_norm_4.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_4.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_4.w_0'], Velocity=['batch_norm_4.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_69/, op_role = 2, op_role_var = ['batch_norm_4.w_0', 'batch_norm_4.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_40.b_0'], VelocityOut=['batch_norm_40.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_40.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_40.b_0'], Velocity=['batch_norm_40.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_70/, op_role = 2, op_role_var = ['batch_norm_40.b_0', 'batch_norm_40.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_40.w_0'], VelocityOut=['batch_norm_40.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_40.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_40.w_0'], Velocity=['batch_norm_40.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_71/, op_role = 2, op_role_var = ['batch_norm_40.w_0', 'batch_norm_40.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_41.b_0'], VelocityOut=['batch_norm_41.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_41.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_41.b_0'], Velocity=['batch_norm_41.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_72/, op_role = 2, op_role_var = ['batch_norm_41.b_0', 'batch_norm_41.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_41.w_0'], VelocityOut=['batch_norm_41.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_41.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_41.w_0'], Velocity=['batch_norm_41.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_73/, op_role = 2, op_role_var = ['batch_norm_41.w_0', 'batch_norm_41.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_42.b_0'], VelocityOut=['batch_norm_42.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_42.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_42.b_0'], Velocity=['batch_norm_42.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_74/, op_role = 2, op_role_var = ['batch_norm_42.b_0', 'batch_norm_42.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_42.w_0'], VelocityOut=['batch_norm_42.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_42.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_42.w_0'], Velocity=['batch_norm_42.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_75/, op_role = 2, op_role_var = ['batch_norm_42.w_0', 'batch_norm_42.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_43.b_0'], VelocityOut=['batch_norm_43.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_43.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_43.b_0'], Velocity=['batch_norm_43.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_76/, op_role = 2, op_role_var = ['batch_norm_43.b_0', 'batch_norm_43.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_43.w_0'], VelocityOut=['batch_norm_43.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_43.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_43.w_0'], Velocity=['batch_norm_43.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_77/, op_role = 2, op_role_var = ['batch_norm_43.w_0', 'batch_norm_43.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_44.b_0'], VelocityOut=['batch_norm_44.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_44.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_44.b_0'], Velocity=['batch_norm_44.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_78/, op_role = 2, op_role_var = ['batch_norm_44.b_0', 'batch_norm_44.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_44.w_0'], VelocityOut=['batch_norm_44.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_44.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_44.w_0'], Velocity=['batch_norm_44.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_79/, op_role = 2, op_role_var = ['batch_norm_44.w_0', 'batch_norm_44.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_45.b_0'], VelocityOut=['batch_norm_45.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_45.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_45.b_0'], Velocity=['batch_norm_45.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_80/, op_role = 2, op_role_var = ['batch_norm_45.b_0', 'batch_norm_45.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_45.w_0'], VelocityOut=['batch_norm_45.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_45.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_45.w_0'], Velocity=['batch_norm_45.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_81/, op_role = 2, op_role_var = ['batch_norm_45.w_0', 'batch_norm_45.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_46.b_0'], VelocityOut=['batch_norm_46.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_46.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_46.b_0'], Velocity=['batch_norm_46.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_82/, op_role = 2, op_role_var = ['batch_norm_46.b_0', 'batch_norm_46.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_46.w_0'], VelocityOut=['batch_norm_46.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_46.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_46.w_0'], Velocity=['batch_norm_46.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_83/, op_role = 2, op_role_var = ['batch_norm_46.w_0', 'batch_norm_46.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_47.b_0'], VelocityOut=['batch_norm_47.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_47.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_47.b_0'], Velocity=['batch_norm_47.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_84/, op_role = 2, op_role_var = ['batch_norm_47.b_0', 'batch_norm_47.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_47.w_0'], VelocityOut=['batch_norm_47.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_47.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_47.w_0'], Velocity=['batch_norm_47.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_85/, op_role = 2, op_role_var = ['batch_norm_47.w_0', 'batch_norm_47.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_48.b_0'], VelocityOut=['batch_norm_48.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_48.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_48.b_0'], Velocity=['batch_norm_48.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_86/, op_role = 2, op_role_var = ['batch_norm_48.b_0', 'batch_norm_48.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_48.w_0'], VelocityOut=['batch_norm_48.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_48.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_48.w_0'], Velocity=['batch_norm_48.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_87/, op_role = 2, op_role_var = ['batch_norm_48.w_0', 'batch_norm_48.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_49.b_0'], VelocityOut=['batch_norm_49.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_49.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_49.b_0'], Velocity=['batch_norm_49.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_88/, op_role = 2, op_role_var = ['batch_norm_49.b_0', 'batch_norm_49.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_49.w_0'], VelocityOut=['batch_norm_49.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_49.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_49.w_0'], Velocity=['batch_norm_49.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_89/, op_role = 2, op_role_var = ['batch_norm_49.w_0', 'batch_norm_49.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_5.b_0'], VelocityOut=['batch_norm_5.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_5.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_5.b_0'], Velocity=['batch_norm_5.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_90/, op_role = 2, op_role_var = ['batch_norm_5.b_0', 'batch_norm_5.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_5.w_0'], VelocityOut=['batch_norm_5.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_5.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_5.w_0'], Velocity=['batch_norm_5.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_91/, op_role = 2, op_role_var = ['batch_norm_5.w_0', 'batch_norm_5.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_50.b_0'], VelocityOut=['batch_norm_50.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_50.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_50.b_0'], Velocity=['batch_norm_50.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_92/, op_role = 2, op_role_var = ['batch_norm_50.b_0', 'batch_norm_50.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_50.w_0'], VelocityOut=['batch_norm_50.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_50.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_50.w_0'], Velocity=['batch_norm_50.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_93/, op_role = 2, op_role_var = ['batch_norm_50.w_0', 'batch_norm_50.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_51.b_0'], VelocityOut=['batch_norm_51.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_51.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_51.b_0'], Velocity=['batch_norm_51.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_94/, op_role = 2, op_role_var = ['batch_norm_51.b_0', 'batch_norm_51.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_51.w_0'], VelocityOut=['batch_norm_51.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_51.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_51.w_0'], Velocity=['batch_norm_51.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_95/, op_role = 2, op_role_var = ['batch_norm_51.w_0', 'batch_norm_51.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_52.b_0'], VelocityOut=['batch_norm_52.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_52.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_52.b_0'], Velocity=['batch_norm_52.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_96/, op_role = 2, op_role_var = ['batch_norm_52.b_0', 'batch_norm_52.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_52.w_0'], VelocityOut=['batch_norm_52.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_52.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_52.w_0'], Velocity=['batch_norm_52.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_97/, op_role = 2, op_role_var = ['batch_norm_52.w_0', 'batch_norm_52.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_53.b_0'], VelocityOut=['batch_norm_53.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_53.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_53.b_0'], Velocity=['batch_norm_53.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_98/, op_role = 2, op_role_var = ['batch_norm_53.b_0', 'batch_norm_53.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_53.w_0'], VelocityOut=['batch_norm_53.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_53.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_53.w_0'], Velocity=['batch_norm_53.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_99/, op_role = 2, op_role_var = ['batch_norm_53.w_0', 'batch_norm_53.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_54.b_0'], VelocityOut=['batch_norm_54.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_54.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_54.b_0'], Velocity=['batch_norm_54.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_100/, op_role = 2, op_role_var = ['batch_norm_54.b_0', 'batch_norm_54.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_54.w_0'], VelocityOut=['batch_norm_54.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_54.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_54.w_0'], Velocity=['batch_norm_54.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_101/, op_role = 2, op_role_var = ['batch_norm_54.w_0', 'batch_norm_54.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_55.b_0'], VelocityOut=['batch_norm_55.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_55.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_55.b_0'], Velocity=['batch_norm_55.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_102/, op_role = 2, op_role_var = ['batch_norm_55.b_0', 'batch_norm_55.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_55.w_0'], VelocityOut=['batch_norm_55.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_55.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_55.w_0'], Velocity=['batch_norm_55.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_103/, op_role = 2, op_role_var = ['batch_norm_55.w_0', 'batch_norm_55.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_56.b_0'], VelocityOut=['batch_norm_56.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_56.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_56.b_0'], Velocity=['batch_norm_56.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_104/, op_role = 2, op_role_var = ['batch_norm_56.b_0', 'batch_norm_56.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_56.w_0'], VelocityOut=['batch_norm_56.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_56.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_56.w_0'], Velocity=['batch_norm_56.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_105/, op_role = 2, op_role_var = ['batch_norm_56.w_0', 'batch_norm_56.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_57.b_0'], VelocityOut=['batch_norm_57.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_57.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_57.b_0'], Velocity=['batch_norm_57.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_106/, op_role = 2, op_role_var = ['batch_norm_57.b_0', 'batch_norm_57.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_57.w_0'], VelocityOut=['batch_norm_57.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_57.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_57.w_0'], Velocity=['batch_norm_57.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_107/, op_role = 2, op_role_var = ['batch_norm_57.w_0', 'batch_norm_57.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_58.b_0'], VelocityOut=['batch_norm_58.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_58.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_58.b_0'], Velocity=['batch_norm_58.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_108/, op_role = 2, op_role_var = ['batch_norm_58.b_0', 'batch_norm_58.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_58.w_0'], VelocityOut=['batch_norm_58.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_58.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_58.w_0'], Velocity=['batch_norm_58.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_109/, op_role = 2, op_role_var = ['batch_norm_58.w_0', 'batch_norm_58.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_59.b_0'], VelocityOut=['batch_norm_59.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_59.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_59.b_0'], Velocity=['batch_norm_59.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_110/, op_role = 2, op_role_var = ['batch_norm_59.b_0', 'batch_norm_59.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_59.w_0'], VelocityOut=['batch_norm_59.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_59.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_59.w_0'], Velocity=['batch_norm_59.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_111/, op_role = 2, op_role_var = ['batch_norm_59.w_0', 'batch_norm_59.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_6.b_0'], VelocityOut=['batch_norm_6.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_6.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_6.b_0'], Velocity=['batch_norm_6.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_112/, op_role = 2, op_role_var = ['batch_norm_6.b_0', 'batch_norm_6.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_6.w_0'], VelocityOut=['batch_norm_6.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_6.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_6.w_0'], Velocity=['batch_norm_6.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_113/, op_role = 2, op_role_var = ['batch_norm_6.w_0', 'batch_norm_6.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_60.b_0'], VelocityOut=['batch_norm_60.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_60.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_60.b_0'], Velocity=['batch_norm_60.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_114/, op_role = 2, op_role_var = ['batch_norm_60.b_0', 'batch_norm_60.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_60.w_0'], VelocityOut=['batch_norm_60.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_60.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_60.w_0'], Velocity=['batch_norm_60.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_115/, op_role = 2, op_role_var = ['batch_norm_60.w_0', 'batch_norm_60.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_61.b_0'], VelocityOut=['batch_norm_61.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_61.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_61.b_0'], Velocity=['batch_norm_61.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_116/, op_role = 2, op_role_var = ['batch_norm_61.b_0', 'batch_norm_61.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_61.w_0'], VelocityOut=['batch_norm_61.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_61.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_61.w_0'], Velocity=['batch_norm_61.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_117/, op_role = 2, op_role_var = ['batch_norm_61.w_0', 'batch_norm_61.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_62.b_0'], VelocityOut=['batch_norm_62.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_62.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_62.b_0'], Velocity=['batch_norm_62.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_118/, op_role = 2, op_role_var = ['batch_norm_62.b_0', 'batch_norm_62.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_62.w_0'], VelocityOut=['batch_norm_62.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_62.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_62.w_0'], Velocity=['batch_norm_62.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_119/, op_role = 2, op_role_var = ['batch_norm_62.w_0', 'batch_norm_62.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_63.b_0'], VelocityOut=['batch_norm_63.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_63.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_63.b_0'], Velocity=['batch_norm_63.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_120/, op_role = 2, op_role_var = ['batch_norm_63.b_0', 'batch_norm_63.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_63.w_0'], VelocityOut=['batch_norm_63.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_63.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_63.w_0'], Velocity=['batch_norm_63.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_121/, op_role = 2, op_role_var = ['batch_norm_63.w_0', 'batch_norm_63.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_64.b_0'], VelocityOut=['batch_norm_64.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_64.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_64.b_0'], Velocity=['batch_norm_64.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_122/, op_role = 2, op_role_var = ['batch_norm_64.b_0', 'batch_norm_64.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_64.w_0'], VelocityOut=['batch_norm_64.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_64.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_64.w_0'], Velocity=['batch_norm_64.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_123/, op_role = 2, op_role_var = ['batch_norm_64.w_0', 'batch_norm_64.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_65.b_0'], VelocityOut=['batch_norm_65.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_65.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_65.b_0'], Velocity=['batch_norm_65.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_124/, op_role = 2, op_role_var = ['batch_norm_65.b_0', 'batch_norm_65.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_65.w_0'], VelocityOut=['batch_norm_65.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_65.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_65.w_0'], Velocity=['batch_norm_65.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_125/, op_role = 2, op_role_var = ['batch_norm_65.w_0', 'batch_norm_65.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_66.b_0'], VelocityOut=['batch_norm_66.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_66.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_66.b_0'], Velocity=['batch_norm_66.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_126/, op_role = 2, op_role_var = ['batch_norm_66.b_0', 'batch_norm_66.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_66.w_0'], VelocityOut=['batch_norm_66.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_66.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_66.w_0'], Velocity=['batch_norm_66.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_127/, op_role = 2, op_role_var = ['batch_norm_66.w_0', 'batch_norm_66.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_67.b_0'], VelocityOut=['batch_norm_67.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_67.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_67.b_0'], Velocity=['batch_norm_67.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_128/, op_role = 2, op_role_var = ['batch_norm_67.b_0', 'batch_norm_67.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_67.w_0'], VelocityOut=['batch_norm_67.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_67.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_67.w_0'], Velocity=['batch_norm_67.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_129/, op_role = 2, op_role_var = ['batch_norm_67.w_0', 'batch_norm_67.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_68.b_0'], VelocityOut=['batch_norm_68.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_68.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_68.b_0'], Velocity=['batch_norm_68.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_130/, op_role = 2, op_role_var = ['batch_norm_68.b_0', 'batch_norm_68.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_68.w_0'], VelocityOut=['batch_norm_68.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_68.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_68.w_0'], Velocity=['batch_norm_68.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_131/, op_role = 2, op_role_var = ['batch_norm_68.w_0', 'batch_norm_68.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_69.b_0'], VelocityOut=['batch_norm_69.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_69.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_69.b_0'], Velocity=['batch_norm_69.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_132/, op_role = 2, op_role_var = ['batch_norm_69.b_0', 'batch_norm_69.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_69.w_0'], VelocityOut=['batch_norm_69.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_69.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_69.w_0'], Velocity=['batch_norm_69.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_133/, op_role = 2, op_role_var = ['batch_norm_69.w_0', 'batch_norm_69.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_7.b_0'], VelocityOut=['batch_norm_7.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_7.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_7.b_0'], Velocity=['batch_norm_7.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_134/, op_role = 2, op_role_var = ['batch_norm_7.b_0', 'batch_norm_7.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_7.w_0'], VelocityOut=['batch_norm_7.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_7.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_7.w_0'], Velocity=['batch_norm_7.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_135/, op_role = 2, op_role_var = ['batch_norm_7.w_0', 'batch_norm_7.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_70.b_0'], VelocityOut=['batch_norm_70.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_70.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_70.b_0'], Velocity=['batch_norm_70.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_136/, op_role = 2, op_role_var = ['batch_norm_70.b_0', 'batch_norm_70.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_70.w_0'], VelocityOut=['batch_norm_70.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_70.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_70.w_0'], Velocity=['batch_norm_70.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_137/, op_role = 2, op_role_var = ['batch_norm_70.w_0', 'batch_norm_70.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_71.b_0'], VelocityOut=['batch_norm_71.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_71.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_71.b_0'], Velocity=['batch_norm_71.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_138/, op_role = 2, op_role_var = ['batch_norm_71.b_0', 'batch_norm_71.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_71.w_0'], VelocityOut=['batch_norm_71.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_71.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_71.w_0'], Velocity=['batch_norm_71.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_139/, op_role = 2, op_role_var = ['batch_norm_71.w_0', 'batch_norm_71.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_72.b_0'], VelocityOut=['batch_norm_72.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_72.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_72.b_0'], Velocity=['batch_norm_72.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_140/, op_role = 2, op_role_var = ['batch_norm_72.b_0', 'batch_norm_72.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_72.w_0'], VelocityOut=['batch_norm_72.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_72.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_72.w_0'], Velocity=['batch_norm_72.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_141/, op_role = 2, op_role_var = ['batch_norm_72.w_0', 'batch_norm_72.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_73.b_0'], VelocityOut=['batch_norm_73.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_73.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_73.b_0'], Velocity=['batch_norm_73.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_142/, op_role = 2, op_role_var = ['batch_norm_73.b_0', 'batch_norm_73.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_73.w_0'], VelocityOut=['batch_norm_73.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_73.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_73.w_0'], Velocity=['batch_norm_73.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_143/, op_role = 2, op_role_var = ['batch_norm_73.w_0', 'batch_norm_73.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_74.b_0'], VelocityOut=['batch_norm_74.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_74.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_74.b_0'], Velocity=['batch_norm_74.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_144/, op_role = 2, op_role_var = ['batch_norm_74.b_0', 'batch_norm_74.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_74.w_0'], VelocityOut=['batch_norm_74.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_74.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_74.w_0'], Velocity=['batch_norm_74.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_145/, op_role = 2, op_role_var = ['batch_norm_74.w_0', 'batch_norm_74.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_75.b_0'], VelocityOut=['batch_norm_75.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_75.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_75.b_0'], Velocity=['batch_norm_75.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_146/, op_role = 2, op_role_var = ['batch_norm_75.b_0', 'batch_norm_75.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_75.w_0'], VelocityOut=['batch_norm_75.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_75.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_75.w_0'], Velocity=['batch_norm_75.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_147/, op_role = 2, op_role_var = ['batch_norm_75.w_0', 'batch_norm_75.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_76.b_0'], VelocityOut=['batch_norm_76.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_76.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_76.b_0'], Velocity=['batch_norm_76.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_148/, op_role = 2, op_role_var = ['batch_norm_76.b_0', 'batch_norm_76.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_76.w_0'], VelocityOut=['batch_norm_76.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_76.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_76.w_0'], Velocity=['batch_norm_76.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_149/, op_role = 2, op_role_var = ['batch_norm_76.w_0', 'batch_norm_76.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_77.b_0'], VelocityOut=['batch_norm_77.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_77.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_77.b_0'], Velocity=['batch_norm_77.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_150/, op_role = 2, op_role_var = ['batch_norm_77.b_0', 'batch_norm_77.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_77.w_0'], VelocityOut=['batch_norm_77.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_77.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_77.w_0'], Velocity=['batch_norm_77.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_151/, op_role = 2, op_role_var = ['batch_norm_77.w_0', 'batch_norm_77.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_78.b_0'], VelocityOut=['batch_norm_78.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_78.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_78.b_0'], Velocity=['batch_norm_78.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_152/, op_role = 2, op_role_var = ['batch_norm_78.b_0', 'batch_norm_78.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_78.w_0'], VelocityOut=['batch_norm_78.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_78.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_78.w_0'], Velocity=['batch_norm_78.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_153/, op_role = 2, op_role_var = ['batch_norm_78.w_0', 'batch_norm_78.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_8.b_0'], VelocityOut=['batch_norm_8.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_8.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_8.b_0'], Velocity=['batch_norm_8.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_154/, op_role = 2, op_role_var = ['batch_norm_8.b_0', 'batch_norm_8.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_8.w_0'], VelocityOut=['batch_norm_8.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_8.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_8.w_0'], Velocity=['batch_norm_8.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_155/, op_role = 2, op_role_var = ['batch_norm_8.w_0', 'batch_norm_8.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_9.b_0'], VelocityOut=['batch_norm_9.b_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_9.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_9.b_0'], Velocity=['batch_norm_9.b_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_156/, op_role = 2, op_role_var = ['batch_norm_9.b_0', 'batch_norm_9.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {ParamOut=['batch_norm_9.w_0'], VelocityOut=['batch_norm_9.w_0_velocity_0']} = momentum(inputs={Grad=['batch_norm_9.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=[], Param=['batch_norm_9.w_0'], Velocity=['batch_norm_9.w_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = False, op_device = , op_namescope = /optimizer_157/, op_role = 2, op_role_var = ['batch_norm_9.w_0', 'batch_norm_9.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_0.w_0_fp32_master_0'], ParamOut=['conv2d_0.w_0'], VelocityOut=['conv2d_0.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_0.w_0_fp32_master_0'], Param=['conv2d_0.w_0'], Velocity=['conv2d_0.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_158/, op_role = 2, op_role_var = ['conv2d_0.w_0', 'conv2d_0.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_1.w_0_fp32_master_0'], ParamOut=['conv2d_1.w_0'], VelocityOut=['conv2d_1.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_1.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_1.w_0_fp32_master_0'], Param=['conv2d_1.w_0'], Velocity=['conv2d_1.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_159/, op_role = 2, op_role_var = ['conv2d_1.w_0', 'conv2d_1.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_10.w_0_fp32_master_0'], ParamOut=['conv2d_10.w_0'], VelocityOut=['conv2d_10.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_10.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_10.w_0_fp32_master_0'], Param=['conv2d_10.w_0'], Velocity=['conv2d_10.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_160/, op_role = 2, op_role_var = ['conv2d_10.w_0', 'conv2d_10.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_11.w_0_fp32_master_0'], ParamOut=['conv2d_11.w_0'], VelocityOut=['conv2d_11.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_11.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_11.w_0_fp32_master_0'], Param=['conv2d_11.w_0'], Velocity=['conv2d_11.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_161/, op_role = 2, op_role_var = ['conv2d_11.w_0', 'conv2d_11.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_12.w_0_fp32_master_0'], ParamOut=['conv2d_12.w_0'], VelocityOut=['conv2d_12.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_12.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_12.w_0_fp32_master_0'], Param=['conv2d_12.w_0'], Velocity=['conv2d_12.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_162/, op_role = 2, op_role_var = ['conv2d_12.w_0', 'conv2d_12.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_13.w_0_fp32_master_0'], ParamOut=['conv2d_13.w_0'], VelocityOut=['conv2d_13.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_13.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_13.w_0_fp32_master_0'], Param=['conv2d_13.w_0'], Velocity=['conv2d_13.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_163/, op_role = 2, op_role_var = ['conv2d_13.w_0', 'conv2d_13.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_14.w_0_fp32_master_0'], ParamOut=['conv2d_14.w_0'], VelocityOut=['conv2d_14.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_14.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_14.w_0_fp32_master_0'], Param=['conv2d_14.w_0'], Velocity=['conv2d_14.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_164/, op_role = 2, op_role_var = ['conv2d_14.w_0', 'conv2d_14.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_15.w_0_fp32_master_0'], ParamOut=['conv2d_15.w_0'], VelocityOut=['conv2d_15.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_15.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_15.w_0_fp32_master_0'], Param=['conv2d_15.w_0'], Velocity=['conv2d_15.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_165/, op_role = 2, op_role_var = ['conv2d_15.w_0', 'conv2d_15.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_16.w_0_fp32_master_0'], ParamOut=['conv2d_16.w_0'], VelocityOut=['conv2d_16.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_16.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_16.w_0_fp32_master_0'], Param=['conv2d_16.w_0'], Velocity=['conv2d_16.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_166/, op_role = 2, op_role_var = ['conv2d_16.w_0', 'conv2d_16.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_17.w_0_fp32_master_0'], ParamOut=['conv2d_17.w_0'], VelocityOut=['conv2d_17.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_17.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_17.w_0_fp32_master_0'], Param=['conv2d_17.w_0'], Velocity=['conv2d_17.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_167/, op_role = 2, op_role_var = ['conv2d_17.w_0', 'conv2d_17.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_18.w_0_fp32_master_0'], ParamOut=['conv2d_18.w_0'], VelocityOut=['conv2d_18.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_18.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_18.w_0_fp32_master_0'], Param=['conv2d_18.w_0'], Velocity=['conv2d_18.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_168/, op_role = 2, op_role_var = ['conv2d_18.w_0', 'conv2d_18.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_19.w_0_fp32_master_0'], ParamOut=['conv2d_19.w_0'], VelocityOut=['conv2d_19.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_19.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_19.w_0_fp32_master_0'], Param=['conv2d_19.w_0'], Velocity=['conv2d_19.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_169/, op_role = 2, op_role_var = ['conv2d_19.w_0', 'conv2d_19.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_2.w_0_fp32_master_0'], ParamOut=['conv2d_2.w_0'], VelocityOut=['conv2d_2.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_2.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_2.w_0_fp32_master_0'], Param=['conv2d_2.w_0'], Velocity=['conv2d_2.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_170/, op_role = 2, op_role_var = ['conv2d_2.w_0', 'conv2d_2.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_20.w_0_fp32_master_0'], ParamOut=['conv2d_20.w_0'], VelocityOut=['conv2d_20.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_20.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_20.w_0_fp32_master_0'], Param=['conv2d_20.w_0'], Velocity=['conv2d_20.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_171/, op_role = 2, op_role_var = ['conv2d_20.w_0', 'conv2d_20.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_21.w_0_fp32_master_0'], ParamOut=['conv2d_21.w_0'], VelocityOut=['conv2d_21.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_21.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_21.w_0_fp32_master_0'], Param=['conv2d_21.w_0'], Velocity=['conv2d_21.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_172/, op_role = 2, op_role_var = ['conv2d_21.w_0', 'conv2d_21.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_22.w_0_fp32_master_0'], ParamOut=['conv2d_22.w_0'], VelocityOut=['conv2d_22.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_22.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_22.w_0_fp32_master_0'], Param=['conv2d_22.w_0'], Velocity=['conv2d_22.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_173/, op_role = 2, op_role_var = ['conv2d_22.w_0', 'conv2d_22.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_23.w_0_fp32_master_0'], ParamOut=['conv2d_23.w_0'], VelocityOut=['conv2d_23.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_23.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_23.w_0_fp32_master_0'], Param=['conv2d_23.w_0'], Velocity=['conv2d_23.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_174/, op_role = 2, op_role_var = ['conv2d_23.w_0', 'conv2d_23.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_24.w_0_fp32_master_0'], ParamOut=['conv2d_24.w_0'], VelocityOut=['conv2d_24.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_24.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_24.w_0_fp32_master_0'], Param=['conv2d_24.w_0'], Velocity=['conv2d_24.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_175/, op_role = 2, op_role_var = ['conv2d_24.w_0', 'conv2d_24.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_25.w_0_fp32_master_0'], ParamOut=['conv2d_25.w_0'], VelocityOut=['conv2d_25.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_25.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_25.w_0_fp32_master_0'], Param=['conv2d_25.w_0'], Velocity=['conv2d_25.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_176/, op_role = 2, op_role_var = ['conv2d_25.w_0', 'conv2d_25.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_26.w_0_fp32_master_0'], ParamOut=['conv2d_26.w_0'], VelocityOut=['conv2d_26.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_26.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_26.w_0_fp32_master_0'], Param=['conv2d_26.w_0'], Velocity=['conv2d_26.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_177/, op_role = 2, op_role_var = ['conv2d_26.w_0', 'conv2d_26.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_27.w_0_fp32_master_0'], ParamOut=['conv2d_27.w_0'], VelocityOut=['conv2d_27.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_27.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_27.w_0_fp32_master_0'], Param=['conv2d_27.w_0'], Velocity=['conv2d_27.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_178/, op_role = 2, op_role_var = ['conv2d_27.w_0', 'conv2d_27.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_28.w_0_fp32_master_0'], ParamOut=['conv2d_28.w_0'], VelocityOut=['conv2d_28.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_28.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_28.w_0_fp32_master_0'], Param=['conv2d_28.w_0'], Velocity=['conv2d_28.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_179/, op_role = 2, op_role_var = ['conv2d_28.w_0', 'conv2d_28.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_29.w_0_fp32_master_0'], ParamOut=['conv2d_29.w_0'], VelocityOut=['conv2d_29.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_29.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_29.w_0_fp32_master_0'], Param=['conv2d_29.w_0'], Velocity=['conv2d_29.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_180/, op_role = 2, op_role_var = ['conv2d_29.w_0', 'conv2d_29.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_3.w_0_fp32_master_0'], ParamOut=['conv2d_3.w_0'], VelocityOut=['conv2d_3.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_3.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_3.w_0_fp32_master_0'], Param=['conv2d_3.w_0'], Velocity=['conv2d_3.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_181/, op_role = 2, op_role_var = ['conv2d_3.w_0', 'conv2d_3.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_30.w_0_fp32_master_0'], ParamOut=['conv2d_30.w_0'], VelocityOut=['conv2d_30.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_30.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_30.w_0_fp32_master_0'], Param=['conv2d_30.w_0'], Velocity=['conv2d_30.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_182/, op_role = 2, op_role_var = ['conv2d_30.w_0', 'conv2d_30.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_31.w_0_fp32_master_0'], ParamOut=['conv2d_31.w_0'], VelocityOut=['conv2d_31.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_31.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_31.w_0_fp32_master_0'], Param=['conv2d_31.w_0'], Velocity=['conv2d_31.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_183/, op_role = 2, op_role_var = ['conv2d_31.w_0', 'conv2d_31.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_32.w_0_fp32_master_0'], ParamOut=['conv2d_32.w_0'], VelocityOut=['conv2d_32.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_32.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_32.w_0_fp32_master_0'], Param=['conv2d_32.w_0'], Velocity=['conv2d_32.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_184/, op_role = 2, op_role_var = ['conv2d_32.w_0', 'conv2d_32.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_33.w_0_fp32_master_0'], ParamOut=['conv2d_33.w_0'], VelocityOut=['conv2d_33.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_33.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_33.w_0_fp32_master_0'], Param=['conv2d_33.w_0'], Velocity=['conv2d_33.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_185/, op_role = 2, op_role_var = ['conv2d_33.w_0', 'conv2d_33.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_34.w_0_fp32_master_0'], ParamOut=['conv2d_34.w_0'], VelocityOut=['conv2d_34.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_34.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_34.w_0_fp32_master_0'], Param=['conv2d_34.w_0'], Velocity=['conv2d_34.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_186/, op_role = 2, op_role_var = ['conv2d_34.w_0', 'conv2d_34.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_35.w_0_fp32_master_0'], ParamOut=['conv2d_35.w_0'], VelocityOut=['conv2d_35.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_35.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_35.w_0_fp32_master_0'], Param=['conv2d_35.w_0'], Velocity=['conv2d_35.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_187/, op_role = 2, op_role_var = ['conv2d_35.w_0', 'conv2d_35.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_36.w_0_fp32_master_0'], ParamOut=['conv2d_36.w_0'], VelocityOut=['conv2d_36.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_36.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_36.w_0_fp32_master_0'], Param=['conv2d_36.w_0'], Velocity=['conv2d_36.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_188/, op_role = 2, op_role_var = ['conv2d_36.w_0', 'conv2d_36.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_37.w_0_fp32_master_0'], ParamOut=['conv2d_37.w_0'], VelocityOut=['conv2d_37.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_37.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_37.w_0_fp32_master_0'], Param=['conv2d_37.w_0'], Velocity=['conv2d_37.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_189/, op_role = 2, op_role_var = ['conv2d_37.w_0', 'conv2d_37.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_38.w_0_fp32_master_0'], ParamOut=['conv2d_38.w_0'], VelocityOut=['conv2d_38.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_38.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_38.w_0_fp32_master_0'], Param=['conv2d_38.w_0'], Velocity=['conv2d_38.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_190/, op_role = 2, op_role_var = ['conv2d_38.w_0', 'conv2d_38.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_39.w_0_fp32_master_0'], ParamOut=['conv2d_39.w_0'], VelocityOut=['conv2d_39.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_39.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_39.w_0_fp32_master_0'], Param=['conv2d_39.w_0'], Velocity=['conv2d_39.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_191/, op_role = 2, op_role_var = ['conv2d_39.w_0', 'conv2d_39.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_4.w_0_fp32_master_0'], ParamOut=['conv2d_4.w_0'], VelocityOut=['conv2d_4.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_4.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_4.w_0_fp32_master_0'], Param=['conv2d_4.w_0'], Velocity=['conv2d_4.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_192/, op_role = 2, op_role_var = ['conv2d_4.w_0', 'conv2d_4.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_40.w_0_fp32_master_0'], ParamOut=['conv2d_40.w_0'], VelocityOut=['conv2d_40.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_40.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_40.w_0_fp32_master_0'], Param=['conv2d_40.w_0'], Velocity=['conv2d_40.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_193/, op_role = 2, op_role_var = ['conv2d_40.w_0', 'conv2d_40.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_41.w_0_fp32_master_0'], ParamOut=['conv2d_41.w_0'], VelocityOut=['conv2d_41.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_41.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_41.w_0_fp32_master_0'], Param=['conv2d_41.w_0'], Velocity=['conv2d_41.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_194/, op_role = 2, op_role_var = ['conv2d_41.w_0', 'conv2d_41.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_42.w_0_fp32_master_0'], ParamOut=['conv2d_42.w_0'], VelocityOut=['conv2d_42.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_42.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_42.w_0_fp32_master_0'], Param=['conv2d_42.w_0'], Velocity=['conv2d_42.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_195/, op_role = 2, op_role_var = ['conv2d_42.w_0', 'conv2d_42.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_43.w_0_fp32_master_0'], ParamOut=['conv2d_43.w_0'], VelocityOut=['conv2d_43.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_43.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_43.w_0_fp32_master_0'], Param=['conv2d_43.w_0'], Velocity=['conv2d_43.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_196/, op_role = 2, op_role_var = ['conv2d_43.w_0', 'conv2d_43.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_44.w_0_fp32_master_0'], ParamOut=['conv2d_44.w_0'], VelocityOut=['conv2d_44.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_44.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_44.w_0_fp32_master_0'], Param=['conv2d_44.w_0'], Velocity=['conv2d_44.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_197/, op_role = 2, op_role_var = ['conv2d_44.w_0', 'conv2d_44.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_45.w_0_fp32_master_0'], ParamOut=['conv2d_45.w_0'], VelocityOut=['conv2d_45.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_45.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_45.w_0_fp32_master_0'], Param=['conv2d_45.w_0'], Velocity=['conv2d_45.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_198/, op_role = 2, op_role_var = ['conv2d_45.w_0', 'conv2d_45.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_46.w_0_fp32_master_0'], ParamOut=['conv2d_46.w_0'], VelocityOut=['conv2d_46.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_46.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_46.w_0_fp32_master_0'], Param=['conv2d_46.w_0'], Velocity=['conv2d_46.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_199/, op_role = 2, op_role_var = ['conv2d_46.w_0', 'conv2d_46.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_47.w_0_fp32_master_0'], ParamOut=['conv2d_47.w_0'], VelocityOut=['conv2d_47.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_47.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_47.w_0_fp32_master_0'], Param=['conv2d_47.w_0'], Velocity=['conv2d_47.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_200/, op_role = 2, op_role_var = ['conv2d_47.w_0', 'conv2d_47.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_48.w_0_fp32_master_0'], ParamOut=['conv2d_48.w_0'], VelocityOut=['conv2d_48.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_48.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_48.w_0_fp32_master_0'], Param=['conv2d_48.w_0'], Velocity=['conv2d_48.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_201/, op_role = 2, op_role_var = ['conv2d_48.w_0', 'conv2d_48.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_49.w_0_fp32_master_0'], ParamOut=['conv2d_49.w_0'], VelocityOut=['conv2d_49.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_49.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_49.w_0_fp32_master_0'], Param=['conv2d_49.w_0'], Velocity=['conv2d_49.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_202/, op_role = 2, op_role_var = ['conv2d_49.w_0', 'conv2d_49.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_5.w_0_fp32_master_0'], ParamOut=['conv2d_5.w_0'], VelocityOut=['conv2d_5.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_5.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_5.w_0_fp32_master_0'], Param=['conv2d_5.w_0'], Velocity=['conv2d_5.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_203/, op_role = 2, op_role_var = ['conv2d_5.w_0', 'conv2d_5.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_50.w_0_fp32_master_0'], ParamOut=['conv2d_50.w_0'], VelocityOut=['conv2d_50.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_50.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_50.w_0_fp32_master_0'], Param=['conv2d_50.w_0'], Velocity=['conv2d_50.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_204/, op_role = 2, op_role_var = ['conv2d_50.w_0', 'conv2d_50.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_51.w_0_fp32_master_0'], ParamOut=['conv2d_51.w_0'], VelocityOut=['conv2d_51.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_51.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_51.w_0_fp32_master_0'], Param=['conv2d_51.w_0'], Velocity=['conv2d_51.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_205/, op_role = 2, op_role_var = ['conv2d_51.w_0', 'conv2d_51.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_52.w_0_fp32_master_0'], ParamOut=['conv2d_52.w_0'], VelocityOut=['conv2d_52.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_52.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_52.w_0_fp32_master_0'], Param=['conv2d_52.w_0'], Velocity=['conv2d_52.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_206/, op_role = 2, op_role_var = ['conv2d_52.w_0', 'conv2d_52.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_6.w_0_fp32_master_0'], ParamOut=['conv2d_6.w_0'], VelocityOut=['conv2d_6.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_6.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_6.w_0_fp32_master_0'], Param=['conv2d_6.w_0'], Velocity=['conv2d_6.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_207/, op_role = 2, op_role_var = ['conv2d_6.w_0', 'conv2d_6.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_7.w_0_fp32_master_0'], ParamOut=['conv2d_7.w_0'], VelocityOut=['conv2d_7.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_7.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_7.w_0_fp32_master_0'], Param=['conv2d_7.w_0'], Velocity=['conv2d_7.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_208/, op_role = 2, op_role_var = ['conv2d_7.w_0', 'conv2d_7.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_8.w_0_fp32_master_0'], ParamOut=['conv2d_8.w_0'], VelocityOut=['conv2d_8.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_8.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_8.w_0_fp32_master_0'], Param=['conv2d_8.w_0'], Velocity=['conv2d_8.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_209/, op_role = 2, op_role_var = ['conv2d_8.w_0', 'conv2d_8.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['conv2d_9.w_0_fp32_master_0'], ParamOut=['conv2d_9.w_0'], VelocityOut=['conv2d_9.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['conv2d_9.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['conv2d_9.w_0_fp32_master_0'], Param=['conv2d_9.w_0'], Velocity=['conv2d_9.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_210/, op_role = 2, op_role_var = ['conv2d_9.w_0', 'conv2d_9.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['dist@fc@rank@00000.w.w_0_fp32_master_0'], ParamOut=['dist@fc@rank@00000.w.w_0'], VelocityOut=['dist@fc@rank@00000.w.w_0_fp32_master_0_velocity_0']} = sparse_momentum(inputs={Axis=[], Grad=['gather_0.tmp_0@GRAD'], Index=['class_center_sample_0.tmp_1'], LearningRate=['learning_rate_0'], MasterParam=['dist@fc@rank@00000.w.w_0_fp32_master_0'], Param=['dist@fc@rank@00000.w.w_0'], Velocity=['dist@fc@rank@00000.w.w_0_fp32_master_0_velocity_0']}, axis = 1, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /, op_role = 2, op_role_var = ['dist@fc@rank@00000.w.w_0'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['fc_0.b_0_fp32_master_0'], ParamOut=['fc_0.b_0'], VelocityOut=['fc_0.b_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['fc_0.b_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['fc_0.b_0_fp32_master_0'], Param=['fc_0.b_0'], Velocity=['fc_0.b_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_212/, op_role = 2, op_role_var = ['fc_0.b_0', 'fc_0.b_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['fc_0.w_0_fp32_master_0'], ParamOut=['fc_0.w_0'], VelocityOut=['fc_0.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['fc_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['fc_0.w_0_fp32_master_0'], Param=['fc_0.w_0'], Velocity=['fc_0.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_213/, op_role = 2, op_role_var = ['fc_0.w_0', 'fc_0.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_0.w_0_fp32_master_0'], ParamOut=['prelu_0.w_0'], VelocityOut=['prelu_0.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_0.w_0_fp32_master_0'], Param=['prelu_0.w_0'], Velocity=['prelu_0.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_214/, op_role = 2, op_role_var = ['prelu_0.w_0', 'prelu_0.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_1.w_0_fp32_master_0'], ParamOut=['prelu_1.w_0'], VelocityOut=['prelu_1.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_1.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_1.w_0_fp32_master_0'], Param=['prelu_1.w_0'], Velocity=['prelu_1.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_215/, op_role = 2, op_role_var = ['prelu_1.w_0', 'prelu_1.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_10.w_0_fp32_master_0'], ParamOut=['prelu_10.w_0'], VelocityOut=['prelu_10.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_10.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_10.w_0_fp32_master_0'], Param=['prelu_10.w_0'], Velocity=['prelu_10.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_216/, op_role = 2, op_role_var = ['prelu_10.w_0', 'prelu_10.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_11.w_0_fp32_master_0'], ParamOut=['prelu_11.w_0'], VelocityOut=['prelu_11.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_11.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_11.w_0_fp32_master_0'], Param=['prelu_11.w_0'], Velocity=['prelu_11.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_217/, op_role = 2, op_role_var = ['prelu_11.w_0', 'prelu_11.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_12.w_0_fp32_master_0'], ParamOut=['prelu_12.w_0'], VelocityOut=['prelu_12.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_12.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_12.w_0_fp32_master_0'], Param=['prelu_12.w_0'], Velocity=['prelu_12.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_218/, op_role = 2, op_role_var = ['prelu_12.w_0', 'prelu_12.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_13.w_0_fp32_master_0'], ParamOut=['prelu_13.w_0'], VelocityOut=['prelu_13.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_13.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_13.w_0_fp32_master_0'], Param=['prelu_13.w_0'], Velocity=['prelu_13.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_219/, op_role = 2, op_role_var = ['prelu_13.w_0', 'prelu_13.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_14.w_0_fp32_master_0'], ParamOut=['prelu_14.w_0'], VelocityOut=['prelu_14.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_14.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_14.w_0_fp32_master_0'], Param=['prelu_14.w_0'], Velocity=['prelu_14.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_220/, op_role = 2, op_role_var = ['prelu_14.w_0', 'prelu_14.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_15.w_0_fp32_master_0'], ParamOut=['prelu_15.w_0'], VelocityOut=['prelu_15.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_15.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_15.w_0_fp32_master_0'], Param=['prelu_15.w_0'], Velocity=['prelu_15.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_221/, op_role = 2, op_role_var = ['prelu_15.w_0', 'prelu_15.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_16.w_0_fp32_master_0'], ParamOut=['prelu_16.w_0'], VelocityOut=['prelu_16.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_16.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_16.w_0_fp32_master_0'], Param=['prelu_16.w_0'], Velocity=['prelu_16.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_222/, op_role = 2, op_role_var = ['prelu_16.w_0', 'prelu_16.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_17.w_0_fp32_master_0'], ParamOut=['prelu_17.w_0'], VelocityOut=['prelu_17.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_17.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_17.w_0_fp32_master_0'], Param=['prelu_17.w_0'], Velocity=['prelu_17.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_223/, op_role = 2, op_role_var = ['prelu_17.w_0', 'prelu_17.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_18.w_0_fp32_master_0'], ParamOut=['prelu_18.w_0'], VelocityOut=['prelu_18.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_18.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_18.w_0_fp32_master_0'], Param=['prelu_18.w_0'], Velocity=['prelu_18.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_224/, op_role = 2, op_role_var = ['prelu_18.w_0', 'prelu_18.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_19.w_0_fp32_master_0'], ParamOut=['prelu_19.w_0'], VelocityOut=['prelu_19.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_19.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_19.w_0_fp32_master_0'], Param=['prelu_19.w_0'], Velocity=['prelu_19.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_225/, op_role = 2, op_role_var = ['prelu_19.w_0', 'prelu_19.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_2.w_0_fp32_master_0'], ParamOut=['prelu_2.w_0'], VelocityOut=['prelu_2.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_2.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_2.w_0_fp32_master_0'], Param=['prelu_2.w_0'], Velocity=['prelu_2.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_226/, op_role = 2, op_role_var = ['prelu_2.w_0', 'prelu_2.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_20.w_0_fp32_master_0'], ParamOut=['prelu_20.w_0'], VelocityOut=['prelu_20.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_20.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_20.w_0_fp32_master_0'], Param=['prelu_20.w_0'], Velocity=['prelu_20.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_227/, op_role = 2, op_role_var = ['prelu_20.w_0', 'prelu_20.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_21.w_0_fp32_master_0'], ParamOut=['prelu_21.w_0'], VelocityOut=['prelu_21.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_21.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_21.w_0_fp32_master_0'], Param=['prelu_21.w_0'], Velocity=['prelu_21.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_228/, op_role = 2, op_role_var = ['prelu_21.w_0', 'prelu_21.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_22.w_0_fp32_master_0'], ParamOut=['prelu_22.w_0'], VelocityOut=['prelu_22.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_22.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_22.w_0_fp32_master_0'], Param=['prelu_22.w_0'], Velocity=['prelu_22.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_229/, op_role = 2, op_role_var = ['prelu_22.w_0', 'prelu_22.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_23.w_0_fp32_master_0'], ParamOut=['prelu_23.w_0'], VelocityOut=['prelu_23.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_23.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_23.w_0_fp32_master_0'], Param=['prelu_23.w_0'], Velocity=['prelu_23.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_230/, op_role = 2, op_role_var = ['prelu_23.w_0', 'prelu_23.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_24.w_0_fp32_master_0'], ParamOut=['prelu_24.w_0'], VelocityOut=['prelu_24.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_24.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_24.w_0_fp32_master_0'], Param=['prelu_24.w_0'], Velocity=['prelu_24.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_231/, op_role = 2, op_role_var = ['prelu_24.w_0', 'prelu_24.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_3.w_0_fp32_master_0'], ParamOut=['prelu_3.w_0'], VelocityOut=['prelu_3.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_3.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_3.w_0_fp32_master_0'], Param=['prelu_3.w_0'], Velocity=['prelu_3.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_232/, op_role = 2, op_role_var = ['prelu_3.w_0', 'prelu_3.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_4.w_0_fp32_master_0'], ParamOut=['prelu_4.w_0'], VelocityOut=['prelu_4.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_4.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_4.w_0_fp32_master_0'], Param=['prelu_4.w_0'], Velocity=['prelu_4.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_233/, op_role = 2, op_role_var = ['prelu_4.w_0', 'prelu_4.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_5.w_0_fp32_master_0'], ParamOut=['prelu_5.w_0'], VelocityOut=['prelu_5.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_5.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_5.w_0_fp32_master_0'], Param=['prelu_5.w_0'], Velocity=['prelu_5.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_234/, op_role = 2, op_role_var = ['prelu_5.w_0', 'prelu_5.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_6.w_0_fp32_master_0'], ParamOut=['prelu_6.w_0'], VelocityOut=['prelu_6.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_6.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_6.w_0_fp32_master_0'], Param=['prelu_6.w_0'], Velocity=['prelu_6.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_235/, op_role = 2, op_role_var = ['prelu_6.w_0', 'prelu_6.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_7.w_0_fp32_master_0'], ParamOut=['prelu_7.w_0'], VelocityOut=['prelu_7.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_7.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_7.w_0_fp32_master_0'], Param=['prelu_7.w_0'], Velocity=['prelu_7.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_236/, op_role = 2, op_role_var = ['prelu_7.w_0', 'prelu_7.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_8.w_0_fp32_master_0'], ParamOut=['prelu_8.w_0'], VelocityOut=['prelu_8.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_8.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_8.w_0_fp32_master_0'], Param=['prelu_8.w_0'], Velocity=['prelu_8.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_237/, op_role = 2, op_role_var = ['prelu_8.w_0', 'prelu_8.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
    {MasterParamOut=['prelu_9.w_0_fp32_master_0'], ParamOut=['prelu_9.w_0'], VelocityOut=['prelu_9.w_0_fp32_master_0_velocity_0']} = momentum(inputs={Grad=['prelu_9.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['prelu_9.w_0_fp32_master_0'], Param=['prelu_9.w_0'], Velocity=['prelu_9.w_0_fp32_master_0_velocity_0']}, mu = 0.8999999761581421, multi_precision = True, op_device = , op_namescope = /optimizer_238/, op_role = 2, op_role_var = ['prelu_9.w_0', 'prelu_9.w_0@GRAD'], regularization_coeff = 0.0005000000237487257, regularization_method = l2_decay, rescale_grad = 1.0, use_nesterov = False, with_quant_attr = False)
}
